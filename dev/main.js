// Declare and initialize constants
const apiKey = "dc46558a883b422497f8e35b4098f5da";
const searchForm = document.querySelector('#search-form');
const searchResults = document.querySelector('#search-results');
const pagination = document.querySelector('#pagination');
const prevPageButton = document.querySelector('#prev-page');
const nextPageButton = document.querySelector('#next-page');
const pageNumber = document.querySelector('#page-number');
const totalPageCount = document.querySelector('#total-pages');

// Declare and initialize variables
let currentPage = 1;
let totalResults = 0;
let totalPages = 0;
let currentOffset = 0;
let searchResultsData = [];
let resultsToDisplay = [];
let newSearchResultsData =[];
let previousTotalResults = 0;
let clickedUrls = [];
let freqMap = new Map();
let trainData = [];
let testData = [];
var model;

// Event listener for search form submit
searchForm.addEventListener('submit', event => {
  // location.reload(true);
  event.preventDefault();
  const searchTerm = searchForm.elements['search-term'].value;
  currentOffset = 0;
  currentPage = 1;
  searchBingApi(searchTerm);
});

// Create worker for preprocessing the data
let preprocessing_worker = new Worker('preprocessing_worker.js');

// Create worker for word embedding
let embedding_hash_worker = new Worker('embedding_hash.js');

// Create worker for training linear regression model
let trainWorker = new Worker('train_LR_worker.js');

// Create worker for computing similarity scores
let scoreWorker = new Worker('predict_LR_worker.js');

preprocessing_worker.addEventListener('message', event => {
  searchResultsData = event.data[0];
  corpus = event.data[1];
  freqMap = event.data[2];
  console.log('Preprocessing Worker acheived');
  embedding_hash_worker.postMessage([freqMap,searchResultsData]);
});

embedding_hash_worker.addEventListener('message', event => {
  searchResultsData = event.data;
  console.log('Embedding Worker acheived');
  const startIndex = (currentPage - 1) * 10;
  const endIndex = startIndex + 10;
  testData = searchResultsData.slice(endIndex,);
  // console.log(searchResultsData)
});

trainWorker.addEventListener('message', event=> {
  model = event.data;
  console.log('Model Trained');
  scoreWorker.postMessage([model,testData]);
  console.log(model);
});

scoreWorker.addEventListener('message', event => {
  testData = event.data;
  
  displaySearchResults();
  updatePagination();
})
// Function to search Bing API
function searchBingApi(searchTerm) {

  // // Construct API URL with search term and offset
  // const apiUrl = `https://api.duckduckgo.com/?q=${encodeURIComponent(searchTerm)}&format=json&offset=${previousTotalResults}&count=50`;
  // // const apiUrl = `https://api.bing.microsoft.com/v7.0/search?q=${encodeURIComponent(searchTerm)}&count=50&offset=${previousTotalResults}`;

  // // Fetch data from API
  // fetch(apiUrl).then(response => {
  //   if (!response.ok) {
  //     throw new Error(`HTTP error! Status: ${response.status}`);
  //   }
  //   return response.json();
  // })
  // .then(data => {
    searchResultsData = [{"name":"Sinogram-based motion correction of PET images using optical motion tracking system and list-mode data acquisition","snippet":"A head motion during brain imaging has been recognized as a source of image degradation and introduces distortion in positron emission tomography (PET) image. There are several techniques to correct the motion artifact, but these techniques cannot correct the motion during scanning. The aim of this study is to develop a sinogram-based motion correction (SBMC) method to correct directly the head motion during PET scanning using a motion tracking system and list-mode data acquisition. This method is a rebinning procedure by which the lines of response (LOR) are geometrically transformed according to the current values of the six-dimensional motion data. Michelogram was recomposed using rebinned LOR and motion corrected sinogram was generated. In the motion corrected image, the blurring artifact due to motion was reduced by SBMC method.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1239453"},{"name":"A fault tolerant control architecture for automated highway systems","snippet":"A hierarchical controller for dealing with faults and adverse environmental conditions on an automated highway system is proposed. The controller extends a previous control hierarchy designed to work under normal conditions of operation. The faults are classified according to the capabilities remaining on the vehicle or roadside after the fault has occurred. Information about these capabilities is used by supervisors in each of the layers of the hierarchy to select appropriate fault handling strategies. We outline the strategies needed by the supervisors and give examples of their detailed operation","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=826792"},{"name":"Fault tolerant memory design for HW\/SW co-reliability in massively parallel computing systems","snippet":"A highly dependable embedded fault-tolerant memory architecture for high performance massively parallel computing applications and its dependability assurance techniques are proposed and discussed in this paper. The proposed fault tolerant memory provides two distinctive repair mechanisms: the permanent laser redundancy reconfiguration during the wafer probe stage in the factory to enhance its manufacturing yield and the dynamic BIST\/BISD\/BISR (built-in-self-test-diagnosis-repair)-based reconfiguration of the redundant resources in field to maintain high field reliability. The system reliability which is mainly determined by hardware configuration demanded by software and field reconfiguration\/repair utilizing unused processor and memory modules is referred to as HW\/SW Co-reliability. Various system configuration options in terms of parallel processing unit size and processor\/memory intensity are also introduced and their HW\/SW Co-reliability characteristics are discussed. A modeling and assurance technique for HW\/SW Co-reliability with emphasis on the dependability assurance techniques based on combinatorial modeling suitable for the proposed memory design is developed and validated by extensive parametric simulations. Thereby, design and Implementation of memory-reliability-optimized and highly reliable fault-tolerant field reconfigurable massively parallel computing systems can be achieved.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1201173"},{"name":"Efficient color correction approach for phase unwrapping based on color-encoded digital fringe projection","snippet":"A highly efficient color correction approach based on color-encoded fringe projection is proposed, which combine color image segmentation and color intensity interpolation technique. Only 24 designed color patterns are projected and recorded to implement the process with a high brightness DLP projector and a color camera. To establish the correspondence between the designed color intensity and recorded color intensity, the recorded image is firstly segmented into some adjacent grid region by neighboring pixel intensity fitting error, the grid region is then grown to the region boundary employing some process algorithm, thirdly, the region number is labeled and adjusted based on the designed color pattern by searching the region centre coordinate and applying a man-machine conversation method, finally, the color correspondence relation is established according to the designed color pattern pixel index and the labeled grid region number of recorded image. While doing the color correction, firstly, the initial color intensity is searched according to the minimum color distance between the recorded color and designed color. Secondly, color interpolation is implemented to obtain the true color intensity correspondence to recorded color. The proposed approach validity is testified by experiment results.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5564281"},{"name":"High-performance line conditioner with output voltage regulation and power factor correction","snippet":"A high-performance line conditioner with excellent efficiency and power factor is proposed. The line conditioner consists of a three-leg rectifier-inverter, which operates as a boost converter and a buck converter. This boost-buck topology enables constant output voltage regulation, irrespective of input voltage disturbances. In addition the three-leg bridge can reduce the number of switching devices and system loss, while maintaining the capabilities of power factor correction and good output voltage regulation. The power factor controller for the single-phase pulse-width modulated (PWM) rectifier is derived using the feedback linearisation concept. The inverter side acts as a voltage regulator with current-limiting capability for impulsive loads. The disturbance of input voltage is detected using a fast-sensing technique. Experimental results obtained on a 3 kVA prototype show a normal efficiency of over 95% and input power factor of over 99%.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1262753"},{"name":"Study on Fault Diagnosis Expert System for Power Supply Circuit Board on Vxi Bus","snippet":"A high-tech information electronic equipment of some given type is designed in order to proceed automatically fault detection and improve the efficiency and accuracy of diagnosis. This thesis which is a part of the program introduces the research of algorithm of fault diagnose expert system of a power supply circuit board of an electronic device and algorithm realization and example proving on the hardware platform. It's quicker and more convenient to locate fault on the circuit boards with this equipment. It's proved that this expert system can solve the problems of high cost and long intervals of maintenance and keep the equipment in a stable status","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4028176"},{"name":"Fault diagnosis systems development for Fuel Cell Vehicle","snippet":"A hydrogen-powered fuel cell vehicle is developed, in which a distributed control and communication system based on CAN (Controller Area Network) is built. For vehicle diagnostic purpose, a new on-board fault diagnosis strategy is presented. There are two efficient automotive diagnostic systems based on CAN designed and implemented in this paper: (1)CANoe is a powerful CAN development tool. A fault diagnosis environment based on CANoe is established to satisfy the needs of on-board and off-board fault diagnosis application of FCV. By setting up the communication interface between CANoe and Access, the vehicle fault codes are collected and stored. Meanwhile a database is designed for the management of fault information. (2) A hand-held fault diagnosis equipment as well as a windows analyzer interface is set up. All fault information from FCVpsilas CAN network can be gotten easily by the equipment. With the Serial Communication between the equipment and PC, the fault codes stored in the equipment can be read, analyzed and disposed by PC.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4677487"},{"name":"Non-uniformity correction and calibration of a portable infrared scene projector","snippet":"A key attribute of any tester for FLIR systems is a calibrated uniform source. A uniform source ensures that any anomalies in performance are artifacts of the FLIR being tested and not the tester. Achieving a uniform source from a resistor array based portable infrared scene projector requires implementation of nonuniformity correction algorithms instead of controlling the bonding integrity of a source to a cooler, and the coating properties of the source typical of a conventional blackbody. The necessity to perform the non-uniformity correction on the scene projector is because the source is a two-dimensional array comprised of discrete resistive emitters. Ideally, each emitter of the array would have the same resistance and thus produce the same output for a given drive current. However, there are small variations from emitter to emitter over the thousands of emitters that comprise an array. Once a uniform output is achieved then the output must be calibrated for the system to be used as test equipment. Since the radiance emitted from the monolithic array is created by flowing current through micro resistors, a radiometric approach is used to calibrate the differential output of the scene projector over its dynamic range. The focus of this paper is to describe the approach and results of implementing non-uniformity correction and calibration on a portable infrared scene projector.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1047880"},{"name":"The Two-Level-Turn-Model Fault-Tolerant Routing Scheme in Tori with Convex and Concave Faults","snippet":"A kind of routing scheme with the ability to tolerate the faults is necessary in the massively parallel multiprocessors. In this paper, we have proposed a kind of fault-tolerant routing scheme in the tori networks. The new routing scheme is called the two-level-turn-model routing scheme, which is based on our investigation of the fault-tolerant properties of the turn-model. Through employing two specific kinds of turn model, our routing scheme could tolerate the convex faults and the concave faults both with a few limitations to their shape. At most five virtual channels would be used to avoid the deadlock occurrence in the tori, no matter whether the fault regions are connected and no matter where the faults locate. Actually, if the fault regions encompass no physical boundary nodes in the tori, totally four virtual channels, each pair for each turn model, would be sufficient to preclude the occurrence of the deadlock. At last, the simulation shows the effectiveness of our scheme.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5070601"},{"name":"Slicing and dicing bugs in concurrent programs","snippet":"A lack of scalable verification tools for concurrent programs has not allowed concurrent software development to keep abreast with hardware trends in multi-core technologies. The growing complexity of modern concurrent systems necessitates the use of abstractions in order to verify all the expected behaviors of the system. Current abstraction refinement techniques are restricted to verifying mostly sequential and simpler concurrent programs. In this work, we present a novel incremental underapproximation technique that uses program slicing. Based on a reachability property, an initial backward slice for a single thread is generated. The information in the program slice is coupled with a concrete execution to drive the lone thread; generating an underapproximation of the program behavior space. If the target location is reached in the underapproximation, then we have an actual concrete trace. Otherwise, the initial single-thread slice is refined to include another thread that affects the reachability of the target location. In this case, the concrete execution only considers the two threads in the slice and preemption points between the threads only occur at locations in the slice. This refinement process is repeated until the target location is reached or is shown to be unreachable. Initial results indicate that the incremental technique can potentially allow the discovery of errors in larger systems using fewer resources and produce a better reduction in systems that are correct.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6062158"},{"name":"Cross-layer error resilience for robust systems","snippet":"A large class of robust electronic systems of the future must be designed to perform correctly despite hardware failures. In contrast, today's mainstream systems typically assume error-free hardware. Classical fault-tolerant computing techniques are too expensive for this purpose. This paper presents an overview of new techniques that can enable a sea change in the design of cost-effective robust systems. These techniques utilize globally-optimized cross-layer approaches, i.e., across device, circuit, architecture, runtime, and application layers, to overcome hardware failures.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5654129"},{"name":"Search-based Prediction of Fault-slip-through in Large Software Projects","snippet":"A large percentage of the cost of rework can be avoided by finding more faults earlier in a software testing process. Therefore, determination of which software testing phases to focus improvements work on, has considerable industrial interest. This paper evaluates the use of five different techniques, namely particle swarm optimization based artificial neural networks (PSO-ANN), artificial immune recognition systems (AIRS), gene expression programming (GEP), genetic programming (GP) and multiple regression (MR), for predicting the number of faults slipping through unit, function, integration and system testing phases. The objective is to quantify improvement potential in different testing phases by striving towards finding the right faults in the right phase. We have conducted an empirical study of two large projects from a telecommunication company developing mobile platforms and wireless semiconductors. The results are compared using simple residuals, goodness of fit and absolute relative error measures. They indicate that the four search-based techniques (PSO-ANN, AIRS, GEP, GP) perform better than multiple regression for predicting the fault-slip-through for each of the four testing phases. At the unit and function testing phases, AIRS and PSO-ANN performed better while GP performed better at integration and system testing phases. The study concludes that a variety of search-based techniques are applicable for predicting the improvement potential in different testing phases with GP showing more consistent performance across two of the four test phases.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5635180"},{"name":"Error visualization of tetrahedral subdivision approach for trilinear interpolation","snippet":"A linear interpolation scheme inside a tetrahedral cell often causes a large interpolation error when field values change drastically. However, the error has not been analysed and visualized thoroughly In order to understand the error distribution inside a tetrahedral cell, we propose two types of error norms, which are both an interpolation function error norm and a field data error norm. These error norms make it possible to compare the linear interpolation function with a trilinear interpolation function that has often been used in rectilinear grid cell, and visualize the error distribution by using iso-surface display","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=884539"},{"name":"Logic fault test simulation environment for IP core-based digital systems","snippet":"A logic fault test simulation environment for core-based digital systems is proposed in this paper. The simulation environment emulates a typical built-in self-test (BIST) environment with test pattern generator that sends its outputs to a circuit under test (CUT) and the output streams from the CUT are fed into a response data analyzer. The developed simulator is suitable for testing digital IP cores. The paper describes in details the test architecture and application of the logic fault simulator. Some partial simulation results on ISCAS 85 combinational and ISCAS 89 sequential benchmark circuits are provided.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5235951"},{"name":"Blocking vs. Non-Blocking Coordinated Checkpointing for Large-Scale Fault Tolerant MPI","snippet":"A long-term trend in high-performance computing is the increasing number of nodes in parallel computing platforms, which entails a higher failure probability. Fault programming environments should be used to guarantee the safe execution of critical applications. Research in fault tolerant MPI has led to the development of several fault tolerant MPI environments. Different approaches are being proposed using a variety of fault tolerant message passing protocols based on coordinated checkpointing or message logging. The most popular approach is with coordinated checkpointing. In the literature, two different concepts of coordinated checkpointing have been proposed: blocking and non-blocking. However they have never been compared quantitatively and their respective scalability remains unknown. The contribution of this paper is to provide the first comparison between these two approaches and a study of their scalability. We have implemented the two approaches within the MPICH environments and evaluate their performance using the NAS parallel benchmarks","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4090192"},{"name":"Induction Motor-Drive Systems with Fault Tolerant Inverter-Motor Capabilities","snippet":"A low-cost fault tolerant drive topology for low- speed applications such as \"self-healing\/limp-home\" needs for vehicles and propulsion systems, with capabilities for mitigating transistor open-circuit switch and short-circuit switch faults is presented in this paper. The present fault tolerant topology requires only minimum hardware modifications to the conventional off-the-shelf six-switch three-phase drive, with only the addition of electronic components such as triacs\/SCRs and fast-acting fuses. In addition, the present approach offers the potential of mitigating not only transistor switch faults but also drive related faults such as rectifier diode short-circuit fault or dc link capacitor fault. In this new approach, some of the drawbacks associated with the known fault mitigation techniques such as the need for accessibility to a motor neutral, overrating the motor to withstand higher fundamental rms current magnitudes above its rated rms level, the need for larger size dc link capacitors, or higher dc bus voltage, are overcome here using the present approach. Given in this paper is a complete set of simulation results that demonstrate the soundness and effectiveness of the present topology.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4270862"},{"name":"IP core logic fault test simulation environment","snippet":"A low-level logic fault test simulation environment for embedded systems directed specifically towards application-specific integrated circuits (ASICs) and intellectual property (IP) cores is proposed in the paper. The developed simulation environment emulates a typical builtin self-testing (BIST) architecture with automatic test pattern generator (ATPG) that sends its outputs to a circuit (core) under test (CUT) and the output streams from the CUT are fed into an output response analyzer (ORA). The paper delineates the development of the test architecture, test application and fault injection including the relevance of the logic fault simulator.in great details. Some results on simulation on specific IP cores designed using combinations from ISCAS 85 combinational and ISCAS 89 sequential benchmark circuits are provided as well for evaluation.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5488199"},{"name":"Surface Defects Inspection System Based on Machine Vision","snippet":"A machine vision based tinplate surface inspection system was developed. The system was composed of two parallel line scan CCD cameras, a special designed wide field illumination, which can overcome the vibration of tinplate, and a software based on SOM (Self-Organizing Feature Map) neural network. The images of tinplate were captured by cameras. All kinds of defects candidates such as pinholes, scallops, dust and scratches were found out, and their features can be extracted and selected from images. These candidates were distinguished by the SOM neural network to find out real defects. The inspection speed reached up to 1.4 m\/s, and the resolution was 0.1 mm, and recognition rate was 95.45%.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5631800"},{"name":"Research on real-time error measurement in curve grinding process based on machine vision","snippet":"A machine vision image measurement system for online monitoring of the wheel wear degree during the curve grinding process is designed and developed. The measurement apparatus and its principle of operation are introduced in detail. Real-time image of work piece and wheel in the grinding zone is gathered by CCD camera installed in the grinder. For the purpose of increasing the measurement precision, a new edge detection approach combining Zernike moments operator with Prewitt operator is proposed. The edge of the finished work piece is located with sub-pixel level accuracy, and then the machining error of the work piece is calculated on-line by comparing with the theoretical curve of the work piece. An application of its validity and the experimental results are also given. Experimental results demonstrate the proposed measurement method in this paper is effective, and its detection precision and results are reasonable.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4752079"},{"name":"Inspection system for detecting defects in a transistor using Artificial neural network (ANN)","snippet":"A machine vision system based on ANN for identification of defects occurred in transistor fabrication is presented in this paper. The developed intelligent system can identify commonly occurring errors in the transistor fabrication. The developed machine vision and ANN module is compared with the commercial MATLAB<sup><\/sup> software and found results were satisfactory. This work is broadly divided into four stages, namely intelligent inspection system, machine vision module, ANN module and Inspection expert system. In the first a system with a camera is developed to capture the various segments of the transistor. The second stage is the image processing stage, in this the captured bitmap format image of the transistor is filtered and its size is altered to an acceptable size for the developed ANN using Set Partitioning Hierarchical Tree (SPIHT). These modified data are given as input to the ANN in the third stage. Generalized ANN with Back propagation algorithm is used to inspect the transistor. The ANN is trained and the weight values are updated in such a way that the error in identification is the least possible. The output of ANN is the inspected report. The developed system is explained with a real time industrial application. Thus, the developed algorithms will solve most of the problems in identifying defects in a transistor.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5738806"},{"name":"Masquerade detection augmented with error analysis","snippet":"A masquerade attack, in which one user impersonates another, may be one of the most serious forms of computer abuse. Automatic discovery of masqueraders is sometimes undertaken by detecting significant departures from normal user behavior, as represented by a user profile formed from system audit data. A major obstacle for this type of research is the difficulty in obtaining such system audit data, largely due to privacy concerns. An immense contribution in this regard has been made by Schonlau et al., who have made available UNIX command-line data from 50+ users collected over a number of months. Most of the research in this area has made use of this dataset, so this paper takes as its point of departure the Schonlau et al. dataset and a recent series of experiments with this data framed by the same researchers . In extending that work with a new classification algorithm, a 56% improvement in masquerade detection was achieved at a corresponding false-alarm rate of 1.3%. In addition, encouraging results were obtained at a more realistic sequence length of 10 commands (as opposed to sequences of 100 commands used by Schonlau et al.). A detailed error analysis, based on an alternative configuration of the same data, reveals a serious flaw in this type of data which hinders masquerade detection and indicates some steps that need to be taken to improve future results. The error analysis also demonstrates the insights that can be gained by inspecting decision errors, instead of concentrating only on decision successes.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1282170"},{"name":"Fabric defect detection based on open source computer vision library OpenCV","snippet":"A method for fabric defect detection based on OpenCV with rich computer vision and image processing algorithms and functions is presented. Firstly, OpenCV image processing functions implement fabric image preprocessing. We use morphological opening and closing operations to segment image because of their blur defects. Secondly, seed filling algorithm is applied to connect broke lines to keep defect edge smoothing. Finally, the edge detection function is to complete accurate positioning defects. Experimental results under Borland C++ Builder 6.0 show that OpenCV based fabric defect detection methods are simple, high code integration, accurate defects positioning, which can be applied to develop real-time fabric defect detection system.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5555633"},{"name":"Detection and Correction of Lip-Sync Errors Using Audio and Video Fingerprints","snippet":"A method for measuring and maintaining time synchronization between an audio and video stream is described. Audio and video fingerprints are used to create a combined audio\/video synchronization signature (A\/V Sync Signature) at a known reference point. This signature is used at later points to measure audio\/video timing synchronization relative to the reference point. This method may be used, for example, to automatically detect and correct audio\/video synchronization (i.e. lip-sync) errors in broadcast systems and other applications.  Advantages of the method described over other existing methods include that it does not require modification of the audio or video signals, it can respond to dynamically changing synchronization errors, and it is designed to be robust to modifications of the audio\/video signals.  While the system requires data to be conveyed to the detection point, this data does not need to be synchronized with, or directly attached to, the audio or video streams. As this method uses fingerprints it also enables other fingerprinting applications within systems, such as content identification and verification. In addition, it may be used to maintain synchronization of other metadata associated with audio\/video streams.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7269123"},{"name":"A Method for Optimum Test Point Selection and Fault Diagnosis Strategy for BIT of Avionic System","snippet":"A method for optimum test point selection and the fault diagnosis strategy which is based on the fault message matrix and features of BIT is proposed. The fault message matrix is divided based on the weight of the test points The diagnosis strategy is determined using dividing the fault message matrix and the thought of detecting first and isolating next. Result shows that the optimum method is suitable for BIT to select the appropriate test points and fault diagnosis procedure. Besides, average numbers of test steps were reduced.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4960753"},{"name":"Crosstalk-Insensitive Method for Testing of Delay Faults in Interconnects Between Cores in SoCs","snippet":"A method for reliable measurement of interconnect delays is presented in the paper. The mode of test vectors generation never induces crosstalks. That is why the delay measurement is reliable. Also, minimization of ground bounce noises and reduction of power consumption during the test is an additional advantage. The presented method allows also localizing and identifying static faults of both stuck-at (SaX) and short types. The paper deals with the hardware that is necessary for implementing the method. The techniques for test data compression, that allow substantial reduction of data volume transferred between SoC and ATE, are also proposed.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4286213"},{"name":"Error correction using data hiding technique for JPEG2000 images","snippet":"A method of error correction for JPEG2000 images is proposed in this paper. The method uses the layer structure that is a feature of the JPEG2000 and an error correction code. The upper layers of the code stream are coded using an error correcting code, and the parity data are hidden in the lowest layer. The hidden data are used for error correction at the decoder. Several error correction codes with different strength are selected for the main header, packet headers, and bodies. Since the resulting code stream has the same data structure as a standard JPEG2000 code stream, it can be decoded with a general decoder. Simulation results demonstrated the effectiveness of the proposed method.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1247284"},{"name":"Zero frequency error locking of widely tunable lasers in high spectral efficiency systems using optical injection phase lock loops","snippet":"A method of locking widely tunable lasers with zero frequency error relative to supplied optical and microwave references despite lasers temperature variations was demonstrated. Locking was maintained when changing laser submount temperature from 18C to 23C, while channel spacing variations were kept under 1 Hz. Using a two OIPLL system, 10 Gbit\/s transmission at 18 GHz channel spacing was achieved.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1036542"},{"name":"A practical methodology for experimental fault injection to test complex network-based systems","snippet":"A methodology for dependability assessment of complex computer systems, such as fault tolerant grids, is presented in this paper. The methodology uses communication fault injection and was built by adapting a widely accepted approach for performance analysis. To demonstrate its applicability and usefulness, the methodology was applied to a third party grid platform using a fault injector we are developing. The paper reasons about the advantages of this methodology to perform fault injection campaigns in the prototype phase of a system.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4813788"},{"name":"A design of the low-pass filter using the novel microstrip defected ground structure","snippet":"A new defected ground structure (DGS) for the microstrip line is proposed in this paper. The proposed DGS unit structure can provide the bandgap characteristic in some frequency bands with only one or more unit lattices. The equivalent circuit for the proposed defected ground unit structure is derived by means of three-dimensional field analysis methods. The equivalent-circuit parameters are extracted by using a simple circuit analysis method. By employing the extracted parameters and circuit analysis theory, the bandgap effect for the provided defected ground unit structure can be explained. By using the derived and extracted equivalent circuit and parameters, the low-pass filters are designed and implemented. The experimental results show excellent agreement with theoretical results and the validity of the modeling method for the proposed defected ground unit structure","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=899965"},{"name":"Inverted defected ground structure for microstrip line filters reducing packaging complexity","snippet":"A new defected ground structure for microstrip line circuits was introduced by keeping the ground plane of the circuit fully metallized and etching the slots on the superstrate, which is directly lain on the top of the substrate. The metal of the superstrate is connected by via holes to the ground plane. The structure has the great advantage in reducing the packaging complexity, since it can be directly based on the carrier block without the need of machining a recessed region in it. Moreover, a higher Q-factor is obtained for this kind of structures. The low-pass filter based on this structure was designed, fabricated and measured. The DGS structure located on the superstrate provides the transmission zeros improving the steepness of the transmission characteristic and the attenuation in the stop-band. The filter insertion losses are better than 0.4 dB. The measured data fit well the results of MoM simulation.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4958684"},{"name":"Error concealment using affine transform for H.263 coded video transmissions","snippet":"A new error concealment method is proposed that uses motion estimation to consider actual motions, such as rotation, magnification, reduction, and parallel motion, in moving pictures. Since many videos include a variety of complex three-dimensional motions, the proposed method uses an affine transform to estimate the motion of lost data more accurately, thereby producing a higher peak signal-to-noise ratio value and better subjective video quality","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=907529"},{"name":"A fast and efficient H.264 error concealment technique based on coding modes","snippet":"A new error concealment technique based on the coding modes is proposed for H.264 video sequences. The motion-compensation modes (i.e., block-partitioning types) of surrounding macroblocks are employed to predict the mode of a lost macroblock. This adaptive selection mechanism is combined with a refined set of candidate motion vectors. Experimental results show that the proposed method, as compared to the technique used in the JM reference software, provides 1 to 2 dB gain in PSNR with only 50% increase in computation time.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5463091"},{"name":"Hierarchical defect-oriented fault simulation for digital circuits","snippet":"A new fault model is developed for estimating the coverage of physical defects in digital circuits for given test sets. Based on this model, a new hierarchical defect oriented fault simulation method is proposed. At the higher level simulation we use the functional fault model, at the lower level we use the defect\/fault relationships in the form of defect coverage table and the defect probabilities. A description and the experimental data are given about probabilistic analysis of a complex CMOS gate. Analysis of the quality of 100% stuck-at fault test sets for two benchmark circuits in covering physical defects like internal shorts, stuck-opens and stuck-ons. It has been shown that in the worst case a test with 100% stuck-at fault coverage may, have only 50% coverage for internal shorts in complex CMOS gates. It has been shown that classical test coverage calculation based on counting defects without taking into account the defect probabilities may lead to considerable overestimation of results","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=873781"},{"name":"New fault tolerant robotic central controller for space robot system based on ARM processor","snippet":"A new fault tolerant robotic central controller with dual processing modules is introduced. Each processing module is composed of 32 bit ARM RISC processor and other commercial-off-the-shelf (COTS) devices. As well as, a set of fault handling mechanisms is implemented in the robotic central controller, which can tolerate a single fault. The robotic central controller software based on VxWorks is organized around a set of processes that communicate between each other through a routing process. Considering the demanding of the extremely tight constraints on mass, volume, power consumption and space environmental conditions, the new fault tolerant robotic central controller has been developed. Its excellent data processing capability is enough to meet the space robot missions.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4608500"},{"name":"Class-based neural network method for fault location of large-scale analogue circuits","snippet":"A new method for fault diagnosis of large-scale analogue circuits based on the class concept is developed in this paper. A large analogue circuit is decomposed into blocks\/sub-circuits and the nodes between the blocks are classified into three classes. Only those sub-circuits related to the faulty class need to be treated. Node classification reduces the scope of search for faults, thus reduced after-test time. The proposed method is more suitable for real-time testing and can deal with both hard and soft faults. Tolerance effects are taken into account in the method. The class-based fault diagnosis principle and neural network based method are described in some details. Two non-trivial circuit examples are presented, showing that the proposed method is feasible.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1206417"},{"name":"A new approach to fault-tolerant wormhole routing for mesh-connected parallel computers","snippet":"A new method for fault-tolerant wormhole routing in arbitrary dimensional meshes is introduced. The method was motivated by certain routing requirements of an initial design of the Blue Gene supercomputer at IBM Research. The machine is organized as a three-dimensional mesh containing many thousands of nodes and the routing method should tolerate a few percent of the nodes being faulty. There has been much work on routing methods for meshes that route messages around faults or regions of faults. The new method is to declare certain nonfaulty nodes to be \"lambs.\" A lamb is used for routing but not processing, so a lamb is neither the source nor the destination of a message. The lambs are chosen so that every \"survivor node,\" a node that is neither faulty nor a lamb, can reach every survivor node by at most two rounds of dimension-ordered (such as e-cube) routing. An algorithm for finding a set of lambs is presented. The results of simulations on 2D and 3D meshes of various sizes with various numbers of random node faults are given. For example, on a 32  32  32 3D mesh with 3 percent random faults and using at most two rounds of e-cube routing for each message, the average number of lambs is less than 68, which is less than 7 percent of the number 983 of faults and less than 0.21 percent of the number 32,768 of nodes.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1268400"},{"name":"Unreliability tracing technique for system components based on the fault tree analysis","snippet":"A new method is developed for tracing the unreliability contributions of system components and recognizing the system weak parts using the fault tree analysis (FTA). The method is based on the minimum cut set (MCS) algorithm for evaluating the system reliability using the FTA and the proportional sharing principle (PSP). The fault tree can be expressed as MCSs and the system unreliability can be mathematically expressed by a certain terms of probability of occurrence of MCSs. A unreliability tracing (UT) principle is proposed for allocating the probability of each term to the basic events fairly and reasonably, and then the direct contribution relationship between the system unreliability and basic events can be established. The system UT sharing factors (UTSFs) are derived to easily identify the weakness parts in a system. The applicability of the proposed methods is illustrated by case studies of a simple system and a multiple-output power distribution system.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5528944"},{"name":"The discovery of the fault location in NIGS","snippet":"A new method is discovered for calculating the fault distance of the overhead line of the Neutral Indirect Grounded System (NIGS) in power distribution networks, in which the single phase to ground fault point or distance is difficult to detect, because the zero sequence current is in lower value. It is found that the information of the fault distance is kept in the zero sequence voltage vector which may be measured at the tail terminal of the questioned line by digging the data. Then an algorithm to calculate the fault location on the overhead lines is proposed by considering that the zero sequence voltage vector at the tail terminal. The value of the zero sequence voltage is determined by the fault location, and the phase angle also contains the distance traveled by the load current to the fault point. The system analysis for parameters is conducted for the NIGS by considering line is actual and by the two terminals' parameters.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5569691"},{"name":"Bearing Fault Diagnosis Based on Feature Weighted FCM Cluster Analysis","snippet":"A new method of fault diagnosis based on feature weighted FCM is presented. Feature-weight assigned to a feature indicates the importance of the feature. This paper shows that an appropriate assignment of feature-weight can improve the performance of fuzzy c-means clustering. Feature evaluation based on class separability criterion is discussed in this paper. Experiment shows that the algorithm is able to reliably recognize not only different fault categories but also fault severities. Therefore, it is a promising approach to fault diagnosis of rotating machinery.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4722953"},{"name":"Development of simulation model based on directed fault propagation graph","snippet":"A new method of simulation model is presented in this paper in order to deal with system based fault mode and effect analysis model in modern complex system with large structure. Directed fault propagation graph model based on fault influence degree is proposed and fault propagation model is put forward. With the definition of direct fault propagation influence degree and indirect fault propagation influence degree is introduced, the algorithm of propagation and search method for fault propagation model is discussed. Visualization simulation system based on directed fault propagation graph is developed with object oriented method according to the proposed fault analysis model. The Simulation system can used for fault propagation analysis and fault influence of exist complex system, simulation result can be validated and verified by control area network platform, the method is useful for fault diagnosis and analysis model in modern large complex system.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5620730"},{"name":"Application of genetic algorithms to pattern recognition of defects in GIS","snippet":"A computerized pattern recognition system based on the analysis of phase resolved partial discharge (PRPD) measurements, and utilizing genetic algorithms, is presented. The recognition system was trained to distinguish between basic types of defects appearing in gas-insulated system (GIS), such as voids in spacers, moving metallic particles, protrusions on electrodes, and floating electrodes. The classification of defects is based on 60 measurement parameters extracted from PRPD patterns. Classification of defects appearing in GIS installations is performed using the Bayes classifier combined with genetic algorithms and is compared to the performance of the other classifiers, including minimal-distance, percent score and polynomial classifiers. Tests with a reference database of more than 600 individual measurements collected during laboratory experiments gave satisfactory results of the classification process","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=841804"},{"name":"FADES: a fault emulation tool for fast dependability assessment","snippet":"A confident use of deep submicron VLSI systems requires the study of their behaviour in the presence of faults. Field-programmable gate arrays (FPGAs) are being used to conduct this study by means of fault injection in a very fast way. However, FPGA-based fault injection tools are mainly focused on classical faults like stuck-at and bit-flip, and do not cover fault models related to new semiconductor technologies like delay, pulse, stuck-open, short, open-line, bridging, and indetermination. Moreover, these tools usually require a deep fault injection background to use them. This paper presents FADES, a tool for the early and fast dependability evaluation of VLSI systems. FADES is able to inject the whole set of considered faults and also enables non-skilled users to assess their systems' dependability. The main advantages and drawbacks of FADES are reported, and some open challenges for further research are identified","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4042437"},{"name":"Single-stage Flyback converter for led driver with inductor voltage detection power factor correction","snippet":"A constant current output Flyback converter with power factor correction used for LED driving is presented in this paper. The inductor voltage detection method is applied and acquired the inductor voltage to generate the control signal. Based on this design principle, the inner loop of input current shaping is eliminating, the input voltage sensing and multiplier are also not necessary. The simulation and experimental results are provided to demonstrate the effectiveness of the control scheme based on the lab prototype boards. The output load condition is set from full load to light load, and the results show that the system drives the LEDs with high power factor.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5515433"},{"name":"Control chart of mean with low alpha error probability","snippet":"A control chart of adjustment center imbalance of process with low alpha error probability and stability to unknown distribution parameter is designed. At the heart of algorithm is hypothesis check criterion. According to results of current controlled parameter measurements at every step is made a calculation of line inclination value and is tested hypothesis of its equality to null. If we accept this hypothesis, we consider the current process to be disordered.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4897155"},{"name":"Superconducting Fault Current Limiter Application for Reduction of the Transformer Inrush Current: A Decision Scheme of the Optimal Insertion Resistance","snippet":"A conventional superconducting fault current limiter (SFCL) is usually only connected to a power system for fault current limitation. The study described in this paper, however, attempts to use the hybrid SFCL application to reduce the transformer inrush current. To accomplish this, this paper first suggests the concepts to expand the scope of the SFCL application in the power system. The power system operator should first determine the proper amount of current-limiting resistance (CLR) of the hybrid SFCL. Therefore, this paper suggests a decision scheme of the optimal insertion resistance in an SFCL application to reduce the transformer inrush current. This scheme and the SFCL model are implemented using the electromagnetic transient program (EMTP). We determine the optimal CLR by EMTP simulation, and this value is applied to model the SFCL by the EMTP. The simulation results show the validity and effectiveness of the suggested scheme and the ability of the SFCL to reduce the inrush current.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5482050"},{"name":"Detection and Diagnosis of Recurrent Faults in Software Systems by Invariant Analysis","snippet":"A correctly functioning enterprise-software system exhibits long-term, stable correlations between many of its monitoring metrics. Some of these correlations no longer hold when there is an error in the system, potentially enabling error detection and fault diagnosis. However, existing approaches are inefficient, requiring a large number of metrics to be monitored and ignoring the relative discriminative properties of different metric correlations. In enterprise-software systems, similar faults tend to reoccur. It is therefore possible to significantly improve existing correlation-analysis approaches by learning the effects of common recurrent faults on correlations. We present methods to determine the most significant correlations to track for efficient error detection, and the correlations that contribute the most to diagnosis accuracy. We apply machine learning to identify the relevant correlations, removing the need for manually configured correlation thresholds, as used in the prior approaches. We validate our work on a multi-tier enterprise-software system. We are able to detect and correctly diagnose 8 of 10 injected faults to within three possible causes, and to within two in 7 out of 8 cases. This compares favourably with the existing approaches whose diagnosis accuracy is 3 out of 10 to within 3 possible causes. We achieve a precision of at least 95%.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4708890"},{"name":"A Cost-Effective Dependable Microcontroller Architecture with Instruction-Level Rollback for Soft Error Recovery","snippet":"A cost-effective, dependable microcontroller architecture has been developed. To detect soft errors, we developed an electronic design automation (EDA) tool that generates optimized soft error-detecting logic circuits for flip-flops. After a soft error is detected, the error detection signal goes to a developed rollback control module (RCM), which resets the CPU and restores the CPU's register file from the backup register file using a rollback program routine. After the routine, the CPU restarts from the instruction executed before the soft error occurred. In addition, there is a developed error reset module (ERM) that can restore the RCM from soft errors. We also developed an error correction module (ECM) that corrects ECC errors in RAM after error detection with no delay overheads. Testing on a 32- bit RISC microcontroller and EEMBC benchmarks showed that the area overhead was under 59% and frequency overhead was under 9%. In a soft error injection simulation, the MTBF of random logic circuits, and the MTBF of RAM were 30 and 1.34 times longer, respectively, than those of the original microcontroller.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4272977"},{"name":"CRINet: A secure and fault-tolerant data collection scheme using 3-way forwarding and group key management in wireless sensor networks","snippet":"A critical security threat in a WSN is the compromising of sensor nodes. Not only can attackers use such vulnerability to eavesdrop on the dataflow, but could also inject bogus information into the network. However, most current secure data collection methods trade fault-tolerant ability for end-to-end protection, thus with poor performance. This work proposes CRINet, a secure and fault-tolerant data collection scheme with group key management mechanism. To achieve high reliability, sensing data would be transferred to the sink through multi-path. EBS is applied in CRINet for group key management in order to reduce re-key efforts. Simulation results demonstrate that CRINet scheme is superior in terms of data confidentiality and availability.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5068959"},{"name":"In-line wafer inspection data warehouse for automated defect limited yield analysis","snippet":"A data warehouse approach for the automation of process zone-by-zone defect limited yield analysis is presented in this paper. The system employs pre-calculation of adder defects extraction and clustered defects recognition, a newly developed wafer-wise defect record structure, and a graphical user interface purpose-designed for data selection navigation. Analysis time can be reduced to less than 1% of that of benchmarked conventional procedures","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=902570"},{"name":"Design of circular split-ring type Defected Ground Structure as elliptic filters","snippet":"A Defected Ground Structure in the metallic ground plane of a microstrip line is attractive solution for achieving finite pass band, rejection band and slow-wave characteristics. A split-ring shaped DGS structure has a high selectivity would be preferable owing to the demand for currently expanding communication systems within finite spectrum resources. First of all a split-ring type DGS unit slot is designed underneath a pair of T-shaped microstrip line. [1-4]. Thus just applying a T section at the upper plane of DGS unit a lowpass filter can be changed to a sharp response highpass filters.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5430619"},{"name":"Depth Image-Based Temporal Error Concealment for 3-D Video Transmission","snippet":"A depth image-based error concealment algorithm for 3-D video transmission is proposed, which utilizes the strong correlations between 2-D video and its corresponding the depth map. We first investigate the internal characteristics of the macroblock in the depth map, and then take advantage of these characteristics to recover accurately the lost motion vector for the corrupted blocks, with the joint consideration of the neighbor information and the corresponding depth. Experimental results show that the proposed method provides significant improvements in terms of both objective and subjective evaluations.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5308355"},{"name":"Operating MicroGrid Energy Storage Control during Network Faults","snippet":"A MicroGrid is expected to operate both as a sub system connected to the main grid or as an islanded system. However the provision of fault currents, in an islanded MicroGrid consisting only of micro-generation interfaced with relatively low-current power electronics, is a serious system protection issue. This paper presents the novel concept of using the central energy storage system (flywheel) as the main fault current source in islanded mode. The three-phase MicroGrid test rig used at University of Manchester, and the flywheel control system are described. The importance of accurate systems modeling of the whole microgrid and energy storage unit is shown. A fault study is carried out on the test rig and in PSCAD. The flywheel inverter system is shown to contribute enough fault current for a sufficient duration to cause the system protective device to clear the fault.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4304254"},{"name":"A cost-driven lithographic correction methodology based on off-the-shelf sizing tools","snippet":"A minimum feature sizes continue to shrink, patterned features have become significantly smaller than the wavelength of light used in optical lithography. As a result, the requirements for dimensional variation control, especially in critical dimension (CD) 3\/spl sigma\/, has become more stringent. To meet these requirements, resolution enhancement techniques (RET) such as optical proximity correction (OPC) and phase shift mask (PSM) technologies are applied. These approaches result in a substantial increase in mask costs and make the cost of ownership (COO) a key parameter in the comparison of lithography technologies. No concept of function is injected into the mask flow; that is, current OPC techniques are oblivious to the design intent, and the entire layout is corrected uniformly with the same effort. We propose a minimum cost of correction (MinCorr) methodology to determine the level of correction for each layout feature such that prescribed parametric yield is attained with minimum total RET cost. We highlight potential solutions to the MinCorr problem and give a simple mapping to traditional performance optimization. We conclude with experimental results showing that substantial RET costs may be saved while maintaining a given desired level of parametric yield.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1218771"},{"name":"CPN model for a Hierarchical Fault Tolerance Protocol for Mobile Agent systems","snippet":"A mobile agent (MA) is an autonomous and identifiable software process that travels through a network of heterogeneous machines and acts autonomously on behalf of the user. Improving the survivability of MA in presence of various faults is the major issue concerned with implementation of MA. This paper presents a hierarchical fault tolerance protocol (HFTP) for mobile agents, which can tolerate host failure, system failure as well as link failure by grouping the hosts within a network and rear guard based migration of MA in the global network. It also presents Colored Petri Net (CPN) based architectural modeling of HFTP, which includes systematic specification, design and implementation of components of the system. Various useful results have been drawn by simulation as well as data collector and monitoring tools. We also present a formal analysis of the protocol.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4772581"},{"name":"Torsional oscillations of the turbine-generator due to network faults","snippet":"A model of the electromechanical system, suitable for the analysis of torsional oscillations due to the power system's faults, is established. Results of an example of computer simulation of transient torsional torques in the shaft-line due to a three-phase fault and the subsequent fault clearing, as obtained by the model, are presented. Effect of chosen fault clearing time is discussed.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5606642"},{"name":"A model-based approach for fault-tolerant control","snippet":"A model-based controller architecture for fault-tolerant control (FTC) is presented in this paper. The controller architecture is based on the Youla-Jabr-Bongiorno-Kucera (YJBK) parameterization. The FTC architecture consists of two central parts, fault detection and isolation (FDI) part and a controller reconfiguration part. The theoretical basis for the architecture will be given followed by an investigation of the single parts in the architecture. At last, system interconnection will be considered with respect to the described controller architecture.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5675947"},{"name":"Sensor fault tolerant generic model control for nonlinear systems","snippet":"A modified Strong Tracking Filter (STF) is used to develop a new approach to sensor fault tolerant control. Generic Model Control (GMC) is used to control the nonlinear process while the process runs normally because of its robust control performance. If a fault occurs in the sensor, a sensor bias vector is then introduced to the output equation of the process model. The sensor bias vector is estimated on-line during every control period using the STF. The estimated sensor bias vector is used to develop a fault detection mechanism to supervise the sensors. When a sensor fault occurs, the conventional GMC is switched to a fault tolerant control scheme, which is, in essence, a state estimation and output prediction based GMC. The laboratory experimental results on a three-tank system demonstrate the effectiveness of the proposed Sensor Fault Tolerant Generic Model Control (SFTGMC) approach.","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6083282"},{"name":"Research of Remote Fault Diagnosis System Based on Multi-Agent","snippet":"A Multi-agent based Remote fault Diagnosis system is an important system for high speed and automation which can not only monitor the status of the remote device, but serve for the remote device. Remote Fault diagnosis system are vital aspects in automation process, in this sense, remote diagnosis systems should support decision-making tools, the enterprise thinking and flexibility. In this paper a kind of Remote Diagnosis System based on multi-agent is presented. This model is based on a generic framework using multi-agent systems. Specifically, this paper analyses the architecture of Remote Fault Diagnosis System and the collaboration mechanism between Agents. The method brought forward in the paper was generally applicable to a general fault diagnosis.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5629626"},{"name":"A Multiple Faults Test Generation Algorithm Based on Neural Networks and Chaotic Searching for Digital Circuits","snippet":"A multiple faults test generation algorithm based neural networks for digital circuits is proposed in this paper because the test generation for multiple faults in digital circuits is more difficult. This algorithm change multiple faults into single fault firstly and constructs the constraint network of the fault for the single fault circuit with method of neural networks. The test vectors for multiple faults in the original circuit can be obtained by solving the minimum of energy function of the constraint network for the fault with chaotic searching method. The experimental results on some international standard circuits demonstrate the feasibility of the algorithm.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5676989"},{"name":"Multiwave interaction analysis of a coaxial Bragg structure with a localized defect introduced in sinusoidal corrugations","snippet":"A multiwave interaction formulation is presented to investigate the effects of a localized defect on the reflective spectrum of a coaxial Bragg structure with sinusoidal corrugations. Good agreement has been achieved between the theoretical results obtained by the present formulation and those simulated by the software HFSS, which confirms the validity and the significance of the multiwave interaction formulation. It is found that, the localized defect creates defected eigenmodes within each reflective band gap of the initial standard Bragg structure, which the parameter can be controlled by the location of the localized defect.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5384209"},{"name":"An algorithm for dividing ambiguity sets for analog fault dictionary","snippet":"A new algorithm for dividing ambiguity sets based on the lowest error probability for analog fault dictionary is proposed. The problem of tolerance affecting diagnostic accuracy in analog circuits is discussed. A statistical approach is used to derive the probability distribution of the tolerances of the output signal characteristics both in the absence and in the presence of faults in the circuit. For example, in this paper, Monte Carlo technique has been applied for the analysis of tolerance. The lowest error probabilities are computed according to Bayesian strategy. Using the PSpice software package, a detailed simulation program was developed to implement the proposed technique. The simulation software was packaged and then integrated with a symbolic analysis program that divides the ambiguity sets and structure the software package for the analysis before testing in the fault dictionary. Furthermore, the proposed approach can be easily extended to select the testing nodes leading to the selection of optimized nodes for the analog fault diagnosis.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1187163"},{"name":"A Zero Module Current Obtaining Approach Based on Magnetic Induction for Single Phase Grounding Fault","snippet":"A new approach based on the magnetic field induction is presented to obtain transient zero module current for single phase grounding gault of overhead lines. The paper analyses the characteristics of magnetic field around the overhead lines and presents the magnetic field under the lines is proportional to the zero module current, and the zero module current can be measured by inducting the magnetic field. The paper proposes a zero-module current obtaining approach using a hall sensor to induct magnetic field, and elaborates the solution to the key issues in practical applications, at last simulation and experiment results demonstrate the feasibility of the approach.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5449339"},{"name":"Parametric fault trees with dynamic gates and repair boxes","snippet":"A new approach is proposed to include s-dependencies in fault tree (FT) models. With respect to previous techniques, the approach presented in this paper is based on two peculiar powerful features. First, adopting a parameterization technique, referred to as parametric FT (PFT), to fold equal subtrees (or basic events) in order to resort to a more compact FT representation. It is shown that parameterization can be conveniently adopted as well for dynamic gates. Second, PFT can be modularized and each module translated into a high level colored Petri net in the form of a stochastic well-formed net (SWN). SWN generate a lumped Markov chain and the saving in the dimension of the state space can be very substantial with respect to standard (non colored) Petri nets. Translation of PFT modules into SWN has proved to be very flexible, and various kinds of new dependencies can be easily accommodated. In order to exploit this flexibility a new primitive, called repair box, is introduced. A repair box, attached to an event, causes the starting of a repair activity of all the components that failed as the event occurs. In contrast to all the previous FT based models, the addition of repair boxes enables the approach to model cyclic behaviors. The proposed approach as dynamic repairable PFT (DRPFT) was referred to. A tool supporting DRPFT is briefly described and the tool is validated by analyzing a benchmark proposed recently in the literature for quantitative comparison [H. Zhu et al., 2001].","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1285491"},{"name":"Fault-accommodating thruster force allocation of an AUV considering thruster redundancy and saturation","snippet":"A new approach to the fault-accommodating allocation of thruster forces of an autonomous underwater vehicle (AUV) is investigated in this paper. This paper presents a framework that exploits the excess number of thrusters to accommodate thruster faults during operation. First, a redundancy resolution scheme is presented that considers the presence of an excess number of thrusters along with any thruster faults and determines the reference thruster forces to produce the desired motion. This framework is then extended to incorporate a dynamic state feedback technique to generate reference thruster forces that are within the saturation limit of each thruster. Results from both computer simulations and experiments are provided to demonstrate the viability of the proposed scheme","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=999650"},{"name":"Design and realization of a new compact branch-line coupler using defected ground structure","snippet":"A new compact branch-line directional coupler is proposed combined the T-model branch line coupler with defected ground structure (DGS). Using transmission theory, the parameter selection limit about the T-Model equivalent structure is discussed firstly. Then a T-model branch line coupler with DGS is proposed and optimum designed with simulation software. The measurement results show that the proposed coupler has the advantages as compact and well passband flatness.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4734818"},{"name":"Decoding of the (24, 12, 8) extended golay code up to four errors","snippet":"A new decoder is proposed to decode the (24, 12, 8) binary extended Golay code up to four errors. It consists of the conventional hard decoder for correcting up to three errors, the detection algorithm for four errors and the soft decoding for four errors. For a weight-4 error in a received 24-bit word, Method 1 or 2 is developed to determine all six possible error patterns. The emblematic probability value of each error pattern is then defined as the product of four individual bit-error probabilities corresponding to the locations of the four errors. The most likely one among these six error patterns is obtained by choosing the maximum of the emblematic probability values of all possible error patterns. Finally, simulation results of this decoder in additive white Gaussian noise show that at least 93% and 99% of weight-4 error patterns that occur are corrected if the two E<sub>b<\/sub>\/N<sub>0<\/sub> ratios are greater than 2 and 5 dB, respectively. Consequently, the proposed method can achieve a better percentage of successful decoding for four errors at variable signal-to-noise ratios than Lu et al.'s algorithm in software. However, the speed of the method is slower than Lu et al.'s algorithm.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4777677"},{"name":"Determining the Amount of Audio-Video Synchronization Errors Perceptible to the Average End-User","snippet":"<para> The Media and Acoustics Perception Lab (MAPL) designed a study to determine the minimum amount of audio-visual synchronization (a\/v sync) errors that can be detected by end-users. Lip synchronization is the most noticeable a\/v sync error, and was used as the testing stimuli to determine the perceptual threshold of audio leading errors. The results of the experiment determined that the average audio leading threshold for a\/v sync detection was 185.19 ms, with a standard deviation of 42.32 ms. This threshold determination of lip sync error (with audio leading) will be widely used for validation and verification infrastructures across the industry. By implementing an objective pass\/fail value into software, the system or network under test is held against criteria which were derived from a scientific subjective test. <\/para>","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4599253"},{"name":"Design and development of a 15 kV, 20 kA HTS fault current limiter","snippet":"A 15 kV class high temperature superconducting fault current limiter was developed as part of a Department of Energy Superconductivity Partnership Initiative (SPI) Phase II effort. This is an inductive\/electronic fault current limiter (FCL) that can double as a fast sub-cycle solid state breaker. The said device was shipped to Southern California Edison (SCE) Center Substation at Norwalk, CA from General Atomics on June 15, 1999. Preliminary high voltage and high current testing was conducted. The pre-commercial FCL unit houses three of the world's largest Bi-2223 coils (solenoids each with an outside diameter of 1 m and a coil length of 0.75 m), collaborated by GA and IGC. These coils will operate at 35 K and be able to carry a continuous DC current of 2000 A as well as an AC pulsed current of 9000 A. Detailed specification of the FCL device and a brief description of its various subsystems will be given. Finally, test results at Center Substation are summarized and future work outlined. This Phase II FCL device is important as it has the potential to become the first major commercial product for HTS power utility application.","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=828360"},{"name":"Automated fault data collection, analysis, and reporting","snippet":"A brief summary of the NERC Classification and Standards, NERC standards for Digital Fault Recorders and requirements for Automated Fault reporting are presented. This paper describes a method for meeting these requirements. A common architecture is proposed to implement an automated data collection tool utilizing the existing infrastructure. Two data transfer tools are described & compared. In addition to meeting NERC requirements for Disturbance Monitoring Equipment, the benefits of implementing an enterprise-wide Network Based Fault Data Collection and Analysis system are discussed.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5275547"},{"name":"Validation of guidance control software requirements specification for reliability and fault-tolerance","snippet":"A case study was performed to validate the integrity of a software requirements specification (SRS) for guidance control software (GCS) in terms of reliability and fault-tolerance. A partial verification of the GCS specification resulted. Two modeling formalisms were used to evaluate the SRS and to determine strategies for avoiding design defects and system failures. Z was applied first to detect and remove ambiguity from a part of the natural language based (NL-based) GCS SRS. Next, statecharts and activity-charts were constructed to visualize the Z description and make it executable. Using this formalism, the system behavior was assessed under normal and abnormal conditions. Faults were seeded into the model (i.e., an executable specification) to probe how the system would perform. The result of our analysis revealed that it is beneficial to construct a complete and consistent specification using this method (Z-to-statecharts). We discuss the significance of this approach, compare our work with similar studies, and propose approaches for improving fault tolerance. Our findings indicate that one can better understand the implications of the system requirements using Z-statecharts approach to facilitate their specification and analysis. Consequently, this approach can help to avoid the problems that result when incorrectly specified artifacts (i.e., in this case requirements) force corrective rework","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=981660"},{"name":"Analysis of Hyperion data with the FLAASH atmospheric correction algorithm","snippet":"A combination of good spatial and spectral resolution make visible to shortwave infrared spectral imaging from aircraft or spacecraft a highly valuable technology for remote sensing of the Earth's surface. Many applications require the elimination of atmospheric effects caused by molecular and particulate scattering; a process known as atmospheric correction, compensation, or removal. The Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes (FLAASH) atmospheric correction code derives its physics-based algorithm from the MODTRAN4 radiative transfer code. A new spectra; recalibration algorithm, which has been incorporated into FLAASH, is described. Results from processing Hyperion data with FLAASH are discussed.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1293688"},{"name":"Comparison of the main types of fault-tolerant electrical drives used in vehicle applications","snippet":"A comparative study of several fault-tolerant electrical drives is presented in this paper. As the application is concerned, the authorspsila attention was oriented towards the vehicle transportation. Thus, the main electrical drives under study are: the induction, the switched reluctance and the permanent magnet synchronous machine, respectively. The present work will explore the aforementioned drivespsila capabilities in terms of fault-tolerant operation. The authors present a substantial study for the fault-tolerant issue in electrical drives by using finite element method (FEM). During this numerical analysis many phenomenon will be emphasized and, coming together with some tests, final conclusions will be depicted.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4581328"},{"name":"A new protection algorithm for EHV transmission line based on singularity detection of fault transient voltage","snippet":"A new non-unit voltage protection algorithm for extra high voltage (EHV) transmission lines is presented in this paper. The singularity of fault transient voltage signals in one terminal is utilized to discriminate clearly between internal and external faults. Lipschitz exponent (LE) is obtained from wavelet modulus maximum to detect singularity of signals. A typical 500 kV EHV transmission system has been simulated by ATP to evaluate the scheme. The simulation results show that this scheme is capable of providing correct responses under various system configurations and fault conditions","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1627268"},{"name":"Position and speed sensorless control for PMSM drive using direct position error estimation","snippet":"A new position and speed sensorless control approach is proposed for permanent magnet synchronous motor (PMSM) drives. The controller directly computes an error for the estimated rotor position and adjusts the speed according to this error. The derivation of the position error equation and an idea for eliminating the differential terms, are presented. The proposed approach is applied to a vector controlled PMSM AC drive and phase locked loop (PLL) control is employed for speed adjustment. Several simulations are carried out. The proposed control scheme is verified by experiments using a 3.7 kW salient pole PMSM","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=975540"},{"name":"A Method for the Automatic Selection of Test Frequencies in Analog Fault Diagnosis","snippet":"A new procedure for the selection of test frequencies in the parametric fault diagnosis of analog circuits is presented. It is based on the evaluation of algebraic indices, as the condition number and the norm of the inverse, of a sensitivity matrix of the circuit under test. This matrix is obtained starting from the testability analysis of the circuit. A test index (T.I.) that permits the selection of the set of frequencies that better leads to locating parametric faults in analog circuits is defined. By exploiting symbolic analysis techniques, a program that implements the proposed procedure has been developed. It yields the requested set of frequencies by means of an optimization procedure based on a genetic algorithm that minimizes the T.I. Examples of the application of the proposed procedure are also included.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4389103"},{"name":"A Gibbs-sampler approach to estimate the number of faults in a system using capture-recapture sampling [software reliability]","snippet":"A new recapture debugging model is suggested to estimate the number of faults in a system, , and the failure intensity of each fault, . The Gibbs sampler and the Metropolis algorithm are used in this inference procedure. A numerical illustration suggests a notable improvement on the estimation of  and  compared with that of a removal debugging model","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=922486"},{"name":"Improved unsynchronized two-end algorithm for locating faults in power transmission lines","snippet":"A new two-end algorithm for locating faults in a single power transmission line is presented. Presence of the extra link between the line terminals is taken into account. A distance to fault is determined from unsynchronized measurements of voltages from both line ends and with additional, limited use of currents. Measured currents are utilized under the condition that they come from current transformers, which are not saturated. As a result of that, certain improvement of fault location - when comparing to the other known methods - is achieved. The delivered new algorithm has been tested with the fault data obtained from versatile ATP-EMTP simulations. The sample examples and results of fault location accuracy evaluation are reported and discussed in the paper.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1304371"},{"name":"Triggered vacuum switch-based fault current limiter","snippet":"A new type of fault current limiter (FCL) is proposed based on the triggered vacuum switch (TVS). The TVS-based FCL (TFCL) is mainly composed of a capacitor, a current-limiting reactor connected with the capacitor in series, and a TVS connected with the capacitor in parallel. With TVS at the off or on state, the whole TFCL behaves as conventional series compensation or fault current limitation, respectively. Compared with other types of FCLs, such as superconductivity, thyristor\/GTO-based FCL, TFCL is distinguished by its characteristics, such as high capacity, loss free, and low price. The digital simulation and prototype experiment based on the LC resonant test circuit show that it is feasible to develop TFCL","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=893342"},{"name":"Fault Tolerant Non-trivial Repeating Pattern Discovering for Music Data","snippet":"A non-trivial repeating pattern is commonly used in analyzing the repeated part of a music object and looking for the theme. Non-trivial repeating patterns exclude those patterns included in other longer patterns such that they can reduce the redundancy and speedup music search. So far, existing approaches discover a repeating pattern in such a way that the sequence of notes in a music object appears more than once in exactly matching. If we allow the similar sequences with partial different notes also being a repeating pattern, it can reduce the number of repeating patterns and construct more efficient music indexes. The more accurate music theme also could be analyzed. Therefore, in this paper, we propose a fault-tolerant non-trivial repeating pattern discovering technique. The experimental results show that our approach can not only reduce the number of non-trivial repeating patterns but also improve the hit ratios of queries for music databases","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1651981"},{"name":"Semiconductor production schedule check and correction technique through mobile agent system","snippet":"A novel agent system that can check and correct machine schedules in semiconductor production was proposed The system consists of stationary machine agents that control their machine schedules, mobile agents that move between the machines, machine schedule files described in XML, and a time buffer stage where lots wait to keep the machine schedules. The real-time scheduling system proceeds according to the rules of mobile agent generation, machine scheduling, and machine and mobile agents collaboration. The system was confirmed through the computer simulation, supposing a small-scaled semiconductor production line","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1513371"},{"name":"A new algorithm of improving fault location based on SVM","snippet":"A fault location algorithm using estimated line parameters is provided in this paper. The characteristic of this algorithm is using estimated line parameters. And the influence of the line parameters is eliminated. Support vector machines theory is used to estimate transmission line parameters, which is a nonlinear black box modeling problem. The historical data is used as training sample. EMTP simulation shows that this method notability improves the accuracy of fault location.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1364842"},{"name":"Modelling and analysing fault propagation in safety-related systems","snippet":"A formal specification for analysing and implementing multiple fault diagnosis software is proposed in this paper. The specification computes all potential fault sources that correspond to a set of triggered alarms for a safety-related system, or part of a system. The detection of faults occurring in a safety-related system is a fundamental function that needs to be addressed efficiently. Safety monitors for fault diagnosis have been extensively studied in areas such as aircraft systems and chemical industries. With the introduction of intelligent sensors, diagnosis results are made available to monitoring systems and operators. For complex systems composed of thousands of components and sensors, the diagnosis of multiple faults and the computational burden of processing test results are substantial. This paper addresses the multiple fault diagnosis problem for zero-time propagation using a fault propagation graph. Components represented as nodes in a fault propagation graph are allocated with alarms. When faults occur and are propagated some of these alarms are triggered. The allocation of alarms to nodes is based on a severity analysis performed using a form of failure mode and effect analysis on components in the system.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1270740"},{"name":"Gate level fault diagnosis in scan-based BIST","snippet":"A gate level, automated fault diagnosis scheme is proposed for scan-based BIST designs. The proposed scheme utilizes both fault capturing scan chain information and failing test vector information and enables location identification of single stuck-at faults to a neighborhood of a few gates through set operations on small pass\/fail dictionaries. The proposed scheme is applicable to multiple stuck-at faults and bridging faults as well. The practical applicability of the suggested ideas is confirmed through numerous experimental runs on all three fault models","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=998301"},{"name":"Grid Connection to Stand Alone Transitions of Slip Ring Induction Generator During Grid Faults","snippet":"A grid connected power generation systems based on the superior controllers of an active and reactive power are useless during a grid failures like grid short-circuit or line braking. Therefore the change of operation mode from grid connection to stand alone allows for uninterruptible supply of a selected part of grid connected load. However, in the stand alone operation mode the superior controllers should provide fixed amplitude and frequency of the generated voltage in spite of the load nature. Moreover, a soft transition from grid connection mode to stand alone operation requires that, the mains outage detection method must be applied. A grid voltage recovery requires change of the generator operational mode from stand alone to grid connection. However, the protection of a load from rapid change of the supply voltage phase is necessary. This may be achieved by synchronization of the generated and grid voltages and controllable soft connection of the generator to the grid. The paper presents the transients of controllable soft connection and disconnection to the grid of the variable speed doubly fed induction generator (DFIG) power system. A description of the mains outage detection methods for the DFIG is based on the grid voltage amplitude and frequency measurement and comparison with a standard values. Also an angle controller, between generated and grid voltages, for synchronization process is described. The short description of the sensorless direct voltage control of the autonomous doubly fed induction generator (ADFIG) is presented. All the presented methods are proved based on PSIM simulation software and in a laboratorial conditions and the oscillograms with a test results are presented in the paper. A 2.2 kW slip-ring induction machine was applied as a generator and 3.5 kW DC motor was used as a primary mover to speed adjusting. A switching and sampling frequencies are equal to 8 kHz. For filtering the switching frequency distortions in the output volta- - ge external capacitances equal to 21 muF per phase are connected to the stator. The control algorithm is implemented in a DSP controller build on a floating point ADSP-21061 with an Altera\/FPGA support","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4778033"},{"name":"Jump Simulation: A Technique for Fast and Precise Scan Chain Fault Diagnosis","snippet":"A diagnosis technique is presented to locate seven types of single faults in scan chains, including stuck-at faults and timing faults. This technique implements the Jump Simulation, a novel parallel simulation technique, to quickly search for the upper and lower bounds of the fault. Regardless of the scan chain length, Jump Simulation packs multiple simulations into one so the simulation time is short. In addition, Jump Simulation tightens the bounds by observing the primary outputs and scan outputs of good chains, which are ignored by most previous techniques. Experiments on ISCAS'89 benchmark circuits show that, on the average, only three failing patterns are needed to locate faults within ten scan cells. The proposed technique is still very effective when failure data is truncated due to limited ATE memory","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4079337"},{"name":"Diagnosis of single stuck-at faults and multiple timing faults in scan chains","snippet":"A diagnosis technique to locate single stuck-at faults and multiple timing faults in scan chains is presented. This technique applies single excitation (SE) patterns, in which only one bit is flipped in the presence of multiple faults. With SE patterns, the problem of unknown values in scan chains is eliminated. The diagnosis result is therefore deterministic, not probabilistic. In addition to the first fault, this technique also diagnoses the remaining timing faults by applying multiple excitation patterns. Experiments on benchmark circuits show that average diagnosis resolutions are mostly less than five, even for the tenth fault in the scan chain.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1458787"},{"name":"Armature Fault Diagnostics of a Commutator Motor using the Frequency Response Analysis","snippet":"A diagnostic method for armatures of commutator motors using the frequency response analysis method (FRA) is presented in this paper. The proposed method can, beside the detection of the fault itself, recognize and categorize the fault types. In a first step, the magnetic and capacitive couplings of the armature single components are analyzed. Thereafter, the position-dependent impedance curves of healthy and faulty armatures are obtained by measurements and used for the detection of winding faults. Taking as criteria the main resonant points of the frequency and position dependent armature impedance curve, a diagnostic method that considers three different fault types is developed.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4393108"},{"name":"Calculation of transverse voltages of communication lines induced by the fault current of power system","snippet":"A double-line model is presented to calculate the transverse voltage of communication lines induced by the fault current of power lines. By dividing the communication line into several fictitious segments, a chain composed by the coupling P1-type circuit with distributed source is formed. The enhanced node voltage analysis (ENVA) is also developed in order to evaluate such a kind of model. The ENVA cuts the number of nodes down greatly because of treating the active and coupling impedance branches as a whole. In addition, the transverse voltages in time domain can be obtained easily from those calculated in frequency domain by means of fast Fourier transform. The numerical examples prove the validity and efficiency of the method by comparison with analytical results. The model is of significance to the design and the rights-of-way selection of power lines and communication lines.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1047617"},{"name":"Coverage method for FPGA fault logic blocks by spares","snippet":"A fault coverage method for digital system-on-chip by means of traversal the logic block matrix to repair the FPGA components is proposed. A method enables to obtain the solution in the form of quasioptimal coverage for all faulty blocks by minimum number of spare tiles. A choice one of two traversal strategies for rows or columns of a logic block matrix on the basis of the structurization criteria, which determine a number of faulty blocks, reduced to the unit modified matrix of rows or columns is realized.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5742089"},{"name":"Design and Construction of a Magnetic Fault Current Limiter","snippet":"A fault current limiter using permanent magnets has been designed and its performance simulated using a two-dimensional dimensional time-stepping finite-element method incorporating a model of hysteresis for hard magnetic materials.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4123609"},{"name":"Geometric and shading correction for images of printed materials using boundary","snippet":"A novel technique that uses boundary interpolation to correct geometric distortion and shading artifacts present in images of printed materials is presented. Unlike existing techniques, our algorithm can simultaneously correct a variety of geometric distortions, including skew, fold distortion, binder curl, and combinations of these. In addition, the same interpolation framework can be used to estimate the intrinsic illumination component of the distorted image to correct shading artifacts. We detail our algorithm for geometric and shading correction and demonstrate its usefulness on real-world and synthetic data.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1632208"},{"name":"Optimized design of a low-pass filter using defected ground structures","snippet":"A novel three-pole low-pass filter is designed using low impedance microstrip line and one DGS section in this paper. An equivalent circuit model of a defected ground structure (DGS) is applied to study the characteristics of DGS. Parameters of the model are extracted from the EM simulation results by matching it to a one-pole low-pass filter. The lumped element values of the low-pass filter are optimized in circuit simulator by applying the circuit model of DGS. It is demonstrated that the filter can provide a sharp rate of attenuation in the stop-band as predicted. To further verify this method, a filter using DGS is fabricated measured. The comparison between simulation and measurement confirms the effectiveness of the proposed method.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1606298"},{"name":"Automobile engine fault diagnosis using neural network","snippet":"A number of diagnostic systems for vehicle maintenance\/repair have been developed in recent years. These systems are employed for diagnosing variety of faults in the vehicle and are available at service level. We have made an attempt to design a diagnostic system for detection of faults based on neural network. The system developed is based on a fault table for the engine. Such a diagnostic module is aimed at increasing the utility of the system","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=948707"},{"name":"A comparative analysis of network dependability, fault-tolerance, reliability, security, and survivability","snippet":"A number of qualitative and quantitative terms are used to describe the performance of what has come to be known as information systems, networks or infrastructures. However, some of these terms either have overlapping meanings or contain ambiguities in their definitions presenting problems to those who attempt a rigorous evaluation of the performance of such systems. The phenomenon arises because the wide range of disciplines covered by the term information technology have developed their own distinct terminologies. This paper presents a systematic approach for determining common and complementary characteristics of five widely-used concepts, dependability, fault-tolerance, reliability, security, and survivability. The approach consists of comparing definitions, attributes, and evaluation measures for each of the five concepts and developing corresponding relations. Removing redundancies and clarifying ambiguities will help the mapping of broad user-specified requirements into objective performance parameters for analyzing and designing information infrastructures.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5039586"},{"name":"Packet-level iterative errors-and-erasures decoding for SFH spread-spectrum communications with Reed-Solomon codes and differential encoding","snippet":"A packet-level iterative detection technique that employs errors-and-erasures decoding has been described previously for SFH communications using Reed-Solomon coding. The technique enhances the performance of the SFH system in intersymbol-interference channels with only a minimal increase in complexity over one-shot errors-and-erasures decoding. In this paper, the performance of iterative EE decoding is considered for a SFH system with differentially encoded transmissions. It is shown that the use of differential encoding improves the performance of packet-level iterative detection in an AWGN channel with only a modest increase in detection complexity, and it also improves the performance in an intersymbol-interference channel in many instances. The packet size, the target probability of error, and the channel impulse response are considered, and the effect of each on the performance gain and the complexity is examined","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1606060"},{"name":"Decorrelating compensation scheme for coefficient errors of a filter bank parallel A\/D converter","snippet":"A parallel A\/D conversion scheme with a filter bank for low-IF receivers is presented. The analysis filters of the filter bank divide the frequency components of the received signal, and achieve parallel A\/D conversion. Therefore, the required conversion rates and the resolution of the A\/D converters can be reduced and the receiver can demodulate wideband signals. As the analysis filters consist of analog components, their coefficients include errors. These errors cause mutual interference between signals in orthogonal frequencies. In order to remove this interference, a decorrelating compensation scheme is proposed.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1040416"},{"name":"Pattern recognition-a technique for induction machines rotor fault detection \"eccentricity and broken bar fault\"","snippet":"A pattern recognition technique based on Bayes minimum error classifier is developed to detect broken rotor bar faults and static eccentricity in induction motors at the steady state. The proposed algorithm uses stator currents as input without any other sensors. First, rotor speed is estimated from stator currents, then appropriate features are extracted. The produced feature vector is normalized and fed to the trained Bayes minimum error classifier to determine if motor is healthy or has incipient faults (broken bar fault, static eccentricity or both). Only number of poles and rotor slots are needed as pre-knowledge information. Theoretical approach together with experimental results derived from a 3 hp AC induction motor show the strength of this method. In order to cover many different motor load conditions data are derived from 10% to 130% of the rated load for both a healthy induction motor and an induction motor with a rotor having 4 broken bars and\/or static eccentricity.","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=955745"},{"name":"Response space construction for neural error correction","snippet":"A physiological neuron model that incorporates the recognized prototype of an inhibitory synapse was analyzed in terms of the effects of isolated inhibitory post-synaptic potentials on its ongoing behavior. The nonstationary, transient activity resulting from these perturbations cannot be analyzed in terms of motion on some attractor (because of long-duration aftereffects) nor linearized (as the perturbations are large). Instead, results suggest that changes in the value of either of the system's slow state variables may be used to construct a global response space, within which all attractors and nonstationary behaviors exist.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1381189"},{"name":"NoC Interface for fault-tolerant Message-Passing communication on Multiprocessor SoC platform","snippet":"A prevalent design paradigm in electronic systems design is the usage of multiple programmable processors on general purpose Multiprocessor System-on-Chip (MPSoC) platforms where processors and other sub-systems communicate through communication infrastructures called Network-on-Chip (NoC). This paper presents a new approach to a NoC Interface (NI) called Micronswitch Interface (MSI) designed for message-passing communication with a light-weight Micron Message-Passing (MMP) protocol on Micronmesh MPSoC platform. The operation of the MSI Hardware (HW) and Software (SW) are tightly coupled with that of the MMP protocol in order to improve communication performance. The MSI provides mechanisms for efficient buffer management and fault-tolerant communication which will be necessary for reliable and efficient operation of the MPSoCs. Performance analyses show that the MSI is also able to produce a good throughput and latency.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5397848"},{"name":"Extended fault modeling used in the space shuttle PRA","snippet":"A probabilistic risk assessment (PRA) has been completed for the space shuttle with NASA sponsorship and involvement. This current space shuttle PRA is an advancement over past PRAs conducted for the space shuttle in the technical approaches utilized and in the direct involvement of the NASA centers and prime contractors. One of the technical advancements is the extended fault modeling techniques used. A significant portion of the data collected by NASA for the space shuttle consists of faults, which are not yet failures but have the potential of becoming failures if not corrected. This fault data consists of leaks, cracks, material anomalies, and debonding faults. Detailed, quantitative fault models were developed for the space shuttle PRA which involved assessing the severity of the fault, detection effectiveness, recurrence control effectiveness, and mission-initiation potential. Each of these attributes was transformed into a quantitative weight to provide a systematic estimate of the probability of the fault becoming a failure in a mission. Using the methodology developed, mission failure probabilities were estimated from collected fault data. The methodology is an application of counter-factual theory and defect modeling which produces consistent estimates of failure rates from fault rates. Software was developed to analyze all the relevant fault data collected for given types of faults in given systems. The software allowed the PRA to be linked to NASA's fault databases. This also allows the PRA to be updated as new fault data is collected. This fault modeling and its implementation with FRAS was an important part of the space shuttle PRA.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1285479"},{"name":"Teaching the Art of Fault Diagnosis in Electronics by a Virtual Learning Environment","snippet":"A virtual learning environment (VLE) to improve understanding of simple faul tfinding was created from a series of Web pages, an online quiz with automated marking, and a local-area-network-based simulator. It was tested on 57 first-year students (in 2002) and 69 students in 2003, taking a module in engineering design in electrical engineering in which a battery charger was designed and constructed. The results indicate that there was better than 100% improvement in the number of working battery chargers in both tested years. In addition, the students who used the VLE produced more working chargers and were better able to identify circuit blocks than those that did not. The learning approach is described by the adaptive character of thought cited in the present paper.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1495644"},{"name":"Wavelet neural network method for fault diagnosis of push-pull circuits","snippet":"A wavelet neural network method for fault diagnosis of push-pull circuits is presented. Firstly, output voltage signals under faulty conditions are obtained with simulation. Then wavelet coefficients of output voltage signals are gained by Daubechies wavelet decomposition, and faulty feature vectors are extracted from coefficients. After training the networks by faulty feature vectors, the wavelet neural networks model of the circuit fault diagnosis system is built. The simulation result shows the fault diagnosis method of the push-pull circuits with wavelet neural network is effective.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1527517"},{"name":"Do stack traces help developers fix bugs?","snippet":"A widely shared belief in the software engineering community is that stack traces are much sought after by developers to support them in debugging. But limited empirical evidence is available to confirm the value of stack traces to developers. In this paper, we seek to provide such evidence by conducting an empirical study on the usage of stack traces by developers from the ECLIPSE project. Our results provide strong evidence to this effect and also throws light on some of the patterns in bug fixing using stack traces. We expect the findings of our study to further emphasize the importance of adding stack traces to bug reports and that in the future, software vendors will provide more support in their products to help general users make such information available when filing bug reports.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5463280"},{"name":"An Edge-Adaptive Block Matching Algorithm for Error Concealment","snippet":"A widely-used block matching algorithm (BMA) for error concealment may suffer from the deteriorated quality of a concealed block that includes multiple objects in different motion directions. This paper proposes an edge-adaptive BMA that decomposes a damaged 16 times 16 macroblock (MB) into four 8 times 8 blocks and conceals the 8 times 8 blocks together only when they belong to the same object. The edge-adaptive BMA detects edges on MB boundaries and uses the number and positions of the edges to determine which 8 times 8 blocks belong to the same object. The proposed algorithm improves PSNR by an average of 0.27 dB compared with the existing BMA for error concealment.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4284603"},{"name":"A fault-tolerant protocol for energy-efficient permutation routing in wireless networks","snippet":"A wireless network (WN) is a distributed system where each node is a small hand-held commodity device called a station. Wireless sensor networks have received increasing interest in recent years due to their usage in monitoring and data collection in a wide variety of environments like remote geographic locations, industrial plants, toxic locations, or even office buildings. Two of the most important issues related to a WN are their energy constraints and their potential for developing faults. A station is usually powered by a battery which cannot be recharged while on a mission. Hence, any protocol run by a WN should be energy-efficient. Moreover, it is possible that all stations deployed as part of a WN may not work perfectly. Hence, any protocol designed for a WN should work well even when some of the stations are faulty. The permutation routing problem is an abstraction of many routing problems in a wireless network. In an instance of the permutation routing problem, each of the p-stations in the network is the sender and recipient of n\/p packets. The task is to route the packets to their correct destinations. We consider the permutation routing problem in a single-hop wireless network, where each station is within the transmission range of all other stations. We design a protocol for permutation routing on a WN which is both energy efficient and fault tolerant. We present both theoretical estimates and extensive simulation results to show that our protocol is efficient in terms of energy expenditure at each node even when some of the nodes are faulty. Moreover, we show that our protocol is also efficient for the unbalanced permutation routing problem when each station is the sender and recipient of an unequal number of packets.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1514420"},{"name":"Architectural and Behavioral Modeling with AADL for Fault Tolerant Embedded Systems","snippet":"AADL is an architecture description language intended for model-based engineering of high-integrity systems. The AADL Behavior Annex is an extension allowing the refinement of behavioral aspects described through AADL. When implementing Distributed Real-time Embedded system, fault tolerance concerns are integrated by applying replication patterns. We considered a simplified design of the primary backup replication pattern to express the modeling capabilities of AADL and its annex. Our contribution intends to give accurate description of the synchronization mechanisms integrated in this example.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5479572"},{"name":"ATE applied into fault modeling and fault diagnosis of AC servo motor PWM driver system","snippet":"AC servo motor PWM driver system (including power module, power PWM driver board, cable, motor and photoelectric encoder\/decoder) is a key sub-system of semiconductor assembly and packaging equipment. Aimed at its high fault rate, in this document we build the fault models of system based on PWM (Pulse Width Modulation) voltage, controller command and position feedback, find the test method of main faults, use ATE idea and method to perform test requirement analysis, resource allocation and driver program design, open the closed loop to decouple the system into LRUs (Line Replaceable Units) and locate the fault LRU(s) combining open loop and closed loop, on-line and off-line test. It shows that the fault modeling and ATE diagnosis above is successful by lots of experiments and test verification and has been applied into machine monitoring and fault diagnosis efficiently and effectively.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1609184"},{"name":"Dynamic Behavior of DFIG-Based Wind Turbines during Grid Faults","snippet":"According to grid codes issued by utilities, tripping of wind turbines following grid faults is not allowed. Besides, to provide voltage support to the grid mandatory reactive current supply is necessary. To enable wind turbines ride-through low voltage periods special protection measures have to be implemented. In this paper the behavior of DFIG based wind turbines during grid faults is discussed and elucidated using simulation results. It is shown that with properly designed crowbar and DC-link chopper even zero voltage ride-through is possible.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4239307"},{"name":"An Initial Study on the Bug Report Duplication Problem","snippet":"According to recent work, duplicate bug report entries in bug tracking systems impact negatively on software maintenance and evolution productivity due to, among other factors, the increased time spent on report analysis and validation, what in some cases take over 20 minutes. Therefore, a considerable amount of time is lost mainly with duplicate bug report analysis. This work presents an initial characterization study using data from bug trackers from private and open source projects, in order to understand the possible factors that cause bug report duplication and its impact on software development.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5714448"},{"name":"A self-correcting active pixel sensor using hardware and software correction","snippet":"Active pixel sensor (APS) CMOS technology reduces the cost and power consumption of digital imaging applications. We present a highly reliable system for the production of high-quality images in harsh environments. The system is based on a fault-tolerant architecture that effectively combines hardware redundancy in the APS cells and software correction techniques.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1363709"},{"name":"A model-based approach to adding autonomic capabilities to network fault management system","snippet":"Adding autonomic capabilities to network management systems provides great promise in delivering high QoS while lowering operation and maintenance cost. In this paper, we present a model-based approach to adding autonomic capabilities to a fault management system for cellular networks. We propose the use of modeling techniques to specify software failures and their dispositions at the model level for the target system. This facilitates the deployment of a control loop for adding autonomic capabilities into the system architecture, which include self-monitoring, self-healing, and self-adjusting. Our case study on the intelligent network fault management system illustrates the proposed approach by adding and deploying these autonomic capabilities derived from self-model specifications, to mitigate the risk of specified failures and maintain the level of healthiness of the system, dynamically and effectively.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4575232"},{"name":"Dispersion-Error Optimized ADI FDTD","snippet":"ADI-FDTD method is efficient in solving fine RF\/microwave structures due to its unconditionally stable characteristics. However, it suffers from large dispersions with the increase of time steps. In this paper, an error-minimized ADI-FDTD method is proposed that is less dispersive as compared to the conventional ADI-FDTD method. It is formulated in such a way that no extra memory or simulation time is required in its computations. It is still unconditionally stable but with much less dispersion errors. Numerical examples are presented to demonstrate its efficiency and accuracy","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4014850"},{"name":"Fault-tolerant multimedia communication networks with QoS-based checkpoint protocol","snippet":"Advanced computer and network technologies have lead to the development of computer networks. Here, an application is realized by multiple processes located on multiple computers connected to a communication network. Each process computes and communicates with other processes by exchanging messages through communication channels. Mission-critical applications are required to be executed fault-tolerantly. This paper proposes novel consistency of global checkpoints in multimedia communication networks. Unlike the conventional consistency, it allows for processes to take local checkpoints during communication events and to lose a part of a message in recovery. In addition, we show a checkpoint protocol based on the proposed consistency. The checkpoint protocol is nonblocking for supporting time-constrained applications. In addition, it is QoS-based where a QoS parameter is global consistency","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=953675"},{"name":"Exploring Fine-Grained Fault Tolerance for Nanotechnology Devices With the Recursive NanoBox Processor Grid","snippet":"Advanced molecular nanotechnology devices are predicted to have exceedingly high transient fault rates and large numbers of inherent device defects compared to conventional CMOS devices. We describe and evaluate the Recursive NanoBox Processor Grid as an application specific, fault-tolerant, parallel computing system designed for fabrication with unreliable nanotechnology devices. In this study we construct hardware description language models of a NanoBox Processor cell and evaluate the effectiveness of our recursive fault masking approach in the presence of random errors. Our analysis shows that complex circuits constructed with encoded lookup tables can operate correctly despite 2% of the nodes being in error. The circuits operate partially correct with up to 4% of the nodes being in error","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1695958"},{"name":"A General QoS Error Detection and Diagnosis Framework for Accountable SOA","snippet":"Accountability is a composite measure for different but related quality aspects. To be able to ensure accountability in practice, it is required to define specific quality attributes of accountability, and metrics for each quality attribute. In this paper, we propose a quality detection and diagnosis framework for the service accountability. We first identify types of quality attributes which are essential to manage QoS in accountability framework. We then present a detection and diagnosis model for problematic situations in services system. In this model, we design situation link representing dependencies among quality attributes, and provide information to detect and diagnose problems and their root causes. Based on the model, we propose an integrated model-based and case-based diagnosis method using the situation link.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4690621"},{"name":"Dealing with dormant faults in an embedded fault-tolerant computer system","snippet":"Accumulation of dormant faults is a potential threat in a fault tolerant system, especially because most often fault tolerance is based on the single-fault assumption. We investigate this threat by the example of an automotive steer-by-wire application based on the Time-Triggered Architecture (TTA). By means of a Markov model we illustrate that the effect of fault dormancy can degrade the MTTF of a system by several orders of magnitude. We study potential remedies, of which transparent online testing proves to be the most powerful one, while taking a hot spare offline temporarily to test it provides a more feasible solution, though with tight constraints regarding the test duration.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1260601"},{"name":"Event-based motion correction in PET transmission measurements with a rotating point source","snippet":"Accurate attenuation correction is important for quantitative positron emission tomography (PET) imaging. In PET transmission measurement using external rotating radioactive sources, object motion during the transmission scan can affect measured attenuation correction factors (ACFs), causing incorrect radiotracer distribution or artefacts in reconstructed PET images. Therefore a motion correction method for PET transmission data could be very useful. In this paper we report a compensation method for rigid body motion in PET transmission measurement, in which transmission data are motion-corrected event-by-event, based on known motion, to ensure that events that traverse the same path through the object are recorded on the same LOR. After motion correction, events detected on different LORs may be recorded on the same transmission LOR. To ensure that the corresponding blank LOR records events from the same combination of contributing LORs, the list mode blank data are spatially transformed event-by-event based on the same motion information. The proposed method has been verified in phantom studies with continuous motion.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5874331"},{"name":"Correcting Base-Assignment Errors in Repeat Regions of Shotgun Assembly","snippet":"Accurate base-assignment in repeat regions of a whole genome shotgun assembly is an unsolved problem. Since reads in repeat regions cannot be easily attributed to a unique location in the genome, current assemblers may place these reads arbitrarily. As a result, the base-assignment error rate in repeats is likely to be much higher than that in the rest of the genome. We developed an iterative algorithm, EULER-AIR, that is able to correct base-assignment errors in finished genome sequences in public databases. The Wolbachia genome is among the best finished genomes. Using this genome project as an example, we demonstrated that EULER-AIR can 1) discover and correct base-assignment errors, 2) provide accurate read assignments, 3) utilize finishing reads for accurate base-assignment, and 4) provide guidance for designing finishing experiments. In the genome of Wolbachia, EULER-AIR found 16 positions with ambiguous base-assignment and two positions with erroneous bases. Besides Wolbachia, many other genome sequencing projects have significantly fewer finishing reads and, hence, are likely to contain more base-assignment errors in repeats. We demonstrate that EULER-AIR is a software tool that can be used to find and correct base-assignment errors in a genome assembly project","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4104459"},{"name":"Gabor transform based fault locator for transmission lines","snippet":"Accurate estimation of current and voltage transient parameter is critical for efficient and accurate fault location computation. In this paper, a new fault location scheme is proposed for transmission systems using Gabor transform for signal processing purposes instead of the conventional Fourier methods. The transform is distinctive with more accurate performance specially when dealing with some certain circumstances such as sudden signal changes, dc decaying, non integer harmonics and non stationary quantities for fault signals. For a better extraction of Gabor coefficients, a dedicated artificial neural network is employed. The contribution of this new transform to power system fault location is evaluated through various simulation tests using the Electromagnetic Transient Program EMTP. Simulation results show the potential of the proposed transform in accurate estimation of phasors for more accurate fault location estimation.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5372370"},{"name":"Point defects in quartz crystals and their radiation response - a review [quartz resonator applications]","snippet":"A short review of Al-related point deflects and their radiation effects is presented. These defects exhibit spectroscopic signals which are monitored by a variety of experimental techniques. This discussion is useful to prospective researchers in the area of precision quartz resonators for frequency control in aerospace applications. Irradiation of quartz crystals at 77 K before and after irradiation at 300 K coupled with sweeping can be used for estimating the role of various point defects for their contribution in estimating the frequency offsets in quartz crystals in a radiation environment.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1418540"},{"name":"Preserving non-programmers' motivation with error-prevention and debugging support tools","snippet":"A significant challenge in teaching programming to disadvantaged populations is preserving learners' motivation and confidence. Because programming requires such a diverse set of skills and knowledge, the first steps in learning to program can be highly error-prone, and can quickly exhaust whatever attention learners are willing to give to a programming task. Our approach to preserving learners' motivation is to design highly integrated support tools to prevent the errors they would otherwise make. In this paper, the results of a recent study on programming errors are summarized, and many novel error-preventing tools are proposed.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1260245"},{"name":"On the 2-Adic Complexity and the k-Error 2 -Adic Complexity of Periodic Binary Sequences","snippet":"A significant difference between the linear complexity and the 2-adic complexity of periodic binary sequences is pointed out in this correspondence. Based on this observation, we present the concept of the symmetric 2-adic complexity of periodic binary sequences. The expected value of the 2-adic complexity is determined, and a lower bound on the expected value of the symmetric 2-adic complexity of periodic binary sequences is derived. We study the variance of the 2-adic complexity of periodic binary sequences, and the exact value for it is given. Because the k-adic complexity of periodic binary sequences is unstable, we present the concepts of the <i>kappa<\/i>-error 2-adic complexity and the k-error symmetric 2-adic complexity, and lower bounds on them are also derived. In particular, we give tighter upper and lower bounds for the minimum k-adic complexity of l-sequences by substituting two symbols within one period.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4439852"},{"name":"Symbolic verification and error prediction methodology","snippet":"A SIMD platform provides higher computing power by executing multiple data simultaneously, but this feature is making the design and verification harder. This paper aims at helping designer with an efficient methodology for verifying and evaluating the performance of a SIMD PLX platform design. Using Mathematica, a computer algebra system (CAS), and reverse engineering techniques, the correctness of and errors accumulated in running multimedia applications on this PLX platform can be precisely evaluated, which would not be easily done without human interventions before. The proposed methodology can be easily automated and adapted to other SIMD platforms.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4545458"},{"name":"Distance estimation technique for single line-to-ground faults in a radial distribution system","snippet":"A simple yet powerful algorithm to estimate the distance to a single line-to-ground fault on a distribution feeder is proposed. The algorithm is implemented in a power monitor instrument and the estimation of distance is made within the instrument itself. The algorithm is designed to work where the only available data to the instrument are a single point measurement taken at the substation and the positive and zero-sequence impedance of the primary feeder. The single point measurement consists of three-phase voltage and current waveforms. Network topology data are not available to the algorithm. The new technique accommodates computational power and data constraints while maintaining adequate accuracy of the measurements","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=847216"},{"name":"A novel economical single stage battery charger with power factor correction","snippet":"A single stage AC-DC topology with power factor correction is proposed for battery charger applications. Desired features for battery charger such as low cost, fast charging, charge profile programmability, high efficiency and high reliability are fully achieved by means of proposed solution. Additionally, its multiphase operation configuration provides easy power scaling. The proposed approach is superior to conventional ferro-resonant regulation widely used for EV (electrical vehicle) charger applications. It is especially suitable to low cost and high power applications. The feasibility and practical value of the proposed approach are verified by the experimental results from a 1 kW product prototype.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1179300"},{"name":"Reducing Corrective Maintenance Effort Considering Module's History","snippet":"A software package evolves in time through various maintenance release steps whose effectiveness depends mainly on the number of faults left in the modules. The testing phase is therefore critical to discover these faults. The purpose of this paper is to show a criterion to estimate an optimal repartition of available testing time among software modules in a maintenance release. In order to achieve this objective we have used fault prediction techniques based both on classical complexity metrics and an additional, innovative factor related to the modules age in terms of release. This method can actually diminish corrective maintenance effort, while assuring a high reliability for the delivered software.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1402136"},{"name":"Low-cost, software-based self-test methodologies for performance faults in processor control subsystems","snippet":"A software-based testing methodology for processor control subsystems, targeting hard-to-test performance faults in high-end embedded and general-purpose processors, is presented. An algorithm for directly controlling, using the instruction-set architecture only, the branch-prediction logic, a representative example of the class of processor control subsystems particularly prone to such performance faults, is outlined. Experimental results confirm the viability of the proposed methodology as a low-cost and effective answer to the problem of hard-to-test performance faults in processor architectures","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=929769"},{"name":"A Novel Protecting Method for Induction Motor Against Faults Due to Voltage Unbalance and Single Phasing","snippet":"A system is designed and implemented to make protection for faults due to single phasing, voltage unbalance and under voltage in a 3-ph induction motor. In the system, three potential transformers with transformation ratio 400\/5V are connected to each phase of induction motor. The sampling circuit is realized such that the low AC signals taken from transformer's secondary windings are converted into DC values. Control process is implemented using Microsoft PIC 16F877 microcontroller. Sampled DC values are transmitted to microcontroller using analog to digital converter unit. Measured values are continuously compared with reference values by means of software. When a voltage unbalance, under voltage or single phasing are sensed, the system opens the normally closed contactor by activating the 12 V, 10 A, DC relay and thus cuts the power supply to the induction motor. The corresponding fault is displayed in the seven segment LED display unit and alerts the operator. Trip and reset delays prevent nuisance tripping due to rapidly fluctuating power line conditions.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4460176"},{"name":"Research and Application of Fault-Tolerance Based on Watershed Model Grid Platform","snippet":"A systematic scheme to form the watershed computational platform was developed based on lightweight Grid technique in this paper. The scheme that takes advantage of widely deployed local network makes full use of the non dedicated distributed computing resources. To overcome the vagary of overall system, MPICH-T a trust model based fault tolerant model was adopted, and the checkpoint based on pessimistic log can ensure that process repeats in single node and task migration on multi-nodes. and the transplant of system is guaranteed on the watershed model Grid platform, lastly several experiments were made on this platform and the results show that this platform has better performance though has a slightly time delay and the fault-tolerance mechanism based on MPICHT model is a nice choice suiting to the watershed model Grid platform.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4722282"},{"name":"A fault-tolerant architectural approach for dependable systems","snippet":"A system's structure enables it to generate its intended behavior from its components' behavior. A well-structured system simplifies relationships among components, which can increase dependability. With software systems, the architecture is an abstraction of the structure. Architectural reasoning about dependability has become increasingly important because emerging applications are increasingly complex. We've developed an architectural approach for effectively representing and analyzing fault-tolerant software systems. The proposed solution relies on exception handling to tolerate faults associated with component and connector failures, architectural mismatches, and configuration faults. Our approach, a specialization of the peer-to-peer architectural style, hides inside the architectural elements the complexities of exception handling and propagation. Our goal is to improve a system's overall reliability and availability by making it tolerant of nonmalicious faults.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1605182"},{"name":"Self-organizing maps for automatic fault detection in a vehicle cooling system","snippet":"A telematic based system for enabling automatic fault detection of a population of vehicles is proposed. To avoid sending huge amounts of data over the telematics gateway, the idea is to use low-dimensional representations of sensor values in sub-systems in a vehicle. These low-dimensional representations are then compared between similar systems in a fleet. If a representation in a vehicle is found to deviate from the group of systems in the fleet, then the vehicle is labeled for diagnostics for that subsystem. The idea is demonstrated on the engine coolant system and it is shown how this self-organizing approach can detect varying levels of clogged radiator.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4670481"},{"name":"Comparison of Voltage and Flux Modulation Schemes of StatComs Regarding Transformer Saturation During Fault Recovery","snippet":"A transformer might be driven into saturation by faults in the connected system. In a transmission system with a static synchronous compensator and transformers connected at the point of common coupling, different modulation schemes utilized by the voltage source converter influence the transformer saturation in different ways. This paper compares the saturation effect during fault recovery when two alternative modulation schemes are utilized: voltage modulation and flux modulation. The comparison shows that utilization of flux modulation scheme tends to soften the saturation problem during fault recovery. However, this is achieved at a higher converter transient peak current.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4624567"},{"name":"Implementation of Web-Based Fault Diagnosis Using Improved Fuzzy Petri Nets","snippet":"According to the current application and maintenance situation of numerical control equipment (NCE), a novel remote fault diagnosis expert system is designed to prevent fault occurrence and quicken the recovering process by online real-time monitoring the working state of NCEs. The article addresses the overall framework and relevant application technology of fault diagnosis system (FDS) and emphasizes on the establishment of fuzzy expert system (FES). Improved fuzzy Petri nets (FPNs) model and concurrent reasoning algorithm are applied to handle the fuzziness and concurrency of fault and inadequate and uncertain information. Utilization of simple matrix operation to realize complicated reasoning process that simplifies the diagnostic reasoning decision-making process. Meanwhile, it can be realized easily by computer programming. Finally, a practical fault instance is presented to demonstrate the feasibility and validity of this method.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5359014"},{"name":"Study of online fault diagnosis for distributed substation based on Petri nets","snippet":"According to the distributed substation protection configuration and the principle of fault-clearance, a new model of fault diagnosis based on Petri net is proposed in this paper. In Petri net diagnosis model, all kinds of fault have a specific token which make it easily and clearly to find the fault location and understand the sequence of the fault events. The diagnostic is implemented by solving some matrix equations, which has a fast computational speed and a definite diagnostic result. The approach proposed is particular suitable for substation fault online diagnosis. Specially, in this model, the differential protection of the transformer and the bus bars are concerned as well as over current protection.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5589414"},{"name":"A fault analysis and design consideration of pulsed-power supply for high-power laser","snippet":"According to the requirements of driving flashlamps, the design of a pulsed-power supply (PPS), based on capacitors as energy storage elements, is presented. Special consideration is given to some possible faults such as capacitor internal short-circuit, bus bar breakdown to ground, flashlamp sudden short or break (open circuit), and closing switch restrike in the preionization branch. These faults were analyzed in detail, and both fault current and voltage waveforms are shown through circuit simulation. Based on the analysis and computation undertaken, the pulsed-power system design and protection requirements are proposed. The preliminary experiments undertaken after circuit simulation demonstrated that the design of the PPS met the project requirements.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1197339"},{"name":"Laser Spectrum Measurement and Correction Based on Virtual Instrument Techniques","snippet":"According to the requirements of laser beams' spectrum measurement and correction, the laser spectrum measurement constructed with WDS4A and WDS4C raster and based on virtual instruments (Vis) technique are provided. The detailed methods and applied programs based on techniques of Graphical programming for instrumentations platform Lab VIEW are also introduced. After the description to the principles and the construction of the laser spectrum measure system, the policies to software and modules of the system are carefully discussed, including software structure and system driver configuration, raster control module, spectrum energy measure module, and spectrum calibration module. Meanwhile, the details to realize the laser spectrum correction methods and the raster control for the test are delivered. At last, measure data of the CO<sub>2<\/sub> laser spectrums based on VI techniques in practical implement of different environment are given. Practical examples indicate that, by the use of VI technique, the precision is excelled to plusmn0.015 mum as well as the automation levels have been improved.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4350999"},{"name":"Immunevonics: Avionics fault tolerance inspired by the biology system","snippet":"A novel approach to Advanced Avionics fault tolerance is proposed that takes inspiration from the intersection of human immune system and nervous system as a method of fault recognition and removal. The human body's defenses-the immune system, the sense of pain, and the healing processes-could serve as a conceptual model for high-confidence systems in advance avionics fault tolerance. In this paper we propose a hybrid fault tolerance instrument that suitable for Advanced Avionics system, by use distribute `body' immune system and central `nervous' system management principle. Through an artificial immune algorithm the proposed Advanced Avionics `body' immune system will learn to differentiate between acceptable and abnormal states and transitions within the `immunized' system. Potential faults can then be flagged and suitable recovery methods invoked to return the system to a safe state. When the `body' immune system can not handle the fault then a `pain' message being reported to Operating System health monitor system (OS-HM) and central `nervous' system will tolerant the fault through a three layered Preconscious health monitor and conscious fault handling strategy.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5406480"},{"name":"A novel approach to fault diagnostics and prognostics","snippet":"A novel fault diagnostics and prognostics algorithm based on hidden Markov model (HMM) is proposed. The algorithm combines fault diagnostics and prognostics in a unified framework. The algorithm has been fully tested by using experimental data from a rotating shift testbed in our laboratory.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1241660"},{"name":"A novel fault injection method for system verification based on FPGA boundary scan architecture","snippet":"A novel fault injection (a.k.a. fault insertion) method to facilitate the development of high quality system test is presented in this paper In this method, we utilize the existing boundary scan (BS) architecture of an FPGA to inject a hardware fault condition at any pin of the FPGA on a circuit board. Existing user-defined instructions of most FPGA BS architectures and the newly proposed design of their corresponding user-defined scan registers (USRs) constitute the proposed fault injection architecture. No new instruction, and no modification of the existing test access port (TAP) controller and BS registers are required. In addition, it is possible to reconfigure where and what type of faults to be injected asynchronously via the BS architecture while the system is online. Although the proposed method incurs at least additional delay through a multiplexer on the pin where a fault is injected, the programmability of an FPGA enables us to add fault injection logic only to where fault injection function is desired. Hence, area overhead and performance impact can be significantly reduced.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1041847"},{"name":"Development on surface defect holes inspection based on image recognition","snippet":"A novel method of surface defect holes inspection was proposed based on both 2D and 3D image processing & recognition. In this method, the first step is to detect the holes in the binary image converted by the 3D image which is scanned by a 3D laser scanner, and the second step is to confirm the defect holes by dimension calculation using the data of the scanned 3D image. The software is developed in MATLAB.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5695748"},{"name":"Rate control for low delay H.264\/AVC transmission over channels with burst error","snippet":"A rate control approach is proposed to deal with the low delay H.264\/AVC video transmission over channels with burst errors by applying stochastic optimization technique. Based on the exponential rate-distortion and the linear variance prediction model, the one pass rate control algorithm will take into account the channel state and round trip delay, and make an immediate decision on the optimal rate allocation for the video frame. Simulation results show that for different end to end delay constraints and round trip delay, the number of lost frames is significantly reduced, and the average reconstruction peek signal to noise ratio is improved by 0.5-1.6dB, compared with the reference rate control scheme [ARA, 01]","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4105643"},{"name":"Non-inductive variable reactor design and computer simulation of rectifier type superconducting fault current limiter","snippet":"A rectifier type superconducting fault current limiter with noninductive reactor has been proposed by the authors. The concept behind this SFCL is that the high impedance generated during superconducting to normal state of the trigger coil limits the fault current. In the hybrid bridge circuit of the SFCL, two superconducting coils: a trigger coil and a limiting coil are connected in anti-parallel. Both the coils are magnetically coupled with each other and could have the same value of self inductance so that they can share the line current equally. At fault time when the trigger coil current reaches a certain level, the trigger coil changes from superconducting state to normal state. This super to normal transition of the trigger coil changes the current ratio of the coils and therefore the flux inside the reactor is no longer zero. So, the equivalent impedance of both the coils is increased and limits the fault current. We have carried out computer simulation using PSCAD\/EMTDC and observed the results. Both the simulation and preliminary experiment shows good results. The advantage of using hybrid bridge circuit is that the SFCL can also be used as circuit breaker.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1440066"},{"name":"State-of-the-art, single-phase, active power-factor-correction techniques for high-power applications - an overview","snippet":"A review of high-performance, state-of-the-art, active power-factor-correction (PFC) techniques for high-power, single-phase applications is presented. The merits and limitations of several PFC techniques that are used in today's network-server and telecom power supplies to maximize their conversion efficiencies are discussed. These techniques include various zero-voltage-switching and zero-current-switching, active-snubber approaches employed to reduce reverse-recovery-related switching losses, as well as techniques for the minimization of the conduction losses. Finally, the effect of recent advancements in semiconductor technology, primarily silicon-carbide technology, on the performance and design considerations of PFC converters is discussed.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1435681"},{"name":"Automated detection of injected faults in a differential equation solver","snippet":"Analysis of logical relationships between inputs and outputs of a computational system can significantly reduce the test execution effort via minimizing the number of required test cases. Unfortunately, the available specification documents are often insufficient to build a complete and reliable model of the tested system. In this paper, we demonstrate the use of a data mining method, called Info-Fuzzy Network (IFN), which can automatically induce logical dependencies from execution data of a stable software version, construct a set of non-redundant test cases, and identify faulty outcomes in new, potentially faulty releases of the same system. The proposed approach is applied to the Unstructured Mesh Finite Element Solver (UMFES) which is a general finite element program for solving 2D elliptic partial differential equations. Experimental results demonstrate the capability of the IFN-based testing methodology to detect several kinds of faults injected in the code of this sophisticated application.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1281751"},{"name":"A dynamic fault localization algorithm using digraph","snippet":"Analyzed here is a dynamic learning fault localization algorithm based on directed graph fault propagation model and feedback control. Input and output of the algorithm are named as fault and hypothesis respectively. Because of the complexity and uncertainty of fault and symptom, it's difficult to accurately model the relationship of them in probabilistic fault localization. Fault localization algorithm depends on the prior specified model, and the parameter and structure of model is approximate correct and often differ from the real situation. So we propose DMCA+ algorithm which has 3 features: reduce the requirement for accuracy of initial conditions; statistically learn to automatically adapt the probability distribution of fault occurrence while localizing fault; generalize the MCA+ algorithm of no feedback. The feedback learning is similar with proportional adjusting of PID control, but increment is sensitive to detection rate because little increment adjusts output too slowly and big will result in a large number of error hypotheses. The simulation results show the validity and efficiency of dynamic learning under complex network. In order to promote detection rate, optimizing measures are also discussed.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5212305"},{"name":"Implementation of e-beam proximity effect correction using linear programming techniques for the fabrication of asymmetric bow-tie antennas","snippet":"Antenna-coupled tunnel junction diodes have recently been offering great advantages for IR and Terahertz detection applications. Fabrication has been a major constraint in our ability to field these devices. The first obstacle is the relatively small size of the antenna. As the length of the wave to be detected gets smaller, the size of the antenna shrinks according to the A\/4 rule. This eliminates the use of traditional photolithographic fabrication techniques, which fails in the nanometer geometry range. For this reason, e-beam lithographic technique is used. The second challenge appears in the fabrication of the tunnel junction. The tunnel junction part of the device is formed by sandwiching an insulation layer in between two conductor antenna parts. Previously, many fabrication techniques were offered for the vertical conductor-insulator-conductor (CIC) structures where two metal layers overlap each other forming a tunnel junction vertical to the antenna surface. However, planar CIC structures have become more popular because they enable the surface plasmon excitement across the tunnel junction barrier. The fabrication of planar tunnel junction requires the patterning of a nano-size gap that will enable the tunneling of the electrons in between two conductor antenna wings. At this critical location, e-beam proximity effect (pixel-to-pixel beam interactions) becomes a very important issue to be addressed in order to create a nanometer-range accuracy gap.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5378016"},{"name":"Integrated fault-tolerant multicast and anycast routing algorithms","snippet":"Anycast is a new communication service defined in IPv6 (Internet Protocol Version 6 for the next generation). An anycast message is the one that should be delivered to the `nearest' member in a group of designated recipients. Anycast and multicast mechanisms may be integrated to provide better services. A group of replicated (or mirrored) servers that provides anycast service may also provide multicast services and needs multicast to consistently update, whereas anycast routing may help a multicast request to reach the `nearest' member in a multicast group. A novel integration routing protocol is presented for both multicast and anycast messages communications in the Internet. The protocol is composed of four algorithms: (1) dynamic anycast routing algorithm for efficient transmission of anycast messages over the Internet to a group of servers. (2) integrated anycast routing with core-based tree technique based on multicast routing algorithms taking advantage of short delay, high throughput and load sharing. (3) Fault-tolerant algorithms for both anycast and multicast routing using backup paths restoring techniques. The performance figures have demonstrated the benefits of anycast routing in reducing end-to-end packet delay, and attaining load balance and fault-tolerance for multicast","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=870981"},{"name":"Context-Aware Adaptive Applications: Fault Patterns and Their Automated Identification","snippet":"Applications running on mobile devices are intensely context-aware and adaptive. Streams of context values continuously drive these applications, making them very powerful but, at the same time, susceptible to undesired configurations. Such configurations are not easily exposed by existing validation techniques, thereby leading to new analysis and testing challenges. In this paper, we address some of these challenges by defining and applying a new model of adaptive behavior called an Adaptation Finite-State Machine (A-FSM) to enable the detection of faults caused by both erroneous adaptation logic and asynchronous updating of context information, with the latter leading to inconsistencies between the external physical context and its internal representation within an application. We identify a number of adaptation fault patterns, each describing a class of faulty behaviors. Finally, we describe three classes of algorithms to detect such faults automatically via analysis of the A-FSM. We evaluate our approach and the trade-offs between the classes of algorithms on a set of synthetically generated Context-Aware Adaptive Applications (CAAAs) and on a simple but realistic application in which a cell phone's configuration profile changes automatically as a result of changes to the user's location, speed, and surrounding environment. Our evaluation describes the faults our algorithms are able to detect and compares the algorithms in terms of their performance and storage requirements.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5432224"},{"name":"Edge Defect Detection in Ceramic Tile Based on Boundary Analysis Using Fuzzy Thresholding and Radon Transform","snippet":"Applying image processing technology and machine vision in industry have had significant development in recent decade. Tile and ceramic industry was not excluded form this matter. By using image processing techniques in production line of this industry, it is possible to detect surface defections such as edge defections, cracks and coloring defections. Surface defection is the most common type of defection in tile and ceramic industry. In this article a new and simple approach for detecting surface defections using Fuzzy thersholding and morphological operand and Radon transforms as well as boundary analysis is introduced. The available algorithm in the articles has good ability in determining number and place of defections and displaying them. Compared with results of other methods, the used algorithm in this article has better results on actual data base, in presence of camera noise and environmental lighting effects.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4775686"},{"name":"Design of arc fault detection system based on CAN bus","snippet":"Arc fault detection system (AFDS) is a device intended to protect the power system against the arc fault that may cause fire. When there is an arc fault, the scale of fault current is lower than the initialization of most of the protection devices installed in the lowers, hence AFDS is an effective device to detect the arc fault successfully and interrupt the circuit in time. The characteristics of arc, how it ignites, and what losses it may cause were discussed. The basic structure of AFDS and the primary principles of that AFDS detect arc fault were proposed. For efficiently realizing the global optimization, the single function problem of conventional arc fault circuit interrupter (AFCI) was solved by setting up a detection system. Composed with detectors, controllers and host computer database, system achieved the automatic detection of arc fault, high temperature fault and leakage current fault to protect the conductors, the equipments and ensure human's safety. Detectors, controllers and host computer database were communicated with CAN bus. Device realized the expectations of reducing the communication cost and improving the communication quality.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5306631"},{"name":"Evolution of fault-tolerant and noise-robust digital designs","snippet":"Artificial evolution has been shown to generate remarkable systems of exciting novelty. It is able to automatically generate digital circuit designs and even circuits that are robust to noise and faults. Extensive experiments have been carried out and are presented here to identify more clearly to what extent artificial evolution is able to generate robust designs. The evolved circuits are thoroughly tested whilst being exposed to noise and faults in a simulated environment and the results of their performance are presented. The evolved multiplier and adder circuits show a graceful degradation as noise and failrate are increased. The functionality of all circuits is measured in a simulated environment that to some extent takes into account analogue electronic properties. Also included is a short overview of some recent work illustrating the robustness and tolerance of bio-inspired hardware systems.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1318863"},{"name":"MATLAB Design and Research of Fault Diagnosis Based on ANN for the C3I System","snippet":"Artificial neural networks (ANN) are an information-processing method of a simulation of the structure for biological neurons. C<sup>3<\/sup>I system as a modern combat unit can control and command the army action and can communicate to others. This paper makes a research on the approach of the artificial neural network for fault diagnosis of C<sup>3<\/sup>I system and constructs a fault diagnosis system of C<sup>3<\/sup>I system with ANN. And the system can analyze fault phenomena and detect C<sup>3<\/sup>I system fault. It will greatly improve the response to the C<sup>3<\/sup>I system fault diagnosis and maintenance efficiency.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5600194"},{"name":"A Test of Artificial Neural Network-Based Gross Error Detection Used in Conversion of GPS Heights","snippet":"Artificial neural networks (ANN) has been used in GPS heights conversion. It is impossible to avoid gross errors in conversion of GPS heights. So, the paper puts forward a kind algorithm to detect gross error considering of ANN used in conversion of GPS heights. After the three tests, the algorithm proposed for disposing of gross error is validated in conversion of GPS heights, and some practicable conclusions are received.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4721688"},{"name":"Investigation on flow instrument fast fault detection based on LSSVM predictor","snippet":"Aimed at the issue of real time fault diagnosis in Flow Totalizer, a new type predictor based on Least Squares Support Vector Machine (LSSVM) was put forward. By comparing predictive value with flow meter output value, fault diagnosis was carried out. Predictive errors and the speed of prediction were considered in this algorithm, by dealing with compromise between them, the samples were selected and a higher predicting speed was ensured. The analysis and simulation results showed that the relative higher speed could be acquired by training LSSVM with the selected-samples and reducing the precision of predictor. So this type of predictor was more suitable in real time fault detection.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5583616"},{"name":"Power Cable Fault Diagnosis System Based on LabVIEW","snippet":"Aiming at the online fault diagnoses of the power cable, an online Power Cable Monitoring system is built by using the virtual instrument platform Lab VIEW. In the system, the monitoring surface is designed by using the graphical programming language, and the data acquisition card PCI6221 is used to capture the data and to monitor the state of the online cable. The experiment shows that the functions of the friendly interface, the visual display of the online data of the power cable traveling waves, and the reliable data processing measure are implemented. The system provides the hardware support for the online cable monitoring and fault diagnosis.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5630166"},{"name":"GIS based multilevel intelligent fault diagnosis system on electric power equipment","snippet":"Along with the application which AM\/FM\/GIS has been used in distribution network strengthening, at present it has become a technique development direction that SCADA should integrate with AM\/FM\/GIS. In pace with electrified wire netting scale growing immensely, the equipment requirements are increasing and their function is more and more advanced. It can not satisfy the requirements of a practical system to adopt traditional concentrated and single fault diagnosis methods. We designed a distributed multilevel intelligent fault diagnosis method in this paper. First resolve the equipment fault diagnosis of the whole system into a fault diagnosis of each subsystem, then, according to the characteristic of the equipment type, transfer many sorts of fault diagnosis means from the knowledge-base to carry on cooperation fault diagnosis under the control of multilevel intelligent fault diagnosis tactics. Leave the final diagnosis result with a fault database to offer system-making-policy and alarm information.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1380612"},{"name":"Fast and Efficient Bright-Field AAPSM Conflict Detection and Correction","snippet":"Alternating-aperture phase shift masking (AAPSM), a form of strong resolution enhancement technology, will be used to image critical features on the polysilicon layer at smaller technology nodes. This technology imposes additional constraints on the layouts beyond traditional design rules. Of particular note is the requirement that all critical features be flanked by opposite-phase shifters while the shifters obey minimum width and spacing requirements. A layout is called phase assignable if it satisfies this requirement. Phase conflicts have to be removed to enable the use of AAPSM for layouts that are not phase assignable. Previous work has sought to detect a suitable set of phase conflicts to be removed as well as correct them. This paper has two key contributions: 1) a new computationally efficient approach to detect a minimal set of phase conflicts, which when corrected will produce a phase-assignable layout, and 2) a novel layout modification scheme for correcting these phase conflicts with small layout area increase. Unlike previous formulations of this problem, the proposed solution for the conflict detection problem does not frame it as a graph bipartization problem. Instead, a simpler and more computationally efficient reduction is proposed. This simplification greatly improves the runtime while maintaining the same improvements in the quality of results obtained in Chiang (Proc. DATE, 2005, p. 908). An average runtime speedup of 5.9times is achieved using the new flow. A new layout modification scheme suited for correcting phase conflicts in large standard-cell blocks is also proposed. The experiments show that the percentage area increase for making standard-cell blocks phase assignable ranges from 1.7% to 9.1%","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4039500"},{"name":"Analog Circuit Fault Simulation Based on Saber","snippet":"Although analog circuit simulation tool like Saber software are numerous, however, it is lack of software which can simulate analog circuits affected by fault modes. Research in the fields of analog circuit fault simulation has not achieved the same degree of success as digital circuits, because of the difficulty in modeling the more complex analog behavior. This article presents a new approach to this problem by simulating the good and fault circuits in Saber and introduces the general circuit fault simulation process. In view of failure mechanism under the components, some novel approaches for fault modeling are proposed. Fault injection and simulation interface based on Saber are detailed in this paper. This method is verified by an example and the actual engineering value is indicated.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5709104"},{"name":"A New Approach to Improving the Test Effectiveness in Software Testing Using Fault Collapsing","snippet":"Although mutation is one of the practical ways of enhancing the effectiveness of the test cases to be applied to an application under test, it could be sometimes infeasible for there being too many assumed faults and mutants to be operated in a larger scale system so that the mutation operating becomes time-consuming and even prohibited. Therefore, the number of faults assumed to exist in the software under test should be reduced. Fault collapsing is a common way of reducing the number of faults in hardware testing. However, this strategy can now be well implanted into the area of software testing. In this paper, we utilize the concept of fault dominance and equivalence, which has long been used in hardware testing, for revealing a novel way of reducing the number of faults assumed to hide in software systems. Once the number of faults assumed in software is decreased sharply, the effectiveness of mutation testing would be greatly enhanced. Examples and experimental results are presented to illustrate the effectiveness and the helpfulness of the technology proposed in the paper","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4041890"},{"name":"The Application of Topological Gradients to Defect Identification in Magnetic Flux Leakage-Type NDT","snippet":"An inverse problem is formulated to indentify the shape and size of the defects in a nonlinear ferromagnetic material using the signal profile from magnetic flux leakage-type NDT. This paper presents an efficient algorithm based on topological shape optimization which exploits the topological gradient to accelerate the process of shape optimization to identify the defect. Topological gradient images for the cracks are obtained using 2-D and 3-D finite element models. Robustness of this imaging method in the presence of noise is also evaluated.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5512921"},{"name":"Stator current monitoring to detect mechanical faults in medium size induction motors","snippet":"An investigation into the use of stator current monitoring to detect mechanical irregularities in a medium-size, skewed induction motor is presented. Through the use of analytical mmf and permeance functions, the frequencies of air gap flux density harmonics may be identified, allowing the prediction of induced voltage and current harmonics in the stator windings. The development of a test rig to create both static and dynamic eccentricity conditions is described. The test facility also allows rapid access to the motor bearings, allowing the investigation of faulty bearings. The presented experimental results confirm the theory related to static and dynamic eccentricity. Experiments carried out with a contaminated bearing indicate that techniques used to detect eccentric conditions may also be valid for the detection of bearing degradation.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1348932"},{"name":"Error Corrections in Outdoor Cylindrical near Field Radar Antenna Measurement System","snippet":"An outdoor cylindrical near field antenna measurement system was designed and fabricated in Guadalajara (Spain) for measuring large L-band RADAR antennas. This design was presented in EuCap 2006 by Martin, F., et al. (2006), where the theoretical error analysis and the main description of the complete system were presented. This paper presents the solutions adopted for solving some of the problems presented in the outdoor design: the first of them due to temperature variations, the second one to the effect of the wind and the third one to the reflections in the ground.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4458809"},{"name":"Scatter Correction Method for X-Ray CT Using Primary Modulation: Theory and Preliminary Results","snippet":"An X-ray system with a large area detector has high scatter-to-primary ratios (SPRs), which result in severe artifacts in reconstructed computed tomography (CT) images. A scatter correction algorithm is introduced that provides effective scatter correction but does not require additional patient exposure. The key hypothesis of the algorithm is that the high-frequency components of the X-ray spatial distribution do not result in strong high-frequency signals in the scatter. A calibration sheet with a checkerboard pattern of semitransparent blockers (a \"primary modulator\") is inserted between the X-ray source and the object. The primary distribution is partially modulated by a high-frequency function, while the scatter distribution still has dominant low-frequency components, based on the hypothesis. Filtering and demodulation techniques suffice to extract the low-frequency components of the primary and hence obtain the scatter estimation. The hypothesis was validated using Monte Carlo (MC) simulation, and the algorithm was evaluated by both MC simulations and physical experiments. Reconstructions of a software humanoid phantom suggested system parameters in the physical implementation and showed that the proposed method reduced the relative mean square error of the reconstructed image in the central region of interest from 74.2% to below 1%. In preliminary physical experiments on the standard evaluation phantom, this error was reduced from 31.8% to 2.3%, and it was also demonstrated that the algorithm has no noticeable impact on the resolution of the reconstructed image in spite of the filter-based approach. Although the proposed scatter correction technique was implemented for X-ray CT, it can also be used in other X-ray imaging applications, as long as a primary modulator can be inserted between the X-ray source and the imaged object","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4016177"},{"name":"Analog circuit fault diagnosis based on artificial neural network and embedded system","snippet":"Analog circuit fault diagnosis system based on S3C2410 embedded board is achieved in this paper. The hardware and software design are presented. Momentum addition BP neural network algorithm with embedded system is applied to that system. Real-time data collection and on-line detection of analog circuit fault condition are designed as the basic functions in the embedded system. Diagnosis system of intelligence and minimization is realized actually.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5412941"},{"name":"Fault-tolerant training of neural networks in the presence of MOS transistor mismatches","snippet":"Analog techniques are desirable for hardware implementation of neural networks due to their numerous advantages such as small size, low power, and high speed. However, these advantages are often offset by the difficulty in the training of analog neural network circuitry. In particular, training of the circuitry by software based on hardware models is impaired by statistical variations in the integrated circuit production process, resulting in performance degradation. In this paper, a new paradigm of noise injection during training for the reduction of this degradation is presented. The variations at the outputs of analog neural network circuitry are modeled based on the transistor-level mismatches occurring between identically designed transistors. Those variations are used as additive noise during training to increase the fault tolerance of the trained neural network. The results of this paradigm are confirmed via numerical experiments and physical measurements and are shown to be superior to the case of adding random noise during training","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=924069"},{"name":"Multiple Fault Models for Timed FSMs","snippet":"An implementation under test (IUT) can be formally described using finite-state machines (FSMs). Due to the presence of inherent timing constraints and variables in a communication protocol, an IUT is modeled more accurately by using extended finite-state machines (EFSMs). However, infeasible paths due to the conflicts among timing condition and action variables of EFSMs can complicate the test generation process. The fault detection capability of the graph augmentation method given in M. U. Uyar et al. (2005) and M. A. Fecko et al. (2000) are analyzed in the presence of multiple timing faults. The complexity increases with the consideration of the concurrent running and expiring of timers in a protocol. It is proven that, by using our graph augmentation models, a faulty IUT will be detected for the multiple occurrences of pairwise combinations of a class of timing faults","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4124472"},{"name":"Compensation is not enough [fault-handling and compensation mechanism]","snippet":"An important problem in designing infrastructure to support business-to-business integration (B2Bi) is how to cancel a long-running interaction (either because the user has changed their mind, or in response to an unrecoverable failure). We review the fault-handling and compensation mechanism that is now used in most workflow products and business process modeling standards. We then use an e-procurement case-study to extract a set of requirements for an effective cancellation mechanism, and we show that the standard approach using fault-handling, and compensation transactions is not adequate to meet these requirements.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1233852"},{"name":"Bug reports retrieval using Self-organizing Map","snippet":"An important process when implementing complex software systems consist of documenting the bugs found in that software. However, since many developers are working at the same time on the project, a bug may easily be reported multiple times, resulting in duplicated bug reports. Therefore, developers responsible for fixing bugs may spend time and effort reading and trying to understand two bugs that actually are the same. This way, we propose in this paper an approach for identifying duplicated bug reports that combines document indexing and self-organizing maps (SOM). The results of our experiments show that at most 69% of duplicated bug reports were identified, representing saving of time and effort for the developers.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4746786"},{"name":"DOORS: towards high-performance fault tolerant CORBA","snippet":"An increasing number of applications are being developed using distributed object computing middleware, such as CORBA. Many of these applications require the underlying middleware, operating systems, and networks to provide end-to-end quality of service (QoS) support to enhance their efficiency, predictability, scalability, and fault tolerance. The Object Management Group (OMG), which standardizes CORBA, has addressed many of these application requirements in the Real-time CORBA and Fault-Tolerant CORBA specifications. We provide four contributions to the study of fault-tolerant CORBA middleware for performance-sensitive applications. First, we provide an overview of the Fault Tolerant CORBA specification. Second, we describe a framework called DOORS, which is implemented as a CORBA service to provide end-to-end application-level fault tolerance. Third, we outline how the DOORS' reliability and fault-tolerance model has been incorporated into the standard OMG Fault-tolerant CORBA specification. Finally, we outline the requirements for CORBA ORB core and higher-level services to support the Fault Tolerant CORBA specification efficiently","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=874174"},{"name":"Fault diagnosis of analog circuit based on support vector machines","snippet":"An innovative method based on support vector machines is presented to diagnose the fault of analog circuit. Firstly, in order to get enough fault samples, the circuit program is compiled in MATLAB software to obtain expressions of output signals. Secondly, fault samples are sent into Support Vector Machines to train Support Vector Machines. Thirdly, the test samples are classified by trained Support Vector Machines. Finally, an example of analog circuit fault diagnosis is provided. The result shows that this method has the advantages of simple algorithm, high efficiency, high accuracy, great capability in generalization and classification.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5349243"},{"name":"A interturn fault protection method of HV shunt reactors based on unbalanced parameter detection","snippet":"An interturn fault protection method of the HV shunt reactor is put forward in this paper, which is based on the unbalanced parameter detection. This method is established on the basis of the time-domain parameter model, and uses the electrical quantities at two terminals of the HV reactor to calculate the electrical parameters of each phase by mean of least squares algorithm. Suppose that the interturn short-circuit faults of the HV phase-separated shunt reactors impossibly occur at three phases at the same time, we could identify whether the interturn short-circuit fault of the HV reactor occurs by designing the comprehensive detection criterion which could reflect three-phase unbalanced parameters and parameter mutation. The interturn protection of HV reactors could avoid the impact from power swings and frequency deviation. This method is based on the measurement and the comparison of per-phase equivalent inductance parameters of the reactor. The setting of the protection is simple and convenient to implement. Furthermore, the protection has high sensitivity. Because this interturn protection detects the variation of per-phase inductance parameters, it could avoid the influence from the system operating mode and the fault while those based on the principle of sequence components cannot. It is also adaptive to the installation location of the voltage transformer (PT), and could be easily applied to HV reactors of both transmission lines and busbars.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5666110"},{"name":"A CORBA design pattern to build load balancing and fault tolerant telecommunication software","snippet":"As a mature distributed object computing middleware, CORBA is being used more and more widely in many fields to build large scale distributed software system. In telecommunication system, load balance and fault tolerance are especially important because of strict requirements for high reliability and capacity. In former studies, load balance and fault tolerance are considered separately more often. This paper introduces a new CORBA design pattern, named GenericFactory pattern, and discusses how to combine load disturbance and fault tolerance effectively. Advantages of this pattern are pointed out.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1209829"},{"name":"Research on high-speed fuzzy reasoning with CPLD for fault diagnosis expert system","snippet":"As an effective method for diagnosis reasoning, fuzzy reasoning is hard to meet the real-time challenge for its complex process and time-consuming. According to the principle of conventional fuzzy reasoning with software, a new method to design expert system fuzzy reasoning with CPLD for fault diagnosis is presented. In the new method, fuzzy operating is realized by function transform with ROM, and CPLD provides logic control and process coordination for fuzzy reasoning. After all, the whole fuzzy reasoning is finished with hardware, not software. It is validated by many experiments that the speed of fuzzy reasoning with this method is faster than traditional modes, and it can be applicable to many on-line diagnosis systems based on single-chip controller or DSP (digital signal processor).","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5274648"},{"name":"Online Estimation of Architectural Vulnerability Factor for Soft Errors","snippet":"As CMOS technology scales and more transistors are packed on to the same chip, soft error reliability has become an increasingly important design issue for processors. Prior research has shown that there is significant architecture-level masking, and many soft error solutions take advantage of this effect. Prior work has also shown that the degree of such masking can vary significantly across workloads and between individual workload phases, motivating dynamic adaptation of reliability solutions for optimal cost and benefit. For such adaptation, it is important to be able to accurately estimate the amount of masking or the architecture vulnerability factor (AVF) online, while the program is running. Unfortunately, existing solutions for estimating AVF are often based on offline simulators and hard to implement in real processors. This paper proposes a novel way of estimating AVF online, using simple modifications to the processor. The estimation method applies to both logic and storage structures on the processor. Compared to previous methods for estimating AVF, our method does not require any offline simulation or calibration for different workloads. We tested our method with a widely used simulator from industry, for four processor structures and for 100 to 200 intervals of each of eleven SPEC benchmarks. The results show that our method provides acceptably accurate AVF estimates at runtime. The absolute error rarely exceeds 0.08 across all application intervals for all structures, and the mean absolute error for a given application and structure combination is always within 0.05.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4556738"},{"name":"Using MPLS fault recovery mechanism and bandwidth reservation in network-on-chip","snippet":"As CMOS technology scales down into the deep submicron (DSM) domain, devices and interconnects are subject to new types of malfunctions and failures that are harder to predict and avoid with the current system-on-chip (SoC) design methodologies. In this paper we compare four reconfigurable fault recovery mechanism and path restoration schemes, namely, Haskin, Makam, Simple Dynamic and Shortest Dynamic in real network, in the sense of on chip network design methodology. These schemes are simulated by using NS-2 that provides the advantage of reusability of entities, thus increases the NoC fault-tolerant and reliability with Quality-of-Service.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5451263"},{"name":"Improvement the NOC Bandwidth and fault Tolerant by Multipath routing in three-dimensional topologies for multi-media applications","snippet":"As CMOS technology scales down into the deep submicron (DSM) domain, devices and interconnects are subject to new types of malfunctions and failures that are harder to predict and avoid with the current system-on-chip (SoC) design methodologies. We propose a combination of a topology and Multi-path routing which can increase fault-Tolerant and Communication load which is suitable for multimedia applications. We compare the performance of Fat-Tree, 2d-Mesh and 3d-Mesh architectures using Multi-path routing in the sense of on chip network design methodology. The simulations of each of the architectures are done with IP and Multi-path routing, two-dimensional and three-dimensional topologies. We also carry out the high level simulation of on chip network using NS-2 to verify the analytical analysis.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5451627"},{"name":"An Application of Semantic Annotations to Design Errors","snippet":"As current engineered systems (e.g. aviation systems) have been equipped with automated and computer-based artefacts, human-system interaction (e.g. human computer interaction) has been an important issue. Design errors that are attributable to human-system interaction failures are not pure engineering design issues, but a multidisciplinary subject with related other areas such as management, psychology, physiology or ergonomics. To identify such design errors (called design-induced errors) in accident reports is important for designing more reliable systems. However, the lack of precise definitions of the concept of design-induced error and the diversity of expression of such failures make it difficult to retrieve relevant documents from accident reports. This paper describes how an ontology and annotation scheme can help to overcome such limitations. Engineering designers can be assisted by the developed ontology and annotation scheme to reason on the issues of design induced error","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4021126"},{"name":"Reduction of faults in software testing by fault domination","snippet":"Although mutation testing is one of the practical ways of enhancing test effectiveness in software testing, it could be sometimes infeasible in practical work for a large scale software so that the mutation testing becomes time-consuming and even in prohibited time. Therefore, the number of faults assumed to exist in the software under test should be reduced so as to be able to confine the time complexity of test within a reasonable period of time. This paper utilizes the concept of fault dominance and equivalence, which has long been employed in hardware testing, for revealing a novel way of reducing the number of faults assumed to hide in software systems. Once the number of faults assumed in software is decreased sharply, the effectiveness of mutation testing will be greatly enhanced and become a feasible way of software testing. Examples and experimental results are presented to illustrate the effectiveness and the helpfulness of the technology proposed in the paper.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6074040"},{"name":"Analysis of errors for uplink array of 34-m antennas for deep space applications","snippet":"Although the technologies for large arrays of distributed reflector antennas with just downlink (receiving) capability have been well defined and proven for deep space applications, a similar architecture, i.e., the arraying of distributed reflector antennas for uplink (transmitting) applications has not been proven, tested, or built yet. In previous papers (Hurd, 2005) the need, feasibility, technology challenges and high-level system issues of a large array of reflector antennas with uplink capability for the future deep space network (DSN) were discussed. In particular, the primary design drivers, cost drivers, and technology challenges for uplink array phase calibration were addressed together with some preliminary test results with the 34-m antenna exciters. It is now of great interest to obtain the key requirements for the current Deep Space Network (DSN) 34-m antennas so that they can operate in an uplink array mode. The successful demonstration of the DSN 34-m antennas in uplink array mode serves as a prototype and a key milestone for the future large array development. In this paper, simulation and analysis of the current DSN 34-m antennas in an uplink array mode were discussed","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1559415"},{"name":"Equivalence between Weight Decay Learning and Explicit Regularization to Improve Fault Tolerance of RBF","snippet":"Although weight decay learning has been proposed to improve generalization ability of a neural network, many simulated studies have demonstrated that it is able to improve fault tolerance. To explain the underlying reason, this paper presents an analytical result showing the equivalence between adding weight decay and adding explicit regularization on training a RBF to tolerate multiplicative weight noise. Under a mild condition, it is proved that explicit regularization will be reduced to weight decay.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4696195"},{"name":"Fault location on series-compensated transmission line using measurements of current differential protective relays","snippet":"An accurate algorithm for locating faults on a series-compensated line is presented. This algorithm can be applied with current differential protective relays since two-end currents and one-end voltage are utilized as the fault locator input signals. The algorithm applies two subroutines and the procedure for indicating the valid subroutine. The algorithm has been evaluated using the fault data of ATP-EMTP versatile simulations of faults on a series-compensated transmission line. The presented example shows the validity of the presented algorithm and its high accuracy.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6007182"},{"name":"Switch fault diagnosis of PM brushless DC motor drive using adaptive fuzzy techniques","snippet":"An adaptive neuro-fuzzy inference system (ANFIS) is developed to diagnose open switch faults of PM brushless dc motor drives. Features extracted under healthy and faulty operations using wavelet transform are used to train ANFIS. Testing of the proposed diagnostic system shows it could not only diagnose the fault but identify the faulty switch as well. Good agreement between experimentation and simulation is obtained.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1268142"},{"name":"An adaptive scheme for handling deletion spelling errors' for an intelligent e-learning system","snippet":"An adaptive scheme for handling spelling errors by an e-learner while responding to the e-learning system through typed-in single-word responses is presented in this paper. To simulate the behaviour of a human instructor, the system preprocesses the input word with respect to spelling errors due to wrong letter or missing letter. The appropriately encoded input is then to fed into a neural net that intelligently recognizes the correct response, in spite of minor spelling mistakes committed by the learner. Results show that the scheme intelligent recognises the misspelled words as expected to be done by a human instructor.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5645875"},{"name":"Coupled field-circuit-mechanical model of an electromagnetic actuator operating in error actuated control system","snippet":"An algorithm of coupled field-circuit simulation of the dynamics of an electromagnetic linear actuator operating in error actuated control system is presented. The software consists of three main parts: (a) numerical model of the actuator dynamics which includes equations of a transient electromagnetic field in a non-linear conducting and moving medium, (b) discrete model of electric circuit and (c) optimization solver. Numerical implementation is based on the finite elements. The influence of the PID controller settings on the actuator operation is shown. In order to find optimal parameters of the system the genetic algorithm is applied. The simultaneous optimization of both: actuator structure and regulator settings has been carried out.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4635623"},{"name":"An Atmospheric Correction Parameter Calculator for a single thermal band earth-sensing instrument","snippet":"An atmospheric correction tool has been developed for public web site access for the Landsat-5 and Landsat-7 thermal band. The Atmospheric Correction Parameter Calculator uses the National Centers for Environmental Prediction (NCEP) modeled atmospheric global profiles for a particular date, time and location as input. Using commercially-available MODTRAN software and a suite of integration algorithms, the site-specific atmospheric transmission, and upwelling and downwelling radiances are derived. These calculated parameters can be applied to single band thermal imagery from Landsat-5 Thematic Mapper (TM) or Landsat-7 Enhanced Thematic Mapper Plus (ETM+) to infer an at-surface kinetic temperature for all the pixels in the scene. Given the TM and ETM+ Band-6 instrument calibration uncertainties in Top-of-Atmosphere temperature are 1.0 and 0.6 K, respectively, then the corresponding uncertainties in the inferred surface temperatures are approximately 2-3 K.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1294665"},{"name":"A Hierarchy Management Framework for Automated Network Fault Identification","snippet":"An autonomous diagnosis approach of faulty links is proposed in this paper,. Given the information by which paths a designated network node with management responsibilities can communicate with certain other nodes, and can't communicate with another set of node, with the help of building diagnosis model and computing probability of link's failure the node with management responsibilities would like to identify as quickly as possible a ranked list of the most probable failed network links, and furthermore, accurately check out which links have failed by testing. Based on this approach, a hierarchy network management architecture is designed to deal with the fault diagnosis for a heterogeneous network environment. The simulation shows that this approach has the features of real-time, higher accuracy and autonomy, especially, it will occupy a few bit of bandwidth and even require no bandwidth.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4679266"},{"name":"Controller design and real-time fault diagnosis for a humanoid robot","snippet":"An effective controller is crucial for a humanoid robot since a humanoid robot generally has more than thirty DOFs to be controlled in real-time and needs to deal with information of multiple sensors. On the other hand, real-time fault diagnosis is increasingly important for the humanoid robot due to its mechanism and control complexity and inherent instability of risking tipping itself over. In this paper, we propose a distributed controller consisting of the online planning sub-system and the motion control sub-system based on CAN bus and Ethernet for humanoid robots. Moreover, a real-time fault diagnosis method is proposed to observe the most probable faults, such as joint over-limit, force\/torque sensor failure, encoder failure, and inertial sensor failure. The effectiveness of our designed controller and fault diagnosis was confirmed by experiments on our newly-built humanoid robot.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5585332"},{"name":"A randomized error recovery algorithm for reliable multicast","snippet":"An efficient error recovery algorithm is essential for a liable multicast in large groups. Tree-based protocols (RMTP, TMTP, LBRRM) group receivers into local regions and select a repair server for performing error recovery in each region. Hence a single server bears the entire responsibility of error recovery for a region. In addition, the deployment of repair servers requires topological information of the underlying multicast tree, which is generally not available at the transport layer. This paper presents RRMP, a randomized reliable multicast protocol which improves the robustness of tree-based protocols by diffusing the responsibility of error recovery among all members in a group. The protocol works well within the existing IP multicast framework and does not require additional support from routers. Both analysis and simulation results show that the performance penalty due to randomization is low and can be tuned according to application requirements","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=916706"},{"name":"A pipelined architecture for real-time correction of barrel distortion in wide-angle camera images","snippet":"An efficient pipelined architecture for the real-time correction of barrel distortion in wide-angle camera images is presented in this paper. The distortion correction model is based on least-squares estimation to correct the nonlinear distortion in images. The model parameters include the expanded\/corrected image size, the back-mapping coefficients, distortion center, and corrected center. The coordinate rotation digital computer (CORDIC) based hardware design is suitable for an input image size of 10281028 pixels and is pipelined to operate at a clock frequency of 40 MHz. The VLSI system will facilitate the use of a dedicated hardware that could be mounted along with the camera unit.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1397786"},{"name":"A Fault Management Architecture for Wireless Sensor Network","snippet":"Advancement in wireless communication and electronics has made possible the development of low cost sensor networks. Wireless sensor networks (WSNs) facilitate monitoring and controlling of physical environment from remote location with better accuracy. They can be used for various application areas (e.g. health, military, home). Due to their unique characteristics, they are offering various research issues that are still unsolved. Sensors energy cannot support long haul communication as changing energy supply is not always possible in WSN. Also, failures are inevitable in wireless sensor networks due to inhospitable environment and unattended deployment. Therefore fault management is an essential component of any network management system. In this paper we propose a new fault management architecture for wireless sensor networks. In our solution the network is partitioned into a virtual grid of cells to support scalability and perform fault detection and recovery locally with minimum energy consumption. Specifically, the grid based architecture permits the implementation of fault detection in a distributed manner and allows the failure report to be forwarded across cells. A cell manager and a gateway node are chosen in each cell to perform management tasks. Cell manager and gateway nodes coordinate with each other to detect faults with minimum energy consumption. We assume a homogeneous network where all nodes are equal in resources. The architecture has been evaluated analytically and compared with different proposed solutions.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4600034"},{"name":"Fault-Tolerant Routing Switcher Topologies for Centralized Distribution Systems","snippet":"Advances in digital storage technology have made it possible to store vast amounts of program material in a central location. In addition, advances in digital distribution technology have made it possible to move this material more efficiently between facilities and, ultimately, deliver it via multiple channels to a mass audience. At the heart of the network, switching and routing must be deterministic, error free, and fault free.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7261921"},{"name":"HVDC converter modeling and harmonic calculation under asymmetric faults in the AC system","snippet":"After a detailed analysis of the dynamic processes in the HVDC converter when asymmetric faults occur in the ac system, a sequence-component model of converter based on the advanced switching functions is present. Furthermore, a linear direct method of harmonic calculation is proposed. In this method, both the effect of the harmonic interaction between the ac and dc system and the dynamic processes in the converter are considered. It is demonstrated that quantitative analysis basis for harmonic suppression, filter configuration and setting calculation of relaying can be provided by this method which is verified by comparison with the results obtained by dynamic simulation.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5275286"},{"name":"Redefining and testing interconnect faults in Mesh NoCs","snippet":"An extended fault model and novel strategy to tackle interconnect faults in network-on-chips are proposed. Short faults between distinct channels are considered in a cost-effective test sequence for mesh NoC topologies based on XY routing.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4437574"},{"name":"Fault diagnosis for using TPG low power dissipation and high fault coverage","snippet":"BIST TPG (built in self test) for low power dissipation and high fault coverage presents a low hardware overhead test pattern generator (TPG) for scan-based built-in self-test (BIST) that can reduce switching activity in circuits under test (CUTs) during BIST and also achieve very high fault coverage with reasonable lengths of test sequences. The proposed BIST TPG decreases transitions that occur at scan inputs during scan shift operations and hence reduces switching activity in the CUT. The BIST TPG comprises of two TPG's, LT-RTPG and 3-weight WRBIST. Test patterns generated by the LT-RTPG detect easy-to-detect faults and test patterns generated by the 3-weight WRBIST detect faults that remain undetected after LT-RTPG patterns are applied. The BIST TPG does not require modification of mission logics, which can lead to performance degradation. Recently, techniques to reduce switching activity during BIST have been proposed. A straightforward solution is to reduce the speed of the test clock during scan shift operations. However, since most test application time of scan-based BIST is spent for scan shift operations, this will increase test application time by about a factor of if scan flip-flops are clocked at speed during scan shift operations. Larger reduction in switching activity is achieved in large circuits. Experimental results also show that the BIST-TPG can be implemented with low area overhead. Larger reduction in switching activity is achieved in large circuits. Experimental results also show that the BIST-TPG can be implemented with low area overhead.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5705884"},{"name":"Injecting bit flip faults by means of a purely software approach: a case studied","snippet":"Bit flips provoked by radiation are a main concern for space applications. A fault injection experiment performed using a software simulator is described in this paper. Obtained results allow us to predict a low sensitivity to soft errors for the studied application, putting in evidence critical memory elements.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1173507"},{"name":"A high efficient boost converter with power factor correction","snippet":"Boost converter is widely used as active power factor correction (PFC) pre-regulator. Its input voltage range is universal (90-265 V), and its output voltage is regulated at about 380 V. At low line (90 V) the switch's rms current is high, so the conduction loss of power switch MOSFET is large and the efficiency of whole converter is very low. This paper proposes a new control method that the output voltage varies with the input voltage change. Under this control the MOSFET's on-time is shortened, and the switch's RMS current decreases, which reduces the conduction loss and increases the boost converter efficiency. The distribution of power loss is analyzed by computing software (mathcad 2000) and the realization of this special control method is given. A 1200 W boost power factor corrector with average current control is built up. In order to improve the diode's turn-off loss the performance of a 600 V, 12 A silicon carbide (SiC) Schottky diode is also experimentally evaluated. Measurements of overall efficiency and reverse recovery behavior are compared between SiC diode and fast recovery diode.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1355674"},{"name":"A Novel Fast Error-resilient Video Coding Scheme for H.264","snippet":"Both perceptually and statistically, compressed video with large or disordered motion is sensitive to errors. In this paper, we propose a novel fast error-resilient video coding scheme, which is based on significant macroblock (MB) determination and protection. The scheme takes three impact factors (inter-block mode, motion vector difference and SAD value) to build a statistical model. The model takes error concealment (EC) into consideration in advance and generates several parameters for further significant degree (SD) evaluation for MBs. During encoding, we build an SD table for each frame based on the parameters and pick up those MBs with the largest SD values as significant MBs (SMBs). Few additional computations are induced into SMB determination, thus make our scheme practical in real time video coding scenarios. Simulations show that the scheme has an acceptable SMB determination accuracy and the corresponding protection method can prevent errors effectively","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4064558"},{"name":"Multi-level Bounded Model Checking to detect bugs beyond the bound","snippet":"Bounded Model Checking is a widely used technique both in hardware and software verification. However, it cannot be applied if the bounds (number of time frames to be analyzed) becomes large. Therefore it cannot detect bugs that can be observed only through very long sequence counter-examples. In this paper, we present a method connecting multiple BMCs by sophisticated uses of inductive approach and symbolic simulation. The proposed method can check unbounded properties by analyzing loop behaviors in the design with decision procedures. In our verification flow, a property is automatically decomposed and refined instead of designs. First, a property is decomposed not to consider the reachability from the initial states of the design. Next, if a counter-example is found, the condition to enter it is generated by symbolic simulation. Finally, the reachability from the initial states to the states where the condition becomes true is checked inductively by another Bounded Model Checking. If they are not reachable from the initial states, then the property is refined not to enter the unreal counter-example. Key observation here is that each BMC does not need to process so many time frames as compared with pure BMC from initial states. Therefore, the proposed method can process much larger bounds. Experimental results with two examples have confirmed this advantage.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4695874"},{"name":"Error-Related EEG Potentials Generated During Simulated BrainComputer Interaction","snippet":"Brain-computer interfaces (BCIs) are prone to errors in the recognition of subject's intent. An elegant approach to improve the accuracy of BCIs consists in a verification procedure directly based on the presence of error-related potentials (ErrP) in the electroencephalogram (EEG) recorded right after the occurrence of an error. Several studies show the presence of ErrP in typical choice reaction tasks. However, in the context of a BCI, the central question is: ldquoAre ErrP also elicited when the error is made by the interface during the recognition of the subject's intent?rdquo We have thus explored whether ErrP also follow a feedback indicating incorrect responses of the simulated BCI interface. Five healthy volunteer subjects participated in a new human-robot interaction experiment, which seem to confirm the previously reported presence of a new kind of ErrP. However, in order to exploit these ErrP, we need to detect them in each single trial using a short window following the feedback associated to the response of the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 83.5% and 79.2%, respectively, using a classifier built with data recorded up to three months earlier.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4360138"},{"name":"Normalization of illumination conditions for ground based hyperspectral measurements using dual field of view spectroradiometers and BRDF corrections","snippet":"BRDF effects present in dual field-of-view spectroscopy datasets were investigated. A data-driven normalization procedure was developed by decomposing the target BRDF into a target specific Lambertian component and a bi-directional component characterizing a group of similar targets,. The normalization method was used to convert reflectance factors obtained under cloud obscured conditions into clear sky conditions. An evaluation on four targets measured under different illumination conditions suggests that the normalization can reduce relative reflectance errors between 400 and 1800 nm from 15% to less than 5% even under full cloud obscuration. At higher wavelengths a decreased signal-to-noise ratio increases the error level.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5417752"},{"name":"Reliable backup routing in fault tolerant real-time networks","snippet":"Broadband integrated services digital networks (B-ISDN) are aimed to transport both real-time traffic and non real-time traffic. Many of these applications require quality of service (QoS) guarantees. In the literature, not much work is found to provide QoS guarantees based on fault tolerance. Reliability of network links is considered as one of the parameters when providing QoS guarantees to applications. Considering reliability of network links as a parameter for QoS guarantees gives the applications more flexibility in choosing the network resources. A new terminology for dispersity routing is presented which will be useful in providing QoS guarantees based on reliability. Dispersity routing transmits the traffic along multiple paths. Also, a reliable backup resource allocation method is presented that can be used in the context of dispersity routing for fault tolerant real-time networks. An assumption is made that higher capacity is assigned to the links which are more reliable. This will help in availability of resources for a longer period of time. Also, reliability of links is considered to compute multiple paths along with shortest path metric.","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=962338"},{"name":"A dynamic technique for eliminating buffer overflow vulnerabilities (and other memory errors)","snippet":"Buffer overflow vulnerabilities are caused by programming errors that allow an attacker to cause the program to write beyond the bounds of an allocated memory block to corrupt other data structures. The standard way to exploit a buffer overflow vulnerability involves a request that is too large for the buffer intended to hold it. The buffer overflow error causes the program to write part of the request beyond the bounds of the buffer, corrupting the address space of the program and causing the program to execute injected code contained in the request. We have implemented a compiler that inserts dynamic checks into the generated code to detect all out of bounds memory accesses. When it detects an out of bounds write, it stores the value away in a hash table to return as the value for corresponding out of bounds reads. The net effect is to (conceptually) give each allocated memory block unbounded size and to eliminate out of bounds accesses as a programming error. We have acquired several widely used open source servers (Apache, Sendmail, Pine, Mutt, and Midnight Commander). With standard compilers, all of these servers are vulnerable to buffer overflow attacks as documented at security tracking Web sites. Our compiler eliminates these security vulnerabilities (as well as other memory errors). Our results show that our compiler enables the servers to execute successfully through buffer overflow attacks to continue to correctly service user requests without security vulnerabilities.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1377218"},{"name":"Predicting Re-opened Bugs: A Case Study on the Eclipse Project","snippet":"Bug fixing accounts for a large amount of the software maintenance resources. Generally, bugs are reported, fixed, verified and closed. However, in some cases bugs have to be re-opened. Re-opened bugs increase maintenance costs, degrade the overall user-perceived quality of the software and lead to unnecessary rework by busy practitioners. In this paper, we study and predict re-opened bugs through a case study on the Eclipse project. We structure our study along 4 dimensions: (1) the work habits dimension (e.g., the weekday on which the bug was initially closed on), (2) the bug report dimension (e.g., the component in which the bug was found) (3) the bug fix dimension (e.g., the amount of time it took to perform the initial fix) and (4) the team dimension (e.g., the experience of the bug fixer). Our case study on the Eclipse Platform 3.0 project shows that the comment and description text, the time it took to fix the bug, and the component the bug was found in are the most important factors in determining whether a bug will be re-opened. Based on these dimensions we create decision trees that predict whether a bug will be re-opened after its closure. Using a combination of our dimensions, we can build explainable prediction models that can achieve 62.9% precision and 84.5% recall when predicting whether a bug will be re-opened.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5645566"},{"name":"On Identifying Bug Patterns in Aspect-Oriented Programs","snippet":"Bug patterns are erroneous code idioms or bad coding practices that have been proved fail time and time again. They mainly arise from the misunderstanding of language features, the use of erroneous design patterns or simple mistakes sharing the common behaviors. Aspect-oriented programming (AOP) is a new technique to separate the cross-cutting concerns for improving modularity in software design and implementation. However, there is no effective debugging technique for aspect-oriented programs until now and none of the prior researches focused on the identification of bug patterns in aspect-oriented programs. In this paper, we present six bug patterns in AspectJprogramming language and show the corresponding example for each bug pattern to help to illustrate the symptoms of these patterns. We take this as the first step to provide an underlying basis on testing and debugging of AspectJ programs.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4291035"},{"name":"A discriminative model approach for accurate duplicate bug report retrieval","snippet":"Bug repositories are usually maintained in software projects. Testers or users submit bug reports to identify various issues with systems. Sometimes two or more bug reports correspond to the same defect. To address the problem with duplicate bug reports, a person called a triager needs to manually label these bug reports as duplicates, and link them to their \"master\" reports for subsequent maintenance work. However, in practice there are considerable duplicate bug reports sent daily; requesting triagers to manually label these bugs could be highly time consuming. To address this issue, recently, several techniques have be proposed using various similarity based metrics to detect candidate duplicate bug reports for manual verification. Automating triaging has been proved challenging as two reports of the same bug could be written in various ways. There is still much room for improvement in terms of accuracy of duplicate detection process. In this paper, we leverage recent advances on using discriminative models for information retrieval to detect duplicate bug reports more accurately. We have validated our approach on three large software bug repositories from Firefox, Eclipse, and OpenOffice. We show that our technique could result in 17-31%, 22-26%, and 35-43% relative improvement over state-of-the-art techniques in OpenOffice, Firefox, and Eclipse datasets respectively using commonly available natural language information only.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6062072"},{"name":"Automatic Identification of Bug-Introducing Changes","snippet":"Bug-fixes are widely used for predicting bugs or finding risky parts of software. However, a bug-fix does not contain information about the change that initially introduced a bug. Such bug-introducing changes can help identify important properties of software bugs such as correlated factors or causalities. For example, they reveal which developers or what kinds of source code changes introduce more bugs. In contrast to bug-fixes that are relatively easy to obtain, the extraction of bug-introducing changes is challenging. In this paper, we present algorithms to automatically and accurately identify bug-introducing changes. We remove false positives and false negatives by using annotation graphs, by ignoring non-semantic source code changes, and outlier fixes. Additionally, we validated that the fixes we used are true fixes by a manual inspection. Altogether, our algorithms can remove about 38%~51% of false positives and 14%~15% of false negatives compared to the previous algorithm. Finally, we show applications of bug-introducing changes that demonstrate their value for research","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4019564"},{"name":"A comparison of bug finding tools for Java","snippet":"Bugs in software are costly and difficult to find and fix. In recent years, many tools and techniques have been developed for automatically finding bugs by analyzing source code or intermediate code statically (at compile time). Different tools and techniques have different tradeoffs, but the practical impact of these tradeoffs is not well understood. In this paper, we apply five bug finding tools, specifically Bandera, ESC\/Java 2, FindBugs, JLint, and PMD, to a variety of Java programs. By using a variety of tools, we are able to cross-check their bug reports and warnings. Our experimental results show that none of the tools strictly subsumes another, and indeed the tools often find nonoverlapping bugs. We discuss the techniques each of the tools is based on, and we suggest how particular techniques affect the output of the tools. Finally, we propose a meta-tool that combines the output of the tools together, looking for particular lines of code, methods, and classes that many tools warn about.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1383122"},{"name":"Variable block size error concealment scheme based on H.264\/AVC non-normative decoder","snippet":"As the newest video coding standard, H.264\/AVC can achieve high compression efficiency. At the same time, due to the high-efficiently predictive coding and the variable length entropy coding, it is more sensitive to transmission errors. So error concealment (EC) in H.264 is very important when compressed video sequences are transmitted over error-prone networks and erroneously received. To achieve higher EC performance, this paper proposes variable block size error concealment scheme (VBSEC) by utilizing the new concept of variable block size motion estimation (VBSME) in H.264 standard. This scheme provides four EC modes and four sub-block partitions. The whole corrupted macro-block (MB) will be divided into variable block size adaptively according to the actual motion. More precise motion vectors (MV) will be predicted for each sub-block. We also produce a more accurate distortion function based on spatio-temporal boundary matching algorithm (STBMA). By utilizing VBSEC scheme based on our STBMA distortion function, we can reconstruct the corrupted MB in the inter frame more accurately. The experimental results show that our proposed scheme can obtain maximum PSNR gain up to 1.72 dB and 0.48 dB, respectively compared with the boundary matching algorithm (BMA) adopted in the JM11.0 reference software and STBMA.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4445834"},{"name":"Improving the Table Boundary Detection in PDFs by Fixing the Sequence Error of the Sparse Lines","snippet":"As the rapid growth of PDF documents, recognizing the document structure and components are useful for document storage, classification and retrieval. Table, a ubiquitous document component, becomes an important information source. Accurately detecting the table boundary plays a crucial role for many applications, e.g., the increasing demand on the table data search. Rather than converting PDFs to image or HTML and then processing with other techniques (e.g., OCR), extracting and analyzing texts from PDFs directly is easy and accurate. However, text extraction tools face a common problem: text sequence error. In this paper, we propose two algorithms to recover the sequence of extracted sparse lines, which improve the table content collection. The experimental results show the comparison of the performance of both algorithms, and demonstrate the effectiveness of text sequence recovering for the table boundary detection.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5277535"},{"name":"Exploit failure prediction for adaptive fault-tolerance in cluster computing","snippet":"As the scale of cluster computing grows, it is becoming hard for long-running applications to complete without facing failures on large-scale clusters. To address this issue, checkpointing\/restart is widely used to provide the basic fault-tolerant functionality, yet it suffers from high overhead and its reactive characteristic. In this work, we propose FT-Pro, an adaptive fault management mechanism that optimally chooses migration, checkpointing or no action to reduce the application execution time in the presence of failures based on the failure prediction. A cost-based evaluation model is presented for dynamic decision at run-time. Using the actual failure log from a production cluster at NCSA, we demonstrate that even with modest failure prediction accuracy, FT-Pro outperforms the traditional checkpointing\/restart strategy by 13%-30% in terms of reducing the application execution time despite failures, which is a significant performance improvement for long-running applications.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1630866"},{"name":"Application of neural network and DS evidence fusion algorithm in power transformer fault diagnosis","snippet":"As the transformer's fault types and fault positions are complexity, complementarity, redundancy and strong characteristics of uncertainty, a satisfactory diagnosis of transformer fault types and fault position may not be obtained if only one technique is used. In this paper, a synthetic diagnosis method using neural network and DS evidence theory for transformer fault diagnosis is presented, combining DGA data with application of data fusion theory. This method has advantage of both neural and DS evidential theory, it can effectively solve the problem of uncertainty, and improve the fault diagnosis system's accuracy and reliability. A simulation example in this paper illustrates that the method combing of neural network with DS evidence theory is indeed greatly improved integration of the credibility of the data, making diagnostic systems easy to design, high precision and prone to operation. It can fulfill users' requirement perfectly.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5736014"},{"name":"Compiler-Managed Software-based Redundant Multi-Threading for Transient Fault Detection","snippet":"As transistors become increasingly smaller and faster with tighter noise margins, modern processors are becoming increasingly more susceptible to transient hardware faults. Existing hardware-based redundant multi-threading (HRMT) approaches rely mostly on special-purpose hardware to replicate the program into redundant execution threads and compare their computation results. In this paper, we present a software-based redundant multi-threading (SRMT) approach for transient fault detection. Our SRMT technique uses compiler to automatically generate redundant threads so they can run on general-purpose chip multi-processors (CMPs). We exploit high-level program information available at compile time to optimize data communication between redundant threads. Furthermore, our software-based technique provides flexible program execution environment where the legacy binary codes and the reliability-enhanced codes can co-exist in a mix-and-match fashion, depending on the desired level of reliability and software compatibility. Our experimental results show that compiler analysis and optimization techniques can reduce data communication requirement by up to 88% of HRMT. With general-purpose intra-chip communication mechanisms in CMP machine, SRMT overhead can be as low as 19%. Moreover, SRMT technique achieves error coverage rates of 99.98% and 99.6% for SPEC CPU2000 integer and floating-point benchmarks, respectively. These results demonstrate the competitiveness of SRMT to HRMT approaches","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4145119"},{"name":"Solid-state fault current limiters: Silicon versus silicon carbide","snippet":"As utilities face increasing fault currents in their systems as a result of increasing demand and\/or deployment of new technologies, fault current limiters promise a solution that will mitigate the need for replacing existing breakers as well as being a general protective device for elements connected to the grid. This paper describes some recent advances in semiconductor-based fault current limiting technology including both the more mature silicon developments along with early developments using silicon carbide. The capabilities and limitations of these technologies are compared and contrasted. Some example scenarios of FCLs have been analyzed and are briefly described along with advanced features that semiconductor FCLs may bring to the solution space.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4596612"},{"name":"Enhanced Error Vector Magnitude (EVM) Measurements for Testing WLAN Transceivers","snippet":"As wireless LAN devices become more prevalent in the consumer electronics market, there is an ever increasing pressure to reduce their overall cost. The test cost of such devices is an appreciable percentage of the overall cost, which typically results from the high number of specifications, the high number of distinct test set-ups and equipment pieces that need to be used, and the high cost of each test set-up. In this paper, we investigate the versatility of EVM measurements to test the variable-envelope WLAN (wireless local area networks) receiver and transmitter characteristics. The goal is to optimize EVM test parameters (input data and test limits) and to reduce the number of specification measurements that require high test times and\/or expensive test equipment. Our analysis shows that enhanced EVM measurements (optimized data sequence and limits, use of RMS, scale, and phase error vector values) in conjunction with a set of simple path measurements (input-output impedances) can provide the desired fault coverage while eliminating lengthy spectrum mask and noise figure tests","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4110176"},{"name":"A bug you like: A framework for automated assignment of bugs","snippet":"Assigning bug reports to individual developers is typically a manual, time-consuming, and tedious task. In this paper, we present a framework for automated assignment of bug-fixing tasks. Our approach employs preference elicitation to learn developer predilections in fixing bugs within a given system. This approach infers knowledge about a developer's expertise by analyzing the history of bugs previously resolved by the developer. We apply a vector space model to recommend experts for resolving bugs. When a new bug report arrives, the system automatically assigns it to the appropriate developer considering his or her expertise, current workload, and preferences. We address the task allocation problem by proposing a set of heuristics that support accurate assignment of bug reports to the developers.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5090066"},{"name":"Fault-Tolerant Reconfiguration System for Asymmetric Multilevel Converters Using Bidirectional Power Switches","snippet":"Asymmetric multilevel converters can optimize the number of levels by using H-bridges scaled in the power of three. The shortcoming of this topology is that the H-bridges are not interchangeable, and then, under certain faulty conditions, the converter cannot operate. A reconfiguration system based on bidirectional electronic valves has been designed for three-phase cascaded H-bridge inverters. Once a fault is detected in any of the insulated gate bipolar transistors of any H-bridge, the control is capable to reconfigure the hardware keeping the higher power bridges in operation. In this way, the faulty phase can continue working at the same voltage level by adjusting its gating signals. Some simulations and experiments with a 27-level inverter, to show the operation of the system under a faulty condition, are displayed.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4663108"},{"name":"Orbit drift correction using correctors with ultra-high DAC resolution","snippet":"At BESSY the planned continuous orbit drift correction could not go into routine operation as originally foreseen: the resolution of the 3 mrad correctors controlled by 16 bit DACs was insufficient and perturbed specific experiments unacceptably. Now a novel 216 bit coarse\/fine type I\/O board solved this problem while preserving the full dynamic range of the correctors. Permanent correction activity no more deteriorates experimental conditions. A typical orbit definition within +\/- 5 m at more than 90% of the BPMs during a day is achieved. Even large perturbations caused by e.g. decaying superconducting wave length shifter currents or residual effects of undulator operations are adequately suppressed","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=986620"},{"name":"Application of a New Image Recognition Technology in Fabric Defect Detection","snippet":"At present, defect detection during the manufacturing are still finished by man, there are a lot of weaknesses by this way, such as low detection efficiency, high miss rate. All of these affect the production quality seriously and restrict the further improvement of production efficiency. This paper presents a method using Fisher classifier in computer image pattern recognition for defect detection and grade scoring of fabric, and gives the realization of software programming and testing. The test results show that the defect recognition rate is 94%.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5076687"},{"name":"A Fault Diagnostic Method for EFI Engine Based on MATLAB Software Package","snippet":"At present, the diagnostic instruments used widely here and abroad is not entire, which can not diagnose the mechanical fault without fault code. In order to solve this problem, this paper presents a method for fault diagnosis of electronic fuel injection (EFI) engine using radial basis function (RBF) neural network. By connecting MATLAB software package and ACCESS database, a fault diagnosis program is set up and a fault without code can be found. Meanwhile, the comparison has been done between RBF network and back propagation (BP) network. The simulation experimental results show that the RBF model is more feasible and successful than BP model and makes fault diagnosis easier.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4659886"},{"name":"Use of a simple storage ring simulation for development of enhanced orbit correction software","snippet":"At the Advanced Photon Source (APS) most of the testing of minor operational software is done during accelerator studies time. For major software changes, such as the porting of the complex workstation-based orhit control software to an EPICS IOC, much of the testing was done 'offline' on a test IOC. A configurable storage ring simulator was created in a workstation with corresponding control system records for correctors and orhit readbacks. The simulator??s features will he described as well as the method used to develop and debug the most recent improvement of the APS orhit control software, among others. The simulator is also useful in general-purpose software testing.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1289953"},{"name":"Machine Current Signature Analysis as a Way for Fault Detection in Squirrel Cage Wind Generators","snippet":"At the moment renewable generation systems are increasing its presence. The paper is about how machine current signature analysis (MCSA) can reliably diagnose faults in squirrel cage generators. This paper focuses on the experimental investigation for incipient fault detection and fault detection methods, suitably adapted for use in wind generator systems using squirrel cage. The proposed system diagnoses asynchronous generators having three types of faults such as broken rotor bars, short circuit of stator windings and bearing fault. After processing current data the classical fast Fourier transform is applied to detect characteristics under the healthy and various faulted conditions with MCSA.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4393124"},{"name":"Shadow checker (SC): A low-cost hardware scheme for online detection of faults in small memory structures of a microprocessor","snippet":"At various stages of a product life, faults arise from different sources. During product bring up, logic errors are dominant. During production, manufacturing defects are main concerns while during operation, the concern shifts to aging defects. No matter what the source is, debugging such defects may permit logic, circuit or physical design changes to eliminate them in future. Within a processor chip, there are three broad categories of structures, namely the large memory structures such as caches, small memory structures such as reorder buffer, issue queue, and load-store buffers and the data-path. Most control functions and data steering operations are based on small memory structures and they are hard to debug. In this paper, we propose a lightweight hardware scheme, called shadow checker to detect faults in these critical units. The entries in these units are tested by means of a shadow entry that mimics intended operation. A mismatch traps an error. The shadow checker shadows an entry for a few thousand cycles before moving on to shadow another. This scheme can be employed to test chips during silicon debug, manufacturing test as well as during regular operation. We ran experiments on 13 SPEC2000 benchmarks and found that our scheme detects 100% of inserted faults.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5699222"},{"name":"Whose bug is it anyway? The battle over handling software flaws","snippet":"Attacks exploit vulnerabilities in software code. They come in many forms: logic attacks, Trojan horses, worms and viruses, and variants of each. They serve a host of purposes: corporate espionage, white-collar crime, social \"hacktivism,\" terrorism, and notoriety. Greater connectivity, more complex software, and the persistence of older protocols ensure growing vulnerability. End users lose time and money when networks go down. Software vendors lose face and market share. Security researchers struggle to keep pace with the bugs to keep businesses operating safely. The only people with no complaints are the hackers, who reverse-engineer patches released by vendors to exploit the holes. It's enough to make you nostalgic for the old days of the Nimba and Code Red viruses, when attacks came six months after vendors released patches. Blaster attacks began three weeks after release. Security experts anticipate so-called \"zero day\" vulnerabilities, in which attacks precede patches. Although marathon patching sessions have become the norm for harried IT administrators, even top-of-the-line patch management can't keep up with malicious code's growing sophistication.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1270771"},{"name":"Fault localization using visualization of test information","snippet":"Attempts to reduce the number of delivered faults in software are estimated to consume 50% to 80% of the development and maintenance effort according to J.S. Collofello ans S.N. Woodfield (1989). Among the tasks required to reduce the number of delivered faults, debugging is one of the most time-consuming according to T. Ball and S.G. Eick and Telcordia Technologies, and locating the errors is the most difficult component of this debugging task according to I. Vessey (1985). Clearly, techniques that can reduce the time required to locate faults can have a significant impact on the cost and quality of software development and maintenance.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1317420"},{"name":"Attenuation correction in MR-PET scanners with segmented T1-weighted MR images","snippet":"Attenuation correction of PET data acquired in new hybrid MR-PET scanners which do not offer the possibility of a measured attenuation correction can be done in different ways. A previous report of our group described a method which used attenuation templates. The present study utilizes a new knowledge-based segmentation approach applied on T1-weighted MR images. It examines the position and the tissue membership of each voxel and segments the head volume into attenuation-differing regions: brain tissue, extracerebral soft tissue, skull, air-filled nasal and paranasal cavities as well as the mastoid process. To examine this new approach three groups of subjects having MRI and PET were chosen, the selection criterion being the different MR scanners, while the PET scanner was the ECAT HR+ in all cases: 1) four subjects with 1.5T MR images and CPFPX PET scans, 2) four subjects with 3T MR images and Altanserin PET scans, and 3) three brain tumor patients with 3T MR images from the hybrid MR-BrainPET scanner and FET PET scans. Furthermore, a single subject had 3T MR images, a FDG PET scan, and an additional CT scan. All segmented T1-weighted MR images were converted into attenuation maps for 511 KeV photons with coefficients of 0.096 1\/cm for brain tissue, 0.146 1\/cm for skull, 0.095 1\/cm for soft tissue, 0.054 1\/cm for the mastoid process, and 0.0 1\/cm for nasal and paranasal cavities. The CT volume was also converted from the Hounsfield units into attenuation coefficients valid for 511 keV photons. The 12 segmented-based attenuation (SBA) maps as well as the CT-based attenuation (CBA) map were first filtered by a 3D Gaussian kernel of 10 mm filter width and then used to reconstruct the corresponding PET emission data. These were compared to the PET images attenuation corrected using the conventional PET-based transmission data (PBA). Relative differences (RD) were calculated from ROIs. For the single subject the RD of CBA data exhibit a mean of 1.66%?0.84% with a rang- from -0.88% to 3.42%, while the RD's mean of SBA data is 1.42%?2.61% (range from -4.12% to 4.66%). Comparing the results obtained with the SBA correction only, the RD for 1) range from -6.10% to 2.56% for cortical regions and from -6.99% to 5.64% for subcortical regions; for 2) they range from -7.33% to 2.33% for the cortical regions, subcortical ones being not drawn due to the not significant tracer uptake; for 3) the mean over the three subjects resulted in 0.89%?1.10% for ROIs at 48% threshold of the image's maximum and in 2.25%?1.50% for ROIs at 72% threshold. ROIs on the healthy contra-lateral grey matter show a mean of -3.24%?0.87%. In conclusion, the first attenuation correction results obtained with the new segmented-based method on a strongly heterogeneous collective are very promising. Further improvements of the method will be focused on the delineation of the skull.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5402034"},{"name":"Off-line error prediction, diagnosis and recovery using virtual assembly systems","snippet":"Automated assembly systems often stop their operation due to the unexpected failures occurred during their assembly process. Since these large-scale systems are composed of many parameters, it is difficult to anticipate all possible types of errors with their likelihood of occurrence. Several systems were developed in the literature, focusing on online diagnosing and recovering the assembly process in an intelligent manner based on the predicted error scenarios. However, these systems do not cover all of the possible errors and they are deficient in dealing with the unexpected error situations. The proposed approach uses Monte Carlo simulation of the assembly process with the 3D model of the assembly line to predict the possible errors in an offline manner. After that, these predicted errors can be diagnosed and recovered using Bayesian reasoning and genetic programming. A case study composed of a peg-in-hole assembly was performed and the results are discussed. It is expected that with this new approach, errors can be diagnosed and recovered accurately and costly downtime of robotic assembly systems will be reduced.","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=932651"},{"name":"An on-line monitoring and multi-layer fault diagnosis system of electrical equipment based on geographic information system","snippet":"Automated mapping\/facilities management\/geographic information system (AM\/FM\/GIS), which provides a powerful way to process graphic and non-graphic information, can construct a spatial database system with topological structure and analysis function by combining diversified information in power system with geographic position-related graphic information. Based on the AM\/FM\/GIS and on-line monitoring system, an integrated system is put forward which can implement state monitoring, multi-layer fault diagnosis and assess the faults. By using this integrated system, latent fault and defect can be eliminated, loss due to power cut is reduced and the reliability of running power system is improved. Application indicates it is economical, pragmatic and has excellent performance.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1496124"},{"name":"Update on distribution system fault location technologies and effectiveness","snippet":"Automatic fault location is an area of significant interest and research in the industry. This paper provides an update on the work performed to date with various utilities and the fault location systems. Basic information on the techniques used to locate faults is provided as well as several examples of where these techniques have been deployed.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5371248"},{"name":"Analysis of Error Sources Towards Improved Form Processing","snippet":"Automatic form processing is an important application of document analysis subject. Such a system requires to be trained and tested on a standard database of forms collected from real-life. However, to the best of our knowledge, the only such available databases are NIST Special Databases. These databases consist of images of synthesized form documents. On the other hand, recently we developed a form database, samples of which had been taken from the real-life. ISIFormReader, a form processing system, also developed recently, has been tested using these real-life samples. An intensive study of the processing errors showed that writers' idiosyncracies are one of the major reasons of such errors as analyzed in U. Bhattacharya, et al., (2006). In the present paper, we investigated various other sources of errors which together cause a major concern. These include sample forms which are low in contrast, noisy, smudgy, skewed, scaled disturbing its aspect ratio and so on. An analysis of errors due to similar such sources is important towards development of an improved form processing system.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4273172"},{"name":"A Multi-step Simulation Approach toward Secure Fault Tolerant System Evaluation","snippet":"As new techniques of fault tolerance and security emerge, so does the need for suitable tools to evaluate them. Generally, the security of a system can be estimated and verified via logical test cases, but the performance overhead of security algorithms on a system needs to be numerically analyzed. The diversity in security methods and design of fault tolerant systems make it impossible for researchers to come up with a standard, affordable and openly available simulation tool, evaluation framework or an experimental test-bed. Therefore, researchers choose from a wide range of available modeling-based, implementation-based or simulation-based approaches in order to evaluate their designs. All of these approaches have certain merits and several drawbacks. For instance, development of a system prototype provides a more accurate system analysis but unlike simulation, it is not highly scalable. This paper presents a multi-step, simulation-based performance evaluation methodology for secure fault tolerant systems. We use a divide-and-conquer approach to model the entire secure system in a way that allows the use of different analytical tools at different levels of granularity. This evaluation procedure tries to strike a balance between the efficiency, effort, cost and accuracy of a system's performance analysis. We demonstrate this approach in a step-by-step manner by analyzing the performance of a secure and fault tolerant system using a JAVA implementation in conjunction with the ARENA simulation.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5623417"},{"name":"Layout to Logic Defect Analysis for Hierarchical Test Generation","snippet":"As shown by previous studies, shorts between the interconnect wires should be considered as the predominant cause of failures in CMOS circuits. Fault models and tools for targeting these defects, such as the bridging fault test pattern generators have been available for a long time. However, this paper proposes a new hierarchical approach based on critical area extraction for identifying the possible shorted pairs of nets on the basis of the chip layout information, combined with logic-level test pattern generation for bridging faults. Experiments on real design layouts will show that only a fraction of all the possible pairs of nets have non-zero shorting probabilities. Furthermore, it will also be proven at the logic-level that nearly all such bridging faults can be tested by a simple and robust one-pattern logic test. The methods proposed in this paper are supported by a design flow implementing existing commercial and academic CAD software.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4295251"},{"name":"TIP-OPC: a new topological invariant paradigm for pixel based optical proximity correction","snippet":"As the 193 nm lithography is likely to be used for 45 nm and even 32 nm processes, much more stringent requirement will be posed on optical proximity correction (OPC) technologies. Currently, there are two OPC approaches - the model-based OPC (MB-OPC) and the inverse lithography technology (ILT). MB-OPC generates masks which is less complex compared with ILT. But ILT produces much better results than MB-OPC in terms of contour fidelity because ILT is a pixel based method. Observing that MB-OPC preserves the mask shape topologies which leads to a lower mask complexity, we combine the strengths of both methods - the topology invariant property and the pixel based mask representation. To the best of our knowledge, it is the first time that this topological invariant pixel based OPC (TIP-OPC) paradigm is proposed, which fills the critical hole of the OPC landscape and potentially has many new applications. Our technical novelty includes the lithography friendly mask topological invariant operations, the efficient fast Fourier transform based cost function sensitivity computation and the TIP-OPC algorithm. The experimental results show that TIP-OPC can achieve much better post OPC contours compared with MB-OPC while maintaining the mask shape topologies.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4397370"},{"name":"Fault tolerance techniques for high capacity RAM","snippet":"As the complexity and size of the embedded memories keep increasing, improving the yield of embedded memories is the key step toward improving the overall chip yield of a SOC design. The most well known way to improve the memory yield is by using redundant elements to replace the faulty cells. However, the repair efficiency mainly depends on the type, and the amount of redundancy; and on the redundancy analysis algorithms. Therefore, new types of redundancy based on divided bit-line (DBL), and divided word-line (DWL) techniques are proposed in this work. A memory column (row), including the redundant column (row), is partitioned into column blocks (row blocks), respectively. A row\/column block is used as the basic replacement element instead of a row\/column for the traditional approaches. Based on the new types of redundancy, three types of fault-tolerant memory (FTM) systems are also proposed. If a redundant row\/column block is used as the basic replacement element, then the row block-based FTM (RBFTM)\/column block-based (CBFTM) system is used. If both the DWL, and DBL techniques are implemented onto a memory chip, then the hybrid FTM (HFTM) system is achieved. The storage and remapping of faulty addresses can be implemented with a CAM (content addressable memory) block. To achieving better repair efficiency, a novel hybrid block-repair (HBR) algorithm is also proposed. This algorithm is suitable for hardware implementation with negligible overhead. For the HFTM system, the hardware overheads are less than 0.65%, and 0.7% for 64-Kbit SRAM, and 8-Mbit DRAM, respectively. Moreover, the repair rate can be improved significantly. Experimental results show that our approaches can improve the memory fabrication yield significantly. The characteristics of low power and fast access time of DBL and DWL techniques are also preserved.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1638412"},{"name":"Research on software defect prediction based on data mining","snippet":"As the development of computer technology, software system becomes more and more complicated. Because of human's ability limit, there must be a lot of defects generated in the software development life cycle. This paper reviewed the state of art in the field of software defect management and prediction, and presented data mining technology briefly. Finally, proposed an ideal software defect management and prediction system, researched and analyzed several software defect prediction methods based on data mining techniques and specific models (Bayesian Network and PRM). With this system, we can efficiently draw up some prevention and solution scheme to guide the development of new software.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5451355"},{"name":"Software Reliability Modeling with Test Coverage: Experimentation and Measurement with A Fault-Tolerant Software Project","snippet":"As the key factor in software quality, software reliability quantifies software failures. Traditional software reliability growth models use the execution time during testing for reliability estimation. Although testing time is an important factor in reliability, it is likely that the prediction accuracy of such models can be further improved by adding other parameters which affect the final software quality. Meanwhile, in software testing, test coverage has been regarded as an indicator for testing completeness and effectiveness in the literature. In this paper, we propose a novel method to integrate time and test coverage measurements together to predict the reliability. The key idea is that failure detection is not only related to the time that the software experiences under testing, but also to what fraction of the code has been executed by the testing. This is the first time that execution time and test coverage are incorporated together into one single mathematical form to estimate the reliability achieved. We further extend this method to predict the reliability of fault- tolerant software systems. The experimental results with multi-version software show that our reliability model achieves a substantial estimation improvement compared with existing reliability models.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4402193"},{"name":"Bi-direction Motion Vector retrieval based error concealment scheme for H.264\/AVC","snippet":"As the newest video coding standard, H.264\/AVC adopts the high-efficiently predictive coding and variable length entropy coding to achieve high compression efficiency. On the other side, transmission errors become the major problem faced by video broadcasting service providers. Error concealment (EC) here is adopted to handle slices with huge conjunctive corrupted areas inside. Considering error propagation from corrupted slice to succeeding ones is the key factor affecting the video quality, this paper proposes a novel temporal EC scheme including the bi-direction motion vector (MV) retrieval method and an adaptive EC ordering basing on it. Background and motional steady shift part of slice will be given top and second priority, respectively. Combined with our proposed improved boundary matching algorithm (IBMA) which provides more accurate distortion function, experiments results show that our proposal achieves better performance under different error rate channel, compared with EC algorithm adopted in H.264 reference software.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5069214"},{"name":"Design and realization on the fault diagnostic flat based on virtual instrument for warship equipment","snippet":"Based on virtual instrument (VI) technology, Delphi and database, etc., the fault diagnostic flat for shipboard equipment is developed in order to avoid various abuses that conventional methods brought. The modularization and universalization are proposed in its database-based design concept, realized the design of software and hardware. It broke through conventional check diagnosis patterns for warships equipment, resolved difficult to overcome problems brought on adopting existing conventional fashions examined and repaired shipboard equipment, greatly shortened the cycle of maintenance for naval warships equipment. It was proved by experiments that the flat system has merits, such as operation simpleness, high testing precision, strong flexibility and reliability and extensibility, and economical practicability. Also it is of some values for developing the other fault diagnostic instrument.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5626698"},{"name":"Fault Location Using Sparse IED Recordings","snippet":"Basic goal of power system is to continuously provide electrical energy to users. Like with any other system, failures in power system can occur. In those situations it is critical that remedial actions are applied as soon as possible. To apply correct remedial actions it is very important that accurate fault condition and location are detected. In this paper, different fault location algorithms followed with description of intelligent techniques used for implementation of corresponding algorithms are presented. New approach for fault location using sparse measurements is examined. According to available data, it decides between different algorithms and selects an optimal one. New approach is developed by utilizing different data structures in order to efficiently implement algorithm decision engine, which is presented in paper.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4441687"},{"name":"Navigation error analysis for the rocketplane XP","snippet":"bd Systems has supported the navigation architecture development of the Rocketplane XP as part of its responsibility as the vehicle guidance, navigation, and control (GN&C) subsystem lead contractor. The Rocketplane XP is a reusable sub-orbital space-plane being designed by Rocketplane Limited, Inc. for the commercial space tourism market. The horizontal take-off\/horizontal landing vehicle, which will be one of the world's first true manned aerospace vehicles, has a short development schedule and began operation in 2007. bd Systems performed a detailed trade study to select the navigation subsystem and developed high-fidelity error models for use in the overall vehicle six-degree-offreedom (6DOF) simulation in order to determine the effects of navigation errors on vehicle performance.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4798989"},{"name":"Modeling and simulation of inner defect in impulse storage capacitor","snippet":"Because of big capability and small volume, impulse storage capacitor was found that the fast impulse would do much damage to capacitor insulation. Based on the electrical discharge mechanism, several classical defects of capacitor were put forward in this paper. To estimate the status of insulation, the electrical field distribution of defects should be analyzed carefully. As the most popular defect in storage capacitor, inner defect models had been designed for FEA (finite element analysis). Through simulation and analysis, the result proved that the different size and location of inner defect in insulation would result in dissimilar partial concentration and aberrance of electrical field distribution.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1496303"},{"name":"A method for fault diagnosis of analog circuits based on optimized fuzzy inference","snippet":"Because of the difficulty of disposing of fuzzy inference rules for fault diagnosis, a systematic approach for fault diagnosis of analog circuits based on optimized fuzzy inference is presented. The fuzzy logic system for based on fuzzy inference analog circuits fault diagnosis, quantum-inspired evolutionary algorithm (QEA) is used to optimize the membership function of the rules of the fault diagnosis fuzzy logic system, then self adapted genetic algorithm to select the optimum fuzzy rule aggregate, so the number of fuzzy rules is decreased to make it easy to dispose fault diagnosis of analog circuits. The simulation results of a analog power magnifier circuits show the fault diagnosis method of the analog circuits with optimized fuzzy inference is effective.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5626967"},{"name":"An Integrated Platform for Collaborative Simulation and Fault Diagnosis","snippet":"Before application to real manufacturing, thorough evaluation of working performance and the fault diagnosis strategy is absolutely necessary. A comprehensive multifunctional simulation platform based on collaborative simulation and fault diagnosis is presented. Based on virtual prototype, database, network communication and fault diagnosis technologies, the platform allows users to model in a structural and hierarchical way, study the behaviors of individual components and the interaction of subsystems, as well as adjust the fault diagnosis solutions in a simulation environment. An integrated framework is developed to manage the model databases, assemble the models, run the simulation application and display the simulation result. An embedded fault diagnosis module which can help users have a good command of fault recovery is employed to verify the reliability of fault diagnosis solutions. The developed integrated platform is shown to be a valuable tool both for performance simulation and fault diagnosis education. A case of a large-scale erecting vehicle has been introduced to demonstrate the concept","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4018800"},{"name":"A Fault-Tolerant Framework for Web Services","snippet":"Being the new generation middleware, Web services have been enjoying great popularities in recent years. The high usability of the Web service is becoming a new focus for research. According to the demands of Web services, a fault-tolerant Web services framework named FW4WS is presented. In the article, we set forth the framework and the workflow of the system in detail.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5319378"},{"name":"Robot error detection using an artificial immune system","snippet":"Biology has produced living creatures that exhibit remarkable fault tolerance. The immune system is one feature that enables this. The acquired immune system learns during the life of the individual to differentiate between self (that which is normally present) and non-self (that which is not normally present). This paper describes an artificial immune system (AIS) that is used as an error detection system and is applied to two different robot based applications; the immunization of a fuzzy controller for a Khepera robot that provides object avoidance and a control module of a BAE Systems RASCAL<sup>TM<\/sup> robot. The AIS learns normal behavior (unsupervised) during a fault free learning period and then identifies all error greater that a preset error sensitivity. The AIS was implemented in software but has the potential to be implemented in hardware. The AIS can be independent to the system under test, just requiring the inputs and outputs. This is not only ideal in terms of common mode and design errors but also offers the potential of a general, off-the-shelf, error detection system; the same AIS was applied to both the applications.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1217667"},{"name":"Error Evaluation of BAQ Algorithm for Internal Calibration Data of Spaceborne SAR","snippet":"BAQ is the efficient algorithm for SAR echoes data compression that has the Gauss distribution. Internal calibration is the important system resource, which is to calibrate the system gain change before and after the imaging and also provide more accurate reference function for the range compression. But the echoes are different from calibration data. This paper discussed whether the error brought by BAQ compression algorithm could be accepted by this system. Both the theoretical simulation and the experimental results indicate that the internal calibration data only can be directly transmitted but not compressed, which has the important reference to engineering design.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4241351"},{"name":"A Color Error Correcting Model for Scanning Input Image","snippet":"Based on analyzing the color rendering principle of scanning objects and the causes of color error, a new algorithm of color space conversion for scanning image is proposed. First, some parameters of Neugebauer equation and Yule-Nielson equation are reinterpreted which makes them able to be used for non-dot image and which can only be used for dot image originally. Then, the paper presents the deduction procedures of the color correction equation. Finally the experimental results show that the algorithm may result in more accurate approximation compared with some typical and mainstreams.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4089326"},{"name":"Fault diagnosis of power circuits based on SVM ensemble with quantum particles swarm optimization","snippet":"Based on least squares wavelet support vector machines (LS-WSVM) ensemble with quantum particle swarm optimization algorithm (QPSO), a systematic method for fault diagnosis of power circuits is presented. Firstly, wavelet coefficients of output voltage signals of power circuits under faulty conditions are obtained with wavelet lifting decomposition, and then faulty feature vectors are extracted from the disposed wavelet coefficients. Secondly, a boosting strategy is adopted to select faulty feature vectors automatically for LS-WSVM-based multi-class classifiers, QPSO is applied to select the optimal values of the regularization and kernel parameters of multi-class LS-WSVM. So the multi-class LS-WSVM ensemble model with boosting for the power circuits fault diagnosis system is built. The simulation result of push-pull circuits shows that the fault diagnosis method of the power circuits using LS-WSVM ensemble with QPSO is effective.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4776246"},{"name":"Fault diagnosis for power circuits based on SVM within the Bayesian framework","snippet":"Based on least squares wavelet support vector machines (LS-WSVM) within the Bayesian evidence framework, a systematic method for fault diagnosis of power circuits is presented. In this paper, the Bayesian evidence framework is applied to select the optimal values of the regularization and kernel parameters of multi-class LS-WSVM classifiers. Also wavelet coefficients of output voltage signals of power circuits under faulty conditions are obtained with wavelet lifting decomposition, and then faulty feature vectors are extracted from the disposed wavelet coefficients. The faulty feature vectors are used to train the multi-class LS-WSVM classifiers, so the model of the power circuits fault diagnosis system is built. In push-pull circuits, this method is applied to diagnose the faults of the circuits with simulation; the results show that the fault diagnosis method of the power circuits with LS-WSVM within the Bayesian evidence framework is effective.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4593762"},{"name":"Based on Compact Type of Wavelet Neural Network Tolerance Analog Circuit Fault Diagnosis","snippet":"Based on the classical wavelet neural network, this paper put forward a sort of improved multiple-input multiple-output compact type of wavelet neural network, adopted adaptive learning rate and additional momentum BP algorithm to carry out training, studied its tolerance analog circuit fault diagnosis applications. Simulation results displayed that the compact type of wavelet neural network learning is fast, it can be effective diagnosed and located to tolerance analog circuit fault.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5363065"},{"name":"Research on Multi-Sensor Information Fusion for the Detection of Surface Defects in Copper Strip","snippet":"Based on the defects detection on the surface of the copper strips, this paper firstly studies how to enhance system stability with the multi-sensors information fusion method. This method combines infrared, visible light and laser sensors to deal with defects detection, utilizes fuzzy logic and neural network to carry on the sensor's management, and uses wavelet transformation in image fusion. Experimental results show that this method can effectively detect surface defects in copper strips. Furthermore, it enhances the accuracy of recognizing and classifying, and makes the overall system more automatic and intelligent.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5365587"},{"name":"A Demonstration on Influencing Factors of FDI Location Choice: Based on Co-Integration and Error Correction Model","snippet":"Based on the relevant data in the year of 1983-2008,the paper utilizes Co-integration Test and Error Correction Model to inspect the influencing factors on FDI Location Choice in China. With a such research, the paper comes to the conclusion, that is,there is a long co-integration relation among them,fixed asset investment,R, former accumulated foreign capital amount, loan balance of financial corporation and FDI have a positive correlation, while W,the city person and FDI have a negative correlation;This paper can provide policy reference.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5664938"},{"name":"Using a periodic square wave test signal to detect crosstalk faults","snippet":"Built-in self test (BIST) scheme simplifies the detection of crosstalk faults in deep-submicron VLSI circuits in the boundary scan environment. The scheme tests for crosstalk faults with a periodic square wave test signal under applied random patterns generated by a linear feedback shift register (LFSR), which is transconfigured from the embedded circuit's boundary scan cells. The scheme simplifies test generation and test application while obviating the fault occurrence timing issue. Experimental results show that coverage for the induced-glitch type of crosstalk fault for large benchmark circuits can easily exceed 90%.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1413150"},{"name":"Design and implementation of Weapons Fault Diagnosis Expert System Platform","snippet":"By analyzing the distinguishing features of the current popular weapons fault diagnosis expert system, as well as using component-based software design and complex knowledge representation methods, this paper proposes a solution of overall design and a description of user interfaces about the Weapons Fault Diagnosis Expert System Platform. It studies and discusses the complex knowledge representation about the reusable expert system. The implementation essentials in a variety of reasoning mechanisms are also discussed. It is proved that, a special weapons fault diagnosis expert system can be generated using this expert system platform and the special knowledge of the weapons fault.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5620629"},{"name":"Mechanism of Defects Formation in Ti<inf>1-x<\/inf>V<inf>x<\/inf>CoSb Semiconductor Solid Solution","snippet":"By means of of the combined X-ray diffraction the different ratio of the lattice sites occupation by Co and (Ti,V) atoms in the Ti<sub>1-x<\/sub>V<sub>x<\/sub>CoSb crystals was found. This is equal to doping the TiVCoSb semiconductor with two kinds of acceptor impurities. The break of metallic conductivity in the n-type semiconductor at the increase of the donor impurity concentration was revealed, and it was explained by the simultaneous initiation of acceptor impurity.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4373594"},{"name":"Byzantine fault tolerance can be fast","snippet":"Byzantine fault tolerance is important because it can be used to implement highly-available systems that tolerate arbitrary behavior from faulty components. We present a detailed performance evaluation of BFT, a state-machine replication algorithm that tolerates Byzantine faults in asynchronous systems. Our results contradict the common belief that Byzantine fault tolerance is too slow to be used in practice, BFT performs well so that it can be used to implement real systems. We implemented a replicated NFS file system using BFT that performs 2% faster to 24% slower than production implementations of the NFS protocol that are not fault-tolerant.","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=941437"},{"name":"REIK:  A Novel P2P Overlay Network with Byzantine Fault Tolerance","snippet":"Byzantine faults in a peer-to-peer (P2P) system are re- sulted from adversarial and inconsistent peer behaviors. Faulty peers can disrupt the routing functions in the peer joining and lookup schemes. Byzantine attackers may col- lude with each other to paralyze the entire P2P network op- erations. We discover a novel DHT-based overlay networks (REIK) with Byzantine fault tolerance. REIK based on a ring which embeds an inverse Kautz digraph IK(d, m), to enable multi-path P2P routing. The inverse Kautz network provides multiple entry points and multiple routes between node pair. The REIK overlay is the first constant degree and O(log n) diameter DHT scheme with constant congestion and Byzantine fault tolerance. For large d 2, the REIK overlays handle random and Byzantine faults effectively, far beyond the capability of Chord and CAN.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4438525"},{"name":"Error analysis and experimental tests of CATRASYS (Cassino Tracking System)","snippet":"CATRASYS (Cassino Tracking System) is a low-cost, easyily operated system for monitoring large displacements together with rotation angles of a suitable end-effector, which can be easily attached to any mechanical system. In this paper we present basic performance of CATRASYS by using an analysis for error evaluation and showing experimental tests that have been carried out at the Laboratory of Robotics and Mechatronics in Cassino with available robots","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=972368"},{"name":"A Block Device Driver for Parallel and Fault-tolerant Storage","snippet":"Cauchy-Reed\/Solomon is an XOR-based erasure-tolerant coding scheme which is widely used for reliable distributed storage and fault-tolerant memory. A variety of different codes can be specified, depending on the number of parallel operating storage resources and the desired strength of fault tolerance. First we present an approach to parameterize the codes for different systems and requirements, such as the desired parallelism and reliability. Based on this parameterization, a Linux block device driver was developed which is evaluated in this paper.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5758986"},{"name":"A Survey of Methods for Detection of Stator-Related Faults in Induction Machines","snippet":"As evidenced by industrial surveys, stator-related failures account for a large percentage of faults in induction machines. The objective of this paper is to provide a survey of existing techniques for detection of stator-related faults, which include stator winding turn faults, stator core faults, temperature monitoring and thermal protection, and stator winding insulation testing. The root causes of fault inception, available techniques for detection, and recommendations for further research are presented. Although the primary focus is online and sensorless methods that use machine voltages and currents to extract fault signatures, offline techniques such as partial discharge detection are also examined. Condition monitoring, fault diagnostics, insulation testing, interlaminar core faults, partial discharge (PD), temperature monitoring, turn faults.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4276867"},{"name":"Bright-field AAPSM conflict detection and correction","snippet":"As feature sizes shrink, it will be necessary to use AAPSM (alternating-aperture phase shift masking) to image critical features, especially on the polysilicon layer. This imposes additional constraints on the layouts beyond traditional design rules. Of particular note is the requirement that all critical features be flanked by opposite-phase shifters, while the shifters obey minimum width and spacing requirements. A layout is called phase-assignable if it satisfies this requirement. If a layout is not phase-assignable, the phase conflicts have to be removed to enable the use of AAPSM for the layout. Previous work has sought to detect a suitable set of phase conflicts to be removed, as well as correct them. The contributions of this paper are the following: (1) a new approach to detect a minimal set of phase conflicts (also referred to as AAPSM conflicts), which when corrected will produce a phase-assignable layout; (2) a novel layout modification scheme for correcting these AAPSM conflicts. The proposed approach for conflict detection shows significant improvements in the quality of results and runtime for real industrial circuits, when compared to previous methods. To the best of our knowledge, this is the first time layout modification results are presented for bright-field AAPSM. Our experiments show that the percentage area increase for making a layout phase-assignable ranges from 0.7-11.8%.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1395700"},{"name":"A technique for real-time correction of measurement instrument transducers frequency responses","snippet":"As it is well known, transducers are the most important components of a measurement system: they converts the physical quantity, which has to be measured, in an electrical one, processed in turns by electronic instruments. It is also known that they are the major source of uncertainty, which it is desirable to be as low as possible; obviously, their costs grow, sometimes exponentially, with their performances. Since digital measurement equipments, based on data acquisition systems and digital processors, represent nowadays the core of most electronic measuring instrument, it is possible to use them in order to compensate errors coming from transducers, without increasing their costs. So in this paper a digital technique for the correction of transducers errors is presented: it has got a low computational burden, and therefore it can be implemented in real-time even on low cost digital processors.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4547382"},{"name":"Trace-based microarchitecture-level diagnosis of permanent hardware faults","snippet":"As devices continue to scale, future shipped hardware will likely fail due to in-the-field hardware faults. As traditional redundancy-based hardware reliability solutions that tackle these faults will be too expensive to be broadly deployable, recent research has focused on low-overhead reliability solutions. One approach is to employ low-overhead (ldquoalways-onrdquo) detection techniques that catch high-level symptoms and pay a higher overhead for (rarely invoked) diagnosis. This paper presents trace-based fault diagnosis, a diagnosis strategy that identifies permanent faults in microarchitectural units by analyzing the faulty corepsilas instruction trace. Once a fault is detected, the faulty core is rolled back and re-executes from a previous checkpoint, generating a faulty instruction trace and recording the microarchitecture-level resource usage. A diagnosis process on another fault-free core then generates a fault-free trace which it compares with the faulty trace to identify the faulty unit. Our result shows that this approach successfully diagnoses 98% of the faults studied and is a highly robust and flexible way for diagnosing permanent faults.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4630067"},{"name":"Secure and fault-tolerant voting in distributed systems","snippet":"Concerns about both security and fault-tolerance have had an important impact on the design and use of distributed information systems in the past. As such systems become more prevalent, as well as more pervasive, these concerns will become even more immediately relevant. We focus on integrating security and fault-tolerance into one, general-purpose protocol for secure distributed voting. Distributed voting is a well-known fault-tolerance technique. For the most part, however, security had not been a concern in systems that used voting. More recently, several protocols have been proposed to shore up this lack. These protocols, however, have limitations which make them particularly unsuitable for many aerospace applications, because those applications require very flexible voting schemes (e.g., voting among real-world sensor data). We present a new, more general voting protocol that reduces the vulnerability of the voting process to both attacks and faults. The algorithm is contrasted with the traditional 2-phase commit protocols typically used in distributed voting and with other proposed secure voting schemes. Our algorithm is applicable to exact and inexact voting in networks where atomic broadcast and predetermined message delays are present, such as local area networks. For wide area networks without these properties, we describe yet another approach that satisfies our goals of obtaining security and fault tolerance for a broad range of aerospace information systems","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=931341"},{"name":"Fault-tolerant aspects of MPC","snippet":"Concerns control in the event of equipment failure. Model predictive control (MPC) offers a promising basis for fault-tolerant control. Since MPC relies on an explicit internal model, one could dealing with failures by updating the internal model, and letting the online optimiser work out how to control the system in its new condition. This relies on several assumptions: that the nature of the fault can be located, and its effects modelled; that the model can be updated, essentially automatically; and that the control objectives can be left unaltered after the failure. The first 2 of these may be possible using fault detection and isolation (FDI), and the management of complex models. The technologies concerned seem to offer a very powerful combination","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=847005"},{"name":"Falcon: fault localization in concurrent programs","snippet":"Concurrency fault are difficult to find because they usually occur under specific thread interleavings. Fault-detection tools in this area find data-access patterns among thread interleavings, but they report benign patterns as well as actual faulty patterns. Traditional fault-localization techniques have been successful in identifying faults in sequential, deterministic programs, but they cannot detect faulty data-access patterns among threads. This paper presents a new dynamic fault-localization technique that can pinpoint faulty data-access patterns in multi-threaded concurrent programs. The technique monitors memory-access sequences among threads, detects data-access patterns associated with a program's pass\/fail results, and reports dataaccess patterns with suspiciousness scores. The paper also presents the description of a prototype implementation of the technique in Java, and the results of an empirical study we performed with the prototype on several Java benchmarks. The empirical study shows that the technique can effectively and efficiently localize the faults for our subjects.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6062092"},{"name":"Which concurrent error detection scheme to choose ?","snippet":"Concurrent error detection (CED) techniques (based on hardware duplication, parity codes, etc.) are widely used to enhance system dependability. All CED techniques introduce some form of redundancy. Redundant systems we subject to common-mode failures (CMFs). While most of the studies of CED techniques focus on area overhead, few analyze the CMF vulnerability of these techniques. In this paper, we present simulation results to quantitatively compare various CED schemes based on their area overhead and the protection (data integrity) they provide against multiple failures and CMFs. Our results indicate that, for the simulated combinational logic circuits, although diverse duplex systems (with two different implementations of the same logic function) sometimes have marginally higher area overhead, they provide significant protection against multiple failures and CMFs compared to other CED techniques like parity prediction","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=894311"},{"name":"A study of the internal and external effects of concurrency bugs","snippet":"Concurrent programming is increasingly important for achieving performance gains in the multi-core era, but it is also a difficult and error-prone task. Concurrency bugs are particularly difficult to avoid and diagnose, and therefore in order to improve methods for handling such bugs, we need a better understanding of their characteristics. In this paper we present a study of concurrency bugs in MySQL, a widely used database server. While previous studies of real-world concurrency bugs exist, they have centered their attention on the causes of these bugs. In this paper we provide a complementary focus on their effects, which is important for understanding how to detect or tolerate such bugs at run-time. Our study uncovered several interesting facts, such as the existence of a significant number of latent concurrency bugs, which silently corrupt data structures and are exposed to the user potentially much later. We also highlight several implications of our findings for the design of reliable concurrent systems.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5544315"},{"name":"Comparison of Morlet wavelet filter for defect diagnosis of bearings","snippet":"Condition monitoring helps to avoid unexpected failures of equipments. Rolling element bearings are critical components in rotating equipments. Vibration analysis is a common method used for defect detection and diagnosis of rotating equipments without effecting their operation. The measured vibration signal contains noise, modulations and low frequency components due to unbalance, misalignment, structural losseness etc. Since the impulses due to bearing defects are having low amplitude, it is difficult to detect and identify the location of bearing defect from raw vibration signals, especially during the initial stages of defect development. Since there are more transfer segments, detection of inner race and rolling element defects are also challenging. Morlet wavelet filter (MWF) can be used for denoising of vibration signals so that condition monitoring of bearings can be performed from the denoised signals. The parameters of the wavelet need to be optimized before denoising is performed. Two algorithms used for optimization of MWF are compared in this paper. First algorithm use Shannon entropy and kurtosis for optimization of shape factor and scale of the wavelet respectively. Second algorithm uses kurtosis for optimization of wavelet parameters. Experiments are performed to obtain vibration signal of bearings with defect induced in the rolling element. MWF optimized using the proposed methods were used to denoise the vibration signals. The filtered signals are compared and performances of the algorithms are evaluated.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5779584"},{"name":"Correction of scattered radiation for cone-beam computed tomography at high X-ray energies","snippet":"Cone-beam computed tomography (CT) using X-ray tubes of high energy (450 keV) faces the problem of strong artifacts and a significant contrast degradation in reconstructed images. System components of cone-beam CT scanners operating at high X-ray energies have to be optimized to reduce the amount of scattered photons hitting the detector. In addition it is mandatory to apply scatter correction algorithms. A prototype of a cone-beam CT system equipped with a 450 kV industrial X-ray tube has been developed within the framework of a European research project. The influences of scattered radiation generated by the object have been extensively evaluated using Monte Carlo (MC) simulations. Furthermore scattering reduction and correction methods have been developed. A key task was the implementation of a new hybrid method for the fast and accurate calculation of the scattering intensity distribution in X-ray projections for industrial cone-beam CT.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4775231"},{"name":"Fault diagnosis box based on Cloud Computing","snippet":"Considering the development of Internet and the diversity and complication of fault diagnosis, combined with the practice, we put forward a kind of fault diagnosis box. Efficiency of the fault diagnosis will be significantly improved based on internet. The user input his requirement in the terminal by way of diagnosis box, and then diagnosis platform can analyze the user's needs. The service which can satisfy the need is based on Cloud Computing, and the Cloud Computing can make many expensive hardware resources shared.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5568951"},{"name":"The selection and creation of the rules in rules-based optical proximity correction","snippet":"Considering the efficiency and veracity of rules-based OPC applied to recent large-scale layout, we firstly point out the importance of the selection and creation of rules in rules-based OPC. Our discussion addresses the crucial factors in selecting and creating rules as well as how we select and create more concise and practical rules-base. Based on our ideas we suggest four primary rules and as a result we show some rule data in table. The automatic construction of the rules-base called OPCL is an important part of the whole rules-based OPC software","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=982496"},{"name":"A Novel SVC VoD System with Rate Adaptation and Error Concealment over GPRS\/EDGE Network","snippet":"Considering the high packet losses and low, varying bandwidth of GPRS\/EDGE network and limited computation power of handheld devices, we present and implement a novel SVC video-on-demand system for hand-held devices over GPRS\/EDGE network. For the purpose of handling varying bitrate, we propose a priority based layer switching (PLS) adaptation scheme for SVC stream, which not only performs online 3-D adaptation more quickly by a simple parser, but also optimizes the video quality in a R-D sense under the bandwidth constraint. To resist packet losses, a motion-detection based adaptive error concealment (MDA) algorithm is proposed, which can achieve a PSNR gain of up to 3dB comparing to existing methods, while maintaining low complexity. Moreover, our proposed system was implemented and tested over existing GPRS\/EDGE network deployed in China. The test results demonstrate that the proposed system and schemes have performance advantage in terms of quicker data rate adaptation, higher PSNR and lower overhead.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4566177"},{"name":"Fine-Grained Fault Tolerance for Process Variation-Aware Caches","snippet":"Continuous scaling in CMOS fabrication process makes circuits more vulnerable to process variations, which results in variable delay, malfunctioning, and\/or leaky circuits. Caches are one of the biggest victims of process variations due to their large sizes and minimal cell features. To mitigate the impacts of process variations on caches, we propose to localize the effects of process variations at a word level, not at the conventional cache set, cache way, or cache line level. Faulty words are disabled or shut down completely and accesses to those words are bypassed to a small set of word-length buffers. This technique is shown to be effective in reducing performance penalty due to process variations and in increasing the parametric yield up to 90% when subjected to the performance constraints.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5572757"},{"name":"A fault-tolerant P-Q decoupled control scheme for static synchronous series compensator","snippet":"Control of nonlinear devices in power systems relies on the availability and the quality of sensor measurements. Measurements can be corrupted or interrupted due to sensor failure, broken or bad connections, bad communication, or malfunction of some hardware or software (referred to as missing sensor measurements in this paper). This paper proposes a fault-tolerant control scheme (FTCS) for a static synchronous series compensator (SSSC). This FTCS consists of a sensor evaluation and (missing sensor) restoration scheme (SERS) cascaded with a P-Q decoupled control scheme (PQDC). It is able to provide effective control to the SSSC when single or multiple crucial sensor measurements are unavailable. Simulation studies are carried out to examine the validity of the proposed FTCS. During the simulations, single and multiple phase current sensors are assumed to be missing, respectively. Results show that the SERS restores the missing data correctly during steady and transient states, including small and large disturbances, and unbalanced three-phase operation. Thus, the FTCS continuously provides effective control to the SSSC with and without missing sensor measurements","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1709260"},{"name":"Missing-Sensor-Fault-Tolerant Control for SSSC FACTS Device With Real-Time Implementation","snippet":"Control of power systems relies on the availability and quality of sensor measurements. However, measurements are inevitably subjected to faults caused by sensor failure, broken or bad connections, bad communication, or malfunction of some hardware or software. These faults, in turn, may cause the failure of power system controllers and consequently, severe contingencies in the power system. To avoid such contingencies, this paper presents a sensor evaluation and (missing sensor) restoration scheme (SERS) by using auto-associative neural networks (auto encoders) and particle swarm optimization. Based on the SERS, a missing-sensor-fault-tolerant control is developed for controlling a static synchronous series compensator (SSSC) connected to a power network. This missing-sensor fault-tolerant control (MSFTC) improves the reliability, maintainability, and survivability of the SSSC and the power network. The effectiveness of the MSFTC is demonstrated by a real-time implementation of an SSSC connected to the IEEE 10-machine 39-bus system on a Real Time Digital Simulator and TMS320C6701 digital signal processor platform. The proposed fault-tolerant control can be readily applied to many existing controllers in power systems.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4801588"},{"name":"A fault location algorithm for urban distribution network with DG","snippet":"Conventional power distribution system is radial in nature, characterized by a single source feeding a network of downstream feeders. The distribution automation fault location, primarily considering the fault current amplitude signals, has traditionally been designed assuming the system to be radial. However, the system with distributed generation (DG) may no longer radial, which means more fault direction signals to be needed for fault location. The paper suggests a novel algorithm, based on fault current amplitude difference of zone. The algorithm realizes direction detection using the difference of short - circuit capacity between system source and DG. The PSS\/E simulation results indicate the method could locate fault zone correctly in urban distribution network with DG.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4523852"},{"name":"Improving error resilience of scalable H.264 (SVC) via drift control","snippet":"Common error concealment schemes mitigate errors for frames in which losses occur only, even though errors propagate to future frames. Drift control is generally challenging due to lack of reliable basis for determining what needs to be corrected and how. In this paper, we show that for scalable or multi-layer video, an available base-layer can serve as such basis to allow continous error drift checking and correction of higher layers even when the base-layer is of much lower spatial resolution. The associated algorithm is low-complexity, incurs no additional bit cost, and experiments using SVC reference software show PSNR improvement of up to 5 dB over concealment methods without drift control.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5495975"},{"name":"Comparison and application of different VHDL-based fault injection techniques","snippet":"Compares different VHDL-based fault injection techniques: simulator commands, saboteurs and mutants for the validation of fault tolerant systems. Some extensions and implementation designs of these techniques have been introduced. Also, a wide set of non-usual fault models have been implemented. As an application, a fault tolerant microcomputer system has been validated. Faults have been injected using an injection tool developed by the GSTF. We have injected both transient and permanent faults on the system model, using two different workloads. We have studied the pathology of the propagated errors, measured their latencies, and calculated both detection and recovery coverages. Preliminary results show that coverages for transient faults can be obtained quite accurately with any of the three techniques. This enables the use of different abstraction level models for the same system. We have also verified significant differences in implementation and simulation cost between the studied injection techniques","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=966775"},{"name":"Silk Texture Defect Recognition System Using Computer Vision and Artificial Neural Networks","snippet":"Competiveness of textile industries depends on the quality control of production. In order to minimize production cost, effort is directed towards less defectiveness and time spent on production operations. More accuracy in silk texture defect identification should be maintained so as eliminate any abnormality in the silk texture that hinders its acceptability by the consumer. In this paper, silk texture defect identification is achieved by implementing artificial neural network (ANN) technique. Methodology for feature selection that leads to high recognition rates and to simpler classification systems architectures is presented.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5303972"},{"name":"Using composition to design secure, fault-tolerant systems","snippet":"Complex systems must be analyzed in smaller pieces. Analysis must support both bottom-up (composition) and top-down (refinement) development, and it must support the consideration of several critical properties, e.g., functional correctness, fault tolerance and security, as appropriate. We describe a mathematical framework for performing composition and refinement analysis and discuss some lessons learned from its application. The framework is written and verified in PVS","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=821535"},{"name":"Error propagation in the reliability analysis of component based systems","snippet":"Component based development is gaining popularity in the software engineering community. The reliability of components affects the reliability of the system. Different models and theories have been developed to estimate system reliability given the information about system architecture and the quality of the components. Almost always in these models a key attribute of component-based systems, the error propagation between the components, is overlooked and not taken into account in the reliability prediction. We extend our previous work on Bayesian reliability prediction of component based systems by introducing the error propagation probability into the model. We demonstrate the impact of the error propagation in a case study of an automated personnel access control system. We conclude that error propagation may have a significant impact on the system reliability prediction and, therefore, future architecture-based models should not ignore it","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1544721"},{"name":"Testing Approach of Component Security Based on Fault Injection","snippet":"Component-Based Software Engineering (CBSE) has been the research focus in the field of software engineering at present. But problems with the reliability and security of components have not yet been resolved, which worried the component developer and user. Testing the software components is an important approach which guarantees and enhances their reliability and security. This paper proposes a testing approach of component security based on fault injection (TAFI), and then defines and discusses requirement specification of components security and fault injection model. In addition, 31 software components are analyzed using our approach based on fault injection model. The case study shows that our approach is effective and operable.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4415448"},{"name":"A Flexible Fault-Tolerance Mechanism for the Integrade Grid Middleware","snippet":"Computer grids have attracted great attention of both academic and enterprise communities, becoming an attractive alternative for the execution of applications that demand huge computational power, allowing the integration of computational resources spread through different administrative domains. The dynamic nature of the grid infrastructure, its high scalability, and great heterogeneity exacerbates the likelihood of errors occurrence, imposing fault tolerance as a major requirement for grid middlewares. This paper describes a flexible fault-tolerance mechanism implemented on integrate grid middleware that allows the customization of several fault tolerance parameters and the combination of different fault tolerance techniques. This paper also presents several experiments that measure the benefits of our approach, considering several different execution environments scenarios.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4438275"},{"name":"Characterizing Microarchitecture Soft Error Vulnerability Phase Behavior","snippet":"Computer systems increasingly depend on exploiting program dynamic behavior to optimize performance, power and reliability. Prior studies have shown that program execution exhibits phase behavior in both performance and power domains. Reliabilityoriented program phase behavior, however, remains largely unexplored. As semiconductor transient faults (soft errors) emerge as a critical challenge to reliable system design, characterizing program phase behavior from a reliability perspective is crucial in order to apply dynamic fault-tolerant mechanisms and to optimize performance\/reliability trade-offs. In this paper, we compute run-time program vulnerability to soft errors on four microarchitecture structures (i.e. instruction window, reorder buffer, function units and wakeup table) in a high-performance out-of-order execution superscalar processor. Experimental results on the SPEC2000 benchmarks show a considerable amount of time varying behavior in reliability measurements. Our study shows that a single performance metric, such as IPC, cache miss or branch misprediction, is not a good indicator for program vulnerability. The vulnerabilities of the studied microarchitecture structures are then correlated with program code-structure and run-time events to identify vulnerability phase behavior. We observed that both program code-structure and run-time events appear promising in classifying program reliability phase behavior. Overall, performance counter based schemes achieved an average Coefficient of Variation (COV) of 3.5%, 4.5%, 4.3% and 5.7% on the instruction queue, reorder buffer, function units and the wakeup table, while basic block vectors offer COVs of 4.9%, 5.8%, 5.4% and 6% on the four studied microarchitecture structures respectively. We found that in general, tracking performance metrics performs better than tracking control flow in identifying reliability phase behavior of applications. To our knowledge, this paper is the first to characterize program reliability phase behavior at the microarchitecture level.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1698546"},{"name":"Optimal cost-effective design of parallel systems subject to imperfect fault-coverage","snippet":"Computer-based systems intended for critical applications are usually designed with sufficient redundancy to be tolerant of errors that may occur. However, under imperfect fault-coverage conditions (such as the system cannot adequately detect, locate, and recover from faults and errors in the system), system failures can result even when adequate redundancy is in place. Because parallel architecture is a well-known and powerful architecture for improving the reliability of fault-tolerant systems, this paper presents the cost-effective design policies of parallel systems subject to imperfect fault-coverage. The policies are designed by considering (1) cost of components, (2) failure cost of the system, (3) common-cause failures, and (4) performance levels of the system. Three kinds of cost functions are formulated considering that the total average cost of the system is based on: (1) system unreliability, (2) failure-time of the system, and (3) total processor-hours. It is shown that the MTTF (mean time to failure) of the system decreases by increasing the spares beyond a certain limit. Therefore, this paper also presents optimal design policies to maximize the MTTF of these systems. The results of this paper can also be applied to gracefully degradable systems.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1181898"},{"name":"Overview of an automatic underground distribution fault location system","snippet":"Con Edison uses power quality monitors to locate faults on its primary distribution underground network. The power quality monitors serve as the voltage and current sensors in an automatic fault location system. Fault measurements captured by the meters are downloaded automatically, integrated into a relational database, and processed for impedance calculations. The impedance calculations combined with up-to-date distribution circuit models and geographic information system data are used to build estimated fault location tables and map displays. The systems are integrated on Con Edison's intranet and used in real-time by numerous groups within Con Edison including operations, system protection, and power quality. The system can detect and locate both single-phase faults and multi-phase faults. It sends alerts when subcycle faults and magnetizing inrush current transients are detected. For single-phase faults, the system's accuracy regularly exceeds 80% success in estimating the fault location within 10% of the total number of the feeder structures. In 2008, the system was expanded to incorporate data from feeder relays and in 2009 it may be expanded to include data from transmission digital fault recorders. This document will present an overview of some of the parameters and practices for finding faults in place every day at Con Edison.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5275256"},{"name":"Real-time correction of distortion image based on FPGA","snippet":"Correcting infrared camera distortion is necessary in target tracking and object recognition system. The existent FPGA algorithm didn't utilize sufficiently the advantage of the parallel processing and leaded to the results that a great deal of the system resources were consumed and the running speed was slowed down. The paper analyzed the existing problems such as serial structure in other algorithms, proposed a new parallel algorithm and realized it with the lowest resources. The experiments carried on the chip-virtex5 produced by Xilinx company show that the proposed algorithm has a good real-time performance, use less resource than previous structure and realize the correction of distortion on FPGA on line.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5656804"},{"name":"A class of random multiple bits in a byte error correcting and single byte error detecting (S<sub>t<\/sub>b\/EC-S<sub>b<\/sub>ED) codes","snippet":"Correcting multiple random bit errors that corrupt a single DRAM chip becomes very important in certain applications, such as semiconductor memories used in computer and communication systems, mobile systems, aircraft, and satellites. This is because, in these applications, the presence of strong electromagnetic waves in the environment or the bombardment of an energetic particle on a DRAM chip is highly likely to upset more than just one bit stored in that chip. On the other hand, entire chip failures are often presumed to be less likely events and, in most applications, detection of errors caused by single chip failures are preferred to correction due to check bit length considerations. Under this situation, codes capable of correcting random multiple bit errors that are confined to a single chip output and simultaneously detecting errors caused by single chip failures are attractive for application in high speed memory systems. This paper proposes a class of codes called Single t\/b-error Correcting-Single b-bit byte Error Detecting (S<sub>t<\/sub>b\/EC-S<sub>b<\/sub>ED) codes which have the capability of correcting random t-bit errors occurring within a single b-bit byte and simultaneously indicating single b-bit byte errors. For the practical case where the chip data output is 8 bits, i.e., b = 8, the S<sub>3<\/sub>8\/EC-S<sub>8<\/sub>ED code proposed in this paper, for example, requires only 12 check bits at information length 64 bits. Furthermore, this S<sub>3<\/sub>8\/EC-S<sub>8<\/sub>ED code is capable of correcting errors caused by single subarray data faults, i.e., single 4-bit byte errors, as well. This paper also shows that perfect S<sub>(b-t)<\/sub>b\/EC-S<sub>b<\/sub>ED codes, i.e., perfect S<sub>t<\/sub>b\/EC-S<sub>b<\/sub>ED codes for the case where t = b - 1, do exist and provides a theorem to construct these codes.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1214333"},{"name":"Error Correction of Noisy Interleaved Block Ciphers","snippet":"Correction of noisy cipher is a challenging task. Classical error detection and correction methods are not suitable for encrypted data. Previous work has been done on correcting noisy block ciphers using cipher and plaintext characteristics. For certain amount of errors, when error correction using cipher characteristics fails, the language properties of plaintext data were used, instead, to eliminate noise. However, this method requires an iterative process, and there are cases that may occur when unique solution cannot be achieved. In this paper, error detection and correction is performed at the receiver end, without any changes to the encryption algorithm, using only cipher characteristics. Interleaving the cipher text before transmission and deinterleaving after reception cause bursts of channel errors to be spread out in time, and, thus, to be in the correction capability of, only, cipher characteristic approach.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5701898"},{"name":"Soft errors: is the concern for soft-errors overblown?","snippet":"Cosmic ray particles have the ability to either toggle the state of memory elements or create unwanted glitches in combinational logic that may be latched by memory elements. As supply voltages reduce and feature sizes become smaller in future technologies, soft error tolerance is considered a significant challenge for designing future electronic systems. In some cases, the impact of soft errors is easily overblown. The real challenge at hand is to consider the kind of soft error protection and recovery mechanisms that can be provided while meeting other system parameters such as power consumption, performance, area usage and criticality of a failure. Another challenge is to understand the interactions of other optimizations targeted at other constraints such as performance or power consumption with soft error rates. It is important for system designers to perform soft error analysis to avoid repetitions of widely-publicised soft error failures","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1584102"},{"name":"DMT and DT2: two fault-tolerant architectures developed by CNES for COTS-based spacecraft supercomputers","snippet":"COTS (commercial off-the-shelf) electronic components are attractive for space applications. However, computer designers need to solve a main problem as regards their SEE (single event effect) sensitivity. The purpose of fault tolerance studies conducted at CNES (the French Space Agency) is to prepare the space community for the significant evolution linked to the usage of COTS components. CNES has patented two fault-tolerant architectures with low recurring costs, mass and power consumption, as compared to conventional architectures as e.g. the TMR (triple modular redundancy) one. The former, referred to as DMT, is time redundancy based and minimises recurring costs. It is mainly intended for but not limited to scientific missions. The latter, referred to as DT2, is based on a structural duplex architecture with minimum duplication and is suited for high-end application missions","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1655550"},{"name":"Simulation of Attacks on Network-based Error Detection","snippet":"CRC and checksum are two error-detecting mechanisms widely used in the computer network. A novice evaluating simulation model based on attacking these two codes is proposed and the correspondent evaluating methods are discussed. In this model, the size and content of any data packet is produced by random number generator and the changes of the packet's content is implemented by simulating natural and manual attacks. The results show that these two error-detecting codes have the strong ability against natural attacks, but no ability against manual attacks which facilitate the destruction of data authentication and data-accessing availability.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4426974"},{"name":"Crisp--A Fault Localization Tool for Java Programs","snippet":"Crisp is an Eclipse plug-in tool for constructing intermediate versions of a Java program that is being edited. After a long editing session, a programmer will run regression tests to make sure she has not invalidated previously tested functionality. If a test fails unexpectedly, Crisp allows the programmer to select parts of the edit that affected the failing test and to add them to the original program, creating an intermediate version guaranteed to compile. Then the programmer can re-execute the test in order to locate the exact reasons for the failure by concentrating on those affecting changes that were applied. Using Crisp, a programmer can it- eratively select, apply, and undo individual (or sets of) affecting changes and, thus effectively find a small set of failure-inducing changes. Crisp is an extension to our change impact analysis tool, Chianti, [6].","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4222645"},{"name":"Testing for interconnect crosstalk defects using on-chip embedded processor cores","snippet":"Crosstalk effects degrade the integrity of signals traveling on long interconnects and must be addressed during production testing. External testing for crosstalk is expensive due to the need for high-speed testers. Built-in self-test, while eliminating the need for a high-speed tester, may lead to excessive test overhead as well as overly aggressive testing. To address this problem, we propose a new software-based self-test methodology for system-on-chip (SoC) devices based on embedded processors. It enables an on-chip embedded processor core to test for crosstalk in system-level interconnects by executing a self-test program in the normal operational mode of the SoC. We have demonstrated the feasibility of this method by applying it to test the interconnects of a processor-memory system. The defect coverage was evaluated using a system-level crosstalk defect simulation method.","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=935527"},{"name":"Energy-Efficient Fault-Tolerant Mechanism for Clustered Wireless Sensor Networks","snippet":"Clustering is an effective topology control and communication protocol in wireless sensor networks (sensornets). However, the harsh deployed environments, the serious resource limitation of nodes, and the unbalanced workload among nodes make the clustered sensornets vulnerable to communication faults and errors, which undermine the usability of the network. So mechanisms to improve the robustness and fault-tolerance are highly required in real applications of sensornets. In this paper, a distributed fault-tolerant mechanism called CMATO (Cluster-Member-based fAult-TOlerant mechanism) for sensornets is proposed. It views the cluster as an individual whole and utilizes the monitoring of each other within the cluster to detect and recover from the faults in a quick and energy-efficient way. CMATO only needs the local knowledge of the network, relaxing the pre-deployment of the cluster heads and a k-dominating set (k>1) coverage assumptions. This advantage makes our mechanism flexible to be incorporated into various existing clustering schemes in sensornets. Furthermore, CMATO is able to deal with failures of multiple cluster heads, so it effectively recovers the nodes from the failures of multiple cluster heads and the failures of links within the cluster, gaining a much more robust and fault-tolerant sensornets. The simulation results show that our mechanism outperforms the existing cluster-head based fault-tolerant mechanism in both fault coverage and energy consumption.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4317831"},{"name":"Research on a Novel Method for Measuring Volumetric Error of Coordinate Measuring Machine","snippet":"CMM (Coordinate Measuring Machine) is used as an accurate measuring instrument in manufacturing and design of products. High accurate CMM is required by the development of super-finish, micro-machinery, micro-electronic mechanical systems (MEMS). The precision of CMM is influenced by its volumetric error and dynamic error. The volumetric error is the major component during slowly probing, but the dynamic error is not omitted during fast probing. Compensating the volumetric error is an effective approach to improve the precision of CMM. The volumetric error in the working space of CMM must be measured and the error model must be set up before compensating. In this paper, a novel method to measure 5 volumetric errors of a guide way at the same time by 3-beam laser interferometer is proposed. The error compensating model is validated by experiment. The method can also be used to measure the dynamical error of CMM and CNC machine in real-time. The research can be referred to design the super higher accurate CMM and the accurate CNC machine.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5460155"},{"name":"Detouring: Translating software to circumvent hard faults in simple cores","snippet":"CMOS technology trends are leading to an increasing incidence of hard (permanent) faults in processors. These faults may be introduced at fabrication or occur in the field. Whereas high-performance processor cores have enough redundancy to tolerate many of these faults, the simple, low-power cores that are attractive for multicore chips do not. We propose Detouring, a software-based scheme for tolerating hard faults in simple cores. The key idea is to automatically modify software such that its functionality is unchanged but it does not use any of the faulty hardware. Our initial implementation of Detouring tolerates hard faults in several hardware components, including the instruction cache, registers, functional units, and the operand bypass network. Detouring has no hardware cost and no performance overhead for fault-free cores.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4630073"},{"name":"Detection or isolation of defects? An experimental comparison of unit testing and code inspection","snippet":"Code inspections and white-box testing have both been used for unit testing. One is a static analysis technique, the other, a dynamic one, since it is based on executing test cases. Naturally, the question arises whether one is superior to the other, or, whether either technique is better suited to detect or isolate certain types of defects. We investigated this question with an experiment with a focus on detection of the defects (failures) and isolation of the underlying sources of the defects (faults). The results indicate that there exist significant differences for some of the effects of using code inspection versus testing. White-box testing is more effective, i.e. detects significantly more defects while inspection isolates the underlying source of a larger share of the defects detected. Testers spend significantly more time, hence the difference in efficiency is smaller, and is not statistically significant. The two techniques are also shown to detect and identify different defects, hence motivating the use of a combination of methods.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1251026"},{"name":"Estimating the number of faults remaining in software code documents inspected with iterative code reviews","snippet":"Code review is considered an efficient method for detecting faults in a software code document. The number of faults not detected by the review should be small. Current methods for estimating this number assume reviews with several inspectors, but there are many cases where it is practical to employ only two inspectors. Sufficiently accurate estimates may be obtained by two inspectors employing an iterative code review (ICR) process. This paper introduces a new estimator for the number of undetected faults in an ICR process, so the process may be stopped when a satisfactory result is estimated. This technique employs the Kantorowitz estimator for N-fold inspections, where the N teams are replaced by N reviews. The estimator was tested for three years in an industrial project, where it produced satisfactory results. More experiments are needed in order to fully evaluate the approach.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1421075"},{"name":"Coefficient-based test of parametric faults in analog circuits","snippet":"Coefficient-based test (CBT) is introduced for detecting parametric faults in analog circuits. The method uses pseudo Monte Carlo simulation and system-identification tools to determine whether a given circuit under test (CUT) is faulty. From the circuit description, and component tolerance specifications, the tolerance boxes of all circuit transfer-function coefficients are precomputed and used during the test. Using input\/output signal information, the test procedure attempts to extract the CUT's transfer function. When this extraction is complete-if one or more of these measured transfer-function coefficients are found to be outside their tolerance boxes-the circuit is declared faulty.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1583875"},{"name":"Advanced fault-tolerance techniques for a color digital camera-on-a-chip","snippet":"Color digital imagers contain red, green and blue subpixels within each color pixel. Defects that develop either at fabrication time or due to environmentally induced errors over time can cause a single color subpixel (e.g., R) to fail, while leaving the remaining colors intact. This paper investigates seven software correction algorithms that interpolate the color of a pixel based on its nearest neighbors. Using several measurements of color error, all seven methods were investigated for a large number of digital images. Interpolations using only information from the single failed color (e.g., R) in the neighbors gave the poorest results. Those using all color measurements and a quadratic interpolation formula, combined with the remaining subpixel colors (e.g., G and B) produced significantly better results. A formula developed using the CIE color coordinates of tristimulus values (X, Y, Z) yielded the best results","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=966746"},{"name":"Research on fault diagnosis of HT-60 drilling rig based on neural network expert system","snippet":"Combining the characteristics of drilling rig fault, a solution of fault diagnosis expert system based on artificial neural network is proposeed. The fault diagnosis system is designed for HT-60 drilling rig, which acquires knowledge by neural network and diagnoses by expert system. The system with characteristics of self-learning and self-adaptive can acquire knowledge from existing data in order to achieve the purpose of expanding knowledge, which maks up the inadequacies of traditional expert system. Through analyzing a variety of common faults and solutions, the software interface is established by using the Force Control software to achieve fault diagnosis which is based on artificial neural network expert system.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5619156"},{"name":"Semantic Impact and Faults in Source Code Changes: An Empirical Study","snippet":"Changes to source code have become a critical factor in fault predictions. Text or syntactic approaches have been widely used. Textual analysis focuses on changed text fragments while syntactic analysis focuses on changed syntactic entities. Although both of them have demonstrated their advantages in experimental results, they only study code fragments modified during changes. Because of semantic dependencies within programs, we believe that code fragments impacted by changes are also helpful. Given a source code change, we identify its impact by program slicing along the variable def-use chains. To evaluate the effectiveness of change impacts in fault detection and prediction, we compare impacted code with changed code according to size and fault density. Our experiment on the change history of a successful industrial project shows that: fault density in changed and impacted fragments are higher than other areas; for large changes, their impacts have higher fault density than changes themselves; interferences within change impact contribute to the high fault density in large changes. Our study suggests that, like change itself, change impact is also a high priority indicator in fault prediction, especially for changes of large scales.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5076635"},{"name":"Automatic Detection of In-field Defect Growth in Image Sensors","snippet":"Characterization of in-field defect growth with time in digital image sensors is important for measuring the quality of sensors as they age. While more defects were found in cameras exposed to high cosmic ray radiation environments, comparing the collective growth rate of different sensor types has shown that CCD imagers develop twice as many defects as APS imagers, indicating that CCD imagers may be more sensitive to radiation. The defect growth of individual imagers can be estimated by analyzing historical image sets captured by individual cameras. This paper presents a defect tracing algorithm, which determines the presence or absence of defects by accumulating Bayesian statistics collected over a sequence of images. Recognizing the complexity of image scenes, camera settings, and local clustering of defects in color images (due to demosaicing), refinements of the algorithm have been explored and the resulting detection accuracy has increased significantly. In-field test results from 3 imagers with a total of 26 defects have shown that 96% of the defects' dates were identified with less than 10 days difference compared to visual inspection. In addition to our continuous study of in-field defects in high-end digital SLRs, this paper presents a preliminary study of 10 cellphone cameras. Our test results address the comparison of defects types, distribution and growth found in low-end and high-end cameras with significantly different pixel sizes.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4641186"},{"name":"Adaptive Correction of Errors from Recognized Chinese Ink Texts Based on Context","snippet":"Chinese ink texts can not be converted into encoded texts until their writing characters are correctly recognized. There are many errors in recognized Chinese ink texts even importing language models because Chinese ink texts are free forms and mixed with other languages, as well as their Chinese characters have a large set and complex structures. Recognized writing characters may contain wrong language types, symbols, words, and word pairs. A direct selection and input approach based on context is proposed to adaptively correct theses errors. Each writing characterpsilas recognition candidates are fully visualized. Users can naturally and easily correct recognition errors with direct operations. Userspsila intensions are identified from userspsila gestures and objects invoked by them. Recognized Chinese ink texts can provide multi-levels of information after correction. We have conducted experiments using real-life Chinese ink texts and compared the proposed approach with others. Experimental results demonstrate that the proposed approach is effective and robust.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5190077"},{"name":"An OpenMP Approach to Modeling Dynamic Earthquake Rupture Along Geometrically Complex Faults on CMP Systems","snippet":"Chip multiprocessors (CMP) are widely used for high performance computing and are being configured in a hierarchical manner to compose a CMP compute node in a parallel system. OpenMP parallel programming within such a CMP node can take advantage of the globally shared address space and on-chip high inter-core bandwidth and low inter-core latency. In this paper, we use OpenMP to parallelize a sequential earthquake simulation code for modeling spontaneous dynamic earthquake rupture along geometrically complex faults on two CMP systems, IBM POWER5+ system and SUN Opteron server. The experimental results indicate that the OpenMP implementation has the accurate output results and the good scalability on the two CMP systems. Further, we apply the optimization techniques such as large page and processor binding to the OpenMP implementation to achieve up to 7.05% performance improvement on the CMP systems without any code modification.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5365641"},{"name":"Calculating the fault coverage for dual neighboring faults using single stuck-at fault patterns","snippet":"Chip structures shrink rapidly, but the particles causing the defects do not shrink in the same degree, thus multiple faults are more and more frequent in today's deep sub-micron chips. Scan test patterns are usually calculated to detect single stuck-at faults, and they detect also 'nearly all' multiple faults if at least one of the faults is detectable as a single fault. 'Nearly all' implies that there are exceptions, and indeed sometimes two single-stuck-at faults can only be detected when occurring alone, but not if they occur together. This phenomenon is called fault masking and has been extensively discussed in the literature, but only in considering each pair of possible faults having the same likelihood to occur. In reality, however, pairs of neighboring faults have a much higher likelihood than pairs of distant faults. Using layout and pattern data of a commercial circuit, the extent of fault masking is calculated both for neighboring faults, and for distant faults.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4538792"},{"name":"Transient-fault recovery for chip multiprocessors","snippet":"Chip-level redundant threading with recovery (CRTR) for chip multiprocessors extends previous transient-fault detection schemes to provide fault recovery. To hide interprocessor latency, CRTR uses a long slack enabled by asymmetric commit and uses the trailing thread state for recovery. CRTR increases bandwidth supply by pipelining communication paths and reduces bandwidth demand by extending the dependence-based checking elision.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1261390"},{"name":"FTCloud: A Component Ranking Framework for Fault-Tolerant Cloud Applications","snippet":"Cloud computing is becoming a mainstream aspect of information technology. The cloud applications are usually large-scale, complex, and include a lot of distributed components. Providing highly reliable cloud applications is a challenging and critical research problem. To attack this challenge, we propose FTCloud which is a component ranking based framework for building fault-tolerant cloud applications. FTCloud employs the component invocation structures and the invocation frequencies to identify the significant components in a cloud application. An algorithm is proposed to automatically determine optimal fault tolerance strategy for these significant components. The experimental results show that by tolerating faults of a small part of the most significant components, the reliability of cloud application can be greatly improved.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5635078"},{"name":"Exploring machine learning techniques for fault localization","snippet":"Debugging is the most important task related to the testing activity. It has the goal of locating and removing a fault after a failure occurred during test. However, it is not a trivial task and generally consumes effort and time. Debugging techniques generally use testing information but usually they are very specific for certain domains, languages and development paradigms. Because of this, a neural network (NN) approach has been investigated with this goal. It is independent of the context and presented promising results for procedural code. However it was not validated in the context of object-oriented (OO) applications. In addition to this, the use of other machine learning techniques is also interesting, because they can be more efficient. With this in mind, the present work adapts the NN approach to the OO context and also explores the use of support vector machines (SVMs). Results from the use of both techniques are presented and analysed. They show that their use contributes for easing the fault localization task.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4813783"},{"name":"Accurate microarchitecture-level fault modeling for studying hardware faults","snippet":"Decreasing hardware reliability is expected to impede the exploitation of increasing integration projected by Moore's Law. There is much ongoing research on efficient fault tolerance mechanisms across all levels of the system stack, from the device level to the system level. High-level fault tolerance solutions, such as at the microarchitecture and system levels, are commonly evaluated using statistical fault injections with microarchitecture-level fault models. Since hardware faults actually manifest at a much lower level, it is unclear if such high level fault models are acceptably accurate. On the other hand, lower level models, such as at the gate level, may be more accurate, but their increased simulation times make it hard to track the system-level propagation of faults. Thus, an evaluation of high-level reliability solutions entails the classical tradeoff between speed and accuracy. This paper seeks to quantify and alleviate this tradeoff. We make the following contributions: (1) We introduce SWAT-Sim, a novel fault injection infrastructure that uses hierarchical simulation to study the system-level manifestations of permanent (and transient) gate-level faults. For our experiments, SWAT-Sim incurs a small average performance overhead of under 3x, for the components we simulate, when compared to pure microarchitectural simulations. (2) We study system-level manifestations of faults injected under different microarchitecture-level and gate-level fault models and identify the reasons for the inability of microarchitecture-level faults to model gate-level faults in general. (3) Based on our analysis, we derive two probabilistic microarchitecture-level fault models to mimic gate-level stuck-at and delay faults. Our results show that these models are, in general, inaccurate as they do not capture the complex manifestation of gate-level faults. The inaccuracies in existing models and the lack of more accurate microarchitecture-level models motivate using infrastruc- - tures similar to SWAT-Sim to faithfully model the microarchitecture-level effects of gate-level faults.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4798242"},{"name":"Automatic defect classification of TFT-LCD panels using machine learning","snippet":"Defect classification in the liquid crystal display (LCD) manufacturing process is one of the most crucial issues for quality control. To resolve this constraint, an automatic defect classification (ADC) method based on machine learning is proposed. Key features of LCD micro-defects are defined and extracted, and support vector machine is used for classification. The classification performance is presented through several experimental results.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5213760"},{"name":"Evaluating the accuracy of defect estimation models based on inspection data from two inspection cycles","snippet":"Defect content estimation techniques (DCETs), based on defect data from inspection, estimate the total number of defects in a document to evaluate the development process. For inspections that yield few data points DCETs reportedly underestimate the number of defects. If there is a second inspection cycle, the additional defect data is expected to increase estimation accuracy. In this paper we consider 3 scenarios to combine data sets from the inspection-reinspection process. We evaluate these approaches with data from an experiment in a university environment where 31 teams inspected and reinspected a software requirements document. Main findings of the experiment were that reinspection data improved estimation accuracy. With the best combination approach all examined estimators yielded on average estimates within 20% around the true value, all estimates stayed within 40% around the true value.","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=919089"},{"name":"A die-based defect-limited yield methodology for line control","snippet":"Defect monitoring and control in the semiconductor fab has been well documented over the years. The methodologies typically described in the literature involve controls through full-wafer defect counts, or defect densities, with attempts to correlate defects to electrical fail modes in order to predict the yield impact. These wafer-based methodologies are not adequate for determining the impact of defects on yield. Most notably, severe complications arise when applying wafer-based methods on wafers with mixed distributions (mix of random and clustered defects). This paper describes the proper statistical treatment of defect data to estimate yield impact for mixed-distribution wafer maps. This die-based, defect-limited yield (DLY) methodology properly addresses random and clustered defects, and applies a die-based multi-stage sampling method to select defects for review. The estimated yield impact of defects on the die can then be determined. Additionally, a die normalization technique is described that permits application of this die-based methodology on multiple products with different die sizes.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5551412"},{"name":"Tracking concept drift of software projects using defect prediction quality","snippet":"Defect prediction is an important task in the mining of software repositories, but the quality of predictions varies strongly within and across software projects. In this paper we investigate the reasons why the prediction quality is so fluctuating due to the altering nature of the bug (or defect) fixing process. Therefore, we adopt the notion of a concept drift, which denotes that the defect prediction model has become unsuitable as set of influencing features has changed - usually due to a change in the underlying bug generation process (i.e., the concept). We explore four open source projects (Eclipse, OpenOffice, Netbeans and Mozilla) and construct file-level and project-level features for each of them from their respective CVS and Bugzilla repositories. We then use this data to build defect prediction models and visualize the prediction quality along the time axis. These visualizations allow us to identify concept drifts and - as a consequence - phases of stability and instability expressed in the level of defect prediction quality. Further, we identify those project features, which are influencing the defect prediction quality using both a tree induction-algorithm and a linear regression model. Our experiments uncover that software systems are subject to considerable concept drifts in their evolution history. Specifically, we observe that the change in number of authors editing a file and the number of defects fixed by them contribute to a project's concept drift and therefore influence the defect prediction quality. Our findings suggest that project managers using defect prediction models for decision making should be aware of the actual phase of stability or instability due to a potential concept drift.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5069480"},{"name":"Localizing Software Faults Simultaneously","snippet":"Current automatic diagnosis techniques are predominantly of a statistical nature and, despite typical defect densities, do not explicitly consider multiple faults, as also demonstrated by the popularity of the single-fault Siemens set. We present a logic reasoning approach, called Zoltar-M(ultiple fault), that yields multiple-fault diagnoses, ranked in order of their probability. Although application of Zoltar-M to programs with many faults requires further research into heuristics to reduce computational complexity, theory as well as experiments on synthetic program models and two multiple-fault program versions from the Siemens set show that for multiple-fault programs this approach can outperform statistical techniques, notably spectrum-based fault localization (SFL). As a side-effect of this research, we present a new SFL variant, called Zoltar-S(ingle fault), that is provably optimal for single-fault programs, outperforming all other variants known to date.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5381406"},{"name":"QoS-aware connection resilience for network-aware grid computing fault tolerance","snippet":"Current grid computing fault tolerance leverages IP dynamic rerouting and schemes implemented in the application or in the middleware to overcome both software and hardware failures. Despite the flexibility of current grid computing fault tolerant schemes in recovering inter-service connectivity from an almost comprehensive set of failures, they might not be able to repristinate also connection QoS guarantees, such as minimum bandwidth and maximum delay. This phenomenon is exacerbated when, as in global grid computing, the grid computing sites are not connected by dedicated network resources but share the same network infrastructure with other Internet services. This paper aims at showing the advantages of integrating grid computing fault tolerance schemes with next generation networks (NGNs) resilient schemes. Indeed, by combining the utilization of generalized multi-protocol label switching (GMPLS) resilient schemes, such as path restoration, and application or middleware layer fault tolerant schemes, such as service migration or replication, it is possible to guarantee the necessary QoS to the connections between grid computing sites while limiting the required network and computational resources.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1505834"},{"name":"Service Restoration Methodology for Multiple Fault Case in Distribution Systems","snippet":"Current KEPCO's distribution automation system (DAS) provides a very effective restoration solution for the single fault case but cannot handle multiple faults. This paper proposes a two-step restoration scheme-sequential and simultaneous restoration-for multiple fault cases. Efficiency has been achieved by introduction of restoration performance index (RPI) and load-balancing algorithm. Test results to show effectiveness of the proposed scheme are presented, and field experience of DAS in Korea is described as well","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1717566"},{"name":"Hierarchical fault diagnosis and health monitoring in multi-platform space systems","snippet":"Current spacecraft health monitoring and fault diagnosis practices that involve around-the-clock limit-checking and trend analysis on large amount of telemetry data, do not scale well for future multi-platform space missions due to the presence of larger amount of telemetry data and an increasing need to make the long-duration missions cost-effective by limiting the size of the operations team. The need for efficient utilization of telemetry data by employing machine learning and rule-based reasoning has been pointed out in the literature in order to enhance diagnostic performance and assist the less-experienced personnel in performing monitoring and diagnosis tasks. In this research we develop a systematic and transparent fault diagnosis methodology within a hierarchical fault diagnosis framework for multiplatform space systems. Our proposed Bayesian network-based hierarchical fault diagnosis methodology allows fuzzy rule-based reasoning at different components in the hierarchy. Due to the unavailability of real formation flight data, we demonstrate the effectiveness of our proposed methodology by using synthetic data of a leader-follower formation flight. Our proposed methodology is likely to enhance the level of autonomy in ground support based spacecraft health monitoring and fault diagnosis.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4839690"},{"name":"A power transformer protection with recurrent ANN saturation correction","snippet":"Current transformers (CTs) are present in electric power systems for protection and measurement purposes and they are susceptible to the saturation phenomenon. This paper presents an alternative approach to the correction of distorted waveforms caused by CT saturation. The method uses recurrent artificial neural networks (ANN) algorithms. As an example of an application, a complete protection system for a power transformer based on the deferential logic has been utilized. The EMTP-ATP software has been chosen as the computational tool to simulate the electrical system in order to generate data to train and test the ANNs. Many ANN architectures were trained and tested. Encouraging results related to the application of the new method are presented.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1489125"},{"name":"Defect Prevention: A General Framework and Its Application","snippet":"Defect prevention in CMM and causal analysis and resolution in CMMI are focused on identifying the root cause of defects and preventing defects from recurring. Actions are expected at a project level as well as organization level. This paper provides a general framework of defect prevention activities which consists of organization structure, defect definition, defect prevention process and quality culture establishment. Implementation of defect prevention results in rapid and sustained improvement in software product quality which is evident from an example in Neusoft Group, where defect density in post release phase decreased from 0.85 defects\/KLOC in 2000 to 0.1 defects\/KLOC in 2005","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4032296"},{"name":"Defect tolerance for gracefully-degradable microfluidics-based biochips","snippet":"Defect tolerance is an important design consideration for microfluidics-based biochips that are used for safety-critical applications. We propose a defect tolerance methodology based on graceful degradation and dynamic reconfiguration. We first introduce tile-based biochip architecture, which is scalable for large-scale bioassays. A clustered defect model is used to evaluate the graceful degradation method for tile-based biochips. The proposed schemes ensure that the bioassays mapped to a droplet-based microfluidic array during design can be executed on a defective biochip through operation rescheduling and\/or resource rebinding. Real-life biochemical procedures, namely polymerase chain reaction (PCR) and multiplexed in-vitro diagnostics on human physiological fluids, are used to evaluate the proposed defect tolerance schemes.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1443444"},{"name":"A Model Based Framework for Specifying and Executing Fault Injection Experiments","snippet":"Dependability is a fundamental property of computer systems operating in critical environment. The measurement of dependability (and thus the assessment of the solutions applied to improve dependability) typically relies on controlled fault injection experiments that are able to reveal the behavior of the system in case of faults (to test error handling and fault tolerance) or extreme input conditions (to assess robustness of system components). In our paper we present an Eclipse-based fault injection framework that provides a model-based approach and a graphical user interface to specify both the fault injection experiments and the run-time monitoring of the results. It automatically implements the modifications that are required for fault injection and monitoring using the Javassist technology, this way it supports the dependability assessment and robustness testing of software components written in Java.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5261017"},{"name":"Fault Modeling and Functional Test Methods for Digital Microfluidic Biochips","snippet":"Dependability is an important attribute for microfluidic biochips that are used for safety-critical applications, such as point-of-care health assessment, air-quality monitoring, and food-safety testing. Therefore, these devices must be adequately tested after manufacture and during bioassay operations. Known techniques for biochip testing are all function oblivious (i.e., while they can detect and locate defect sites on a microfluidic array, they cannot be used to ensure correct operation of functional units). In this paper, we introduce the concept of functional testing of microfluidic biochips. We address fundamental biochip operations, such as droplet dispensing, droplet transportation, mixing, splitting, and capacitive sensing. Long electrode actuation times are avoided to ensure that there is no electrode degradation during testing. The functional testing of pin-constrained biochips is also studied. We evaluate the proposed test methods using simulations as well as experiments for a fabricated biochip.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5166457"},{"name":"Using motion-compensated frame-rate conversion for the correction of 3:2 pulldown artifacts in video sequences","snippet":"Currently, the most popular method of converting 24 frames per second (fps) film to 60 fields\/s video is to repeat each odd-numbered frame for 3 fields and each even-numbered frame for 2 fields. This method is known as 3:2 pulldown and is an easy and inexpensive way to perform 24 fps to 60 fields\/s frame-rate conversion. However, the 3:2 pulldown introduces artifacts, which are especially visible when viewing on progressive displays and during slow-motion playback. We have developed a motion-compensated frame-rate conversion algorithm to reduce the 3:2 pulldown artifacts. By using frame-rate conversion with interpolation instead of field repetition, mean square error and blocking artifacts are reduced significantly. The techniques developed here can also be applied to the general frame-rate conversion problem","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=867925"},{"name":"Empirical evaluation of the fault-detection effectiveness of smoke regression test cases for GUI-based software","snippet":"Daily builds and smoke regression tests have become popular quality assurance mechanisms to detect defects early during software development and maintenance. In previous work, we addressed a major weakness of current smoke regression testing techniques, i.e., their lack of ability to automatically (re)test graphical user interface (GUI) event interactions - we presented a GUI smoke regression testing process called daily automated regression tester (DART). We have deployed DART and have found several interesting characteristics of GUI smoke tests that we empirically demonstrate in this paper. We also combine smoke tests with different types of test oracles and present guidelines for practitioners to help them generate and execute the most effective combinations of test-case length and test oracle complexity. Our experimental subjects consist of four GUI-based applications. We generate 5000-8000 smoke tests (enough to be run in one night) for each application. Our results show that: (1) short GUI smoke tests with certain test oracles are effective at detecting a large number of faults; (2) there are classes of faults that our smoke test cannot detect; (3) short smoke tests execute a large percentage of code; and (4) the entire smoke testing process is feasible to do in terms of execution time and storage space.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1357785"},{"name":"Impurity and defect passivation in poly-Si films fabricated by aluminium-induced crystallisation","snippet":"Data from resistivity, optical transmission and reflectance, and open-circuit voltage (V\/sub oc\/) measurements show hydrogen or ammonia plasma treatment greatly reduces the effective doping concentration, the parasitic optical absorption and improves the minority carrier properties of poly-Si films fabricated by aluminium-induced crystallisation (AIC) on glass substrates. Two 450 nm thick AIC poly-Si films on glass, one hydrogenated and one non-hydrogenated, were used to fabricate poly-Si\/a-Si:H heterojunctions. The non-hydrogenated sample had a 1-sun V\/sub oc\/ of 136 mV and the hydrogenated sample had a 1-sun V\/sub oc\/ of 236 mV. The poor V\/sub oc\/ indicates that AIC poly-Si films are more suitable as seed layers than as absorber layers. However, heterojunctions are sensitive to surface conditions and thus further V\/sub oc\/ improvements may be possible by surface optimization of the hydrogenated AIC poly-Si film prior to the formation of the heterojunction.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1306140"},{"name":"Data Mining Applied to the Electric Power Industry: Classification of Short-Circuit Faults in Transmission Lines","snippet":"Data mining can play a fundamental role in modern power systems. However, the companies in this area still face several difficulties to benefit from data mining. A major problem is to extract useful information from the currently available non-labeled digitized time series. This work focuses on automatic classification of faults in transmission lines. These faults are responsible for the majority of the disturbances and cascading blackouts. To circumvent the current lack of labeled data, the alternative transients program (ATP) simulator was used to create a public comprehensive labeled dataset. Results with different preprocessing (e.g., wavelets) and learning algorithms (e.g., decision trees and neural networks) are presented, which indicate that neural networks outperform the other methods.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4389729"},{"name":"Novel modular fault tolerant switched reluctance machine for reliable factory automation systems","snippet":"Electrical machines and drives used in diverse critical fields like advanced factory automation systems, automotive and aerospace applications, military, energy and medical equipment, etc. require both special motor and converter topologies to achieve high level fault tolerance. In the paper a novel modular fault tolerant switched reluctance machine is proposed. Its stator is built up of simply to manufacture and to replace modules. The machine is able to have continuous operation despite winding faults of diverse severity. It is fed by a special power converter having separate half H-bridge leg for each coil. Thus a complex and high reliable electrical system is obtained. By advanced dynamic co-simulations (using a coupled Flux 2D and Simulink<sup><\/sup> program) the behaviour of the drive system under five winding fault conditions are studied. The obtained results prove the fault tolerant capacity of the proposed machine.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5520679"},{"name":"Electrical Test Structures for Investigating the Effects of Optical Proximity Correction","snippet":"Electrical test structures have been designed to enable the characterisation of corner serif forms of optical proximity correction. These structures measure the resistance of a conducting track with a right angled corner. Varying amounts of OPC have been applied to the outer and inner corners of the feature and the effect on the resistance of the track investigated. A prototype test mask has been fabricated which contains test structures suitable for on-mask electrical measurement. The same mask was used to print the structures using an i-line lithography tool for on-wafer characterisation. Results from the structures at wafer level have shown that OPC has an impact on the final printed features. In particular the level of corner rounding is dependent upon the dimensions of the OPC features employed and the measured resistance can be used to help quantify the level of aggressiveness of the inner corner serifs.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4814632"},{"name":"Image processing techniques for wafer defect cluster identification","snippet":"Electrical testing determines whether each die on a wafer functions as originally designed. But these tests don't detect all the defective dies in clustered defects on the wafer, such as scratches, stains, or localized failed patterns. Although manual checking prevents many defective dies from continuing on to assembly, it does not detect localized failure patterns-caused by the fabrication process-because they are invisible to the naked eye. To solve these problems, we propose an automatic, wafer-scale, defect cluster identifier. This software tool uses a median filter and a clustering approach to detect the defect clusters and to mark all defective dies. Our experimental results verify that the proposed algorithm effectively detects defect clusters, although it introduces an additional 1% yield loss of electrically good dies. More importantly, it makes automated wafer testing feasible for application in the wafer-probing stage.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=990441"},{"name":"Remote sensing of power system arcing faults","snippet":"Electromagnetic radiation in the form of atmospheric radiowaves (or sferics) originate from power system apparatus when transient fault currents are present. A system to monitor these events via the detection of the induced very high frequency (VHF) sferic radiation has been in operation since November 1998. This system is part of an ongoing research program to develop overhead line fault detection and location equipment. This paper details the implementation of the sferic monitoring system and the latest developments that aim to improve event detection and triggering efficiency. Example transient sferic radiations records taken from the extensive data archive are presented. Fourier time frequency domain analysis is employed to extract features from the sferic signal data. Finally the future application of such monitoring technologies to power distribution networks is discussed.","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=950265"},{"name":"WYSIWIB: A declarative approach to finding API protocols and bugs in Linux code","snippet":"Eliminating OS bugs is essential to ensuring the reliability of infrastructures ranging from embedded systems to servers. Several tools based on static analysis have been proposed for finding bugs in OS code. They have, however, emphasized scalability over usability, making it difficult to focus the tools on specific kinds of bugs and to relate the results to patterns in the source code. We propose a declarative approach to bug finding in Linux OS code using a control-flow based program search engine. Our approach is WYSIWIB (What You See Is Where It Bugs), since the programmer expresses specifications for bug finding using a syntax close to that of ordinary C code. The key advantage of our approach is that search specifications can be easily tailored, to eliminate false positives or catch more bugs. We present three case studies that have allowed us to find hundreds of potential bugs.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5270354"},{"name":"Fault detection in a tristate system environment","snippet":"Embedded computers commonly rely on multiple-board systems, called tristate system environments. These environments consist of an interconnect and drivers or receivers with tristate features and boundary scan capabilities. The authors present a comprehensive fault model that provides 100 percent fault coverage and minimizes test set size","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=958701"},{"name":"Fault tolerance techniques for wireless ad hoc sensor networks","snippet":"Embedded sensor network is a system of nodes, each equipped with a certain amount of sensing, actuating, computation, communication, and storage resources. One of the key prerequisites for effective and efficient embedded sensor systems is development of low cost, low overhead, high resilient fault-tolerance techniques. Cost sensitivity implies that traditional double and triple redundancies are not adequate solutions for embedded sensor systems due to their high cost and high energy-consumption. We address the problem of embedded sensor network-fault-tolerance by proposing heterogeneous back-up scheme, where one type of resource is substituted with another. First we propose a broad spectrum of heterogeneous fault-tolerance techniques for sensor networks including the ones where communication and sensing are mutually backing up each other. Then, we focus our attention on two specific approaches where we back-up one type of sensors with another type of sensor. In the first, we assume faults that manifest through complete malfunctioning and in the second, we assume sensors where fault manifest through high level of error.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1037343"},{"name":"Adaptive Wavelet Domain Audio Steganography with High Capacity and Low Error Rate","snippet":"Embedding a secret message into a cover media without attracting attention, which is known as steganography, is desirable in some security applications. One of the medias which can be used as a cover media is audio signal. In this paper we introduce an adaptive wavelet domain steganography with high capacity and low error rate. We use lifting scheme to create perfect reconstruction filter banks which are Int2Int and hide data in least significant bits (LSB) of details coefficient in an adaptive way to reduce the error rate. Our method have zero error rate for hiding capacity below 100 kilo bits-per-second (kbps) and 0.3% error for 200 kbps, in comparison to 0.9% error of normal wavelet domain LSB steganography. Signal to noise ratio (SNR) values and listening tests results show that the stegano audio is imperceptible from original audio even with hiding capacity up to 200 kbps.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4381305"},{"name":"Corrective Classification: Classifier Ensembling with Corrective and Diverse Base Learners","snippet":"Empirical studies on supervised learning have shown that ensembling methods lead to a model superior to the one built from a single learner under many circumstances especially when learning from imperfect, such as biased or noise infected, information sources. In this paper, we provide a novel corrective classification (C2) design, which incorporates error detection, data cleansing and Bootstrap sampling to construct base learners that constitute the classifier ensemble. The essential goal is to reduce noise impacts and eventually enhance the learners built from noise corrupted data. We further analyze the importance of both the accuracy and diversity of base learners in ensembling, in order to shed some light on the mechanism under which C2 works. Experimental comparisons will demonstrate that C2 is not only superior to the learner built from the original noisy sources, but also more reliable than bagging or the aggressive classifier ensemble (ACE), which are two degenerate components\/variants of C2.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4053179"},{"name":"An empirical study of fault localization for end-user programmers","snippet":"End users develop more software than any other group of programmers, using software authoring devices such as e-mail filtering editors, by-demonstration macro builders, and spreadsheet environments. Despite this, there has been little research on finding ways to help these programmers with the dependability of their software. We have been addressing this problem in several ways, one of which includes supporting end-user debugging activities through fault localization techniques. This paper presents the results of an empirical study conducted in an end-user programming environment to examine the impact of two separate factors in fault localization techniques that affect technique effectiveness. Our results shed new insights into fault localization techniques for end-user programmers and the factors that affect them, with significant implications for the evaluation of those techniques.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1553578"},{"name":"Strategies and behaviors of end-user programmers with interactive fault localization","snippet":"End-user programmers are writing an unprecedented number of programs, due in large part to the significant effort put forth to bring programming power to end users. Unfortunately, this effort has not been supplemented by a comparable effort to increase the correctness of these often faulty programs. To address this need, we have been working towards bringing fault localization techniques to end users. In order to understand how end users are affected by and interact with such techniques, we conducted a think-aloud study, examining the interactive, human-centric ties between end-user debugging and a fault localization technique. Our results provide insights into the contributions such techniques can make to an interactive end-user debugging process.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1260197"},{"name":"Interactive fault localization techniques in a spreadsheet environment","snippet":"End-user programmers develop more software than any other group of programmers, using software authoring devices such as multimedia simulation builders, e-mail filtering editors, by-demonstration macro builders, and spreadsheet environments. Despite this, there has been only a little research on finding ways to help these programmers with the dependability of the software they create. We have been working to address this problem in several ways, one of which includes supporting end-user debugging activities through interactive fault localization techniques. This paper investigates fault localization techniques in the spreadsheet domain, the most common type of end-user programming environment. We investigate a technique previously described in the research literature and two new techniques. We present the results of an empirical study to examine the impact of two individual factors on the effectiveness of fault localization techniques. Our results reveal several insights into the contributions such techniques can make to the end-user debugging process and highlight key issues of interest to researchers and practitioners who may design and evaluate future fault localization techniques.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1628969"},{"name":"Error correction in single-hop wireless sensor networks - A case study","snippet":"Energy efficient communication is a key issue in wireless sensor networks. Common belief is that a multi-hop configuration is the only viable energy efficient technique. In this paper we show that the use of forward error correction techniques in combination with ARQ is a promising alternative. Exploiting the asymmetry between lightweight sensor nodes and a more powerful base station even advanced techniques known from cellular networks can be efficiently applied to sensor networks. Our investigations are based on realistic power models and real measurements and, thus, consider all side-effects. This is to the best of our knowledge the first investigation of advanced forward error correction techniques in sensor networks which is based on real experiments.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5090865"},{"name":"Fault Coverage Measurement of a Timed Test Case Generation Approach","snippet":"Ensuring that a Real-Time Embedded System(RTES) is absent of major faults that may affect the way it performs is a non-trivial task. RTES behaviour is based on the interactions with its surrounding environment and on the timing characteristics of that same environment. As a result, time poses a new dimension to the complexity of the testing process. In previous research, we introduced a `priority-based' approach which tested the logical and timing behaviour of an RTES modeled formally as Uppaal automata. The `priority-based' approach was based on producing sets of timed test traces by achieving timing constraints coverage according to three sets of priorities, namely boundary, out-boundary and in-boundary. In this paper, we extend that work by validating the `priority-based' approach according to a well-known timed fault model. The validation process shows promising results, notably, that the `priority-based' approach is capable of detecting all the fault types included in the proposed fault model.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5457778"},{"name":"Cross-Cultural Differences of Error Climate between Chinese and German Entrepreneurial Firms","snippet":"Entrepreneurial firms' actions can be easily away from the predetermined goals or directions when in developing, which are errors. It would be critical for surviving how the firms cope with those errors. Our research defined the error climate as a general tendency to errors of the employees. The research used the free software, Mx, and studied four industries, including IT and software, catering and hotel, machinery and parts, and construction, to find the characteristics of the error climates of Chinese entrepreneurial firms and German entrepreneurial firms, and compared the differences between the error climates of them. The results suggest that Chinese entprepreueurial firms pay more attention to solve problems caused by errors and German entrepreneurial firms pay more attention to encourage employees communicating when an error occurs. The implication of the results for IT industry is also included.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5634643"},{"name":"Patching Processor Design Errors with Programmable Hardware","snippet":"Equipping processors with programmable hardware to patch design errors lets manufacturers release regular hardware patches, avoiding costly chip recalls and potentially speeding time to market. For each error detected, the manufacturer creates a fingerprint, which the customer uses to program the hardware. The hardware watches for error conditions; when they arise, it takes action to avoid the error. Overall, our scheme enables an exciting new environment where hardware design errors can be handled as easily as system software bugs, by applying a patch to the hardware","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4205120"},{"name":"Reversible Data Hiding-Based Approach for Intra-Frame Error Concealment in H.264\/AVC","snippet":"Error concealment plays an important role in robust video transmission. Recently, Chen and Leung presented an efficient data hiding-based (DH-based) approach to recover corrupted macroblocks from the intra-frame of an H.264\/AVC sequence, but it suffers from the quality degradation problem. Since the quantized discrete cosine transform coefficients of an H.264\/AVC sequence tend to form a Laplace distribution, we therefore propose a reversible DH-based approach for intra-frame error concealment based on this characteristic. Our design is able to achieve no quality degradation. Experimental results demonstrate that the quality of recovered video sequences obtained by our approach is indeed superior to that of the DH-based method. In addition, the quality advantage of our approach is illustrated when compared with the previous five related methods.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5580023"},{"name":"Spatial error concealment technique for losslessly compressed images using data hiding in error-prone channels","snippet":"Error concealment techniques are significant due to the growing interest in imagery transmission over error-prone channels. This paper presents a spatial error concealment technique for losslessly compressed images using least significant bit (LSB)-based data hiding to reconstruct a close approximation after the loss of image blocks during image transmission. Before transmission, block description information (BDI) is generated by applying quantization following discrete wavelet transform. This is then embedded into the LSB plane of the original image itself at the encoder. At the decoder, this BDI is used to conceal blocks that may have been dropped during the transmission. Although the original image is modified slightly by the message embedding process, no perceptible artifacts are introduced and the visual quality is sufficient for analysis and diagnosis. In comparisons with previous methods at various loss rates, the proposed technique is shown to be promising due to its good performance in the case of a loss of isolated and continuous blocks.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6391373"},{"name":"Enhanced temporal error concealment algorithm with edge-sensitive processing order","snippet":"Error concealment techniques are widely used in video decoder with error-prone communication channels. In this paper, an enhanced edge-sensitive processing order for temporal error concealment algorithm is proposed. Side information of neighboring macroblocks of the corrupted macroblocks are considered to derive a suitable processing order for error concealment, and a new motion vector searching algorithm is also proposed for temporal error concealment. Experimental results prove that the processing order plays an important role in error concealment. With considering the processing order, the proposed algorithm outperforms existing algorithms in terms of PSNR and perceptual artifacts, and the improvement of 2.45dB in PSNR can be achieved compared with the same system with raster-scan order.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4542205"},{"name":"Evaluation of Error Control Mechanisms Based on System Throughput and Video Playable Frame Rate on Wireless Channel","snippet":"Error control mechanisms are widely used in video communications over wireless channels. However for improving end-to-end video quality: they consume extra bandwidth and reduce effective system throughput. In this paper, considering the parameters of system throughput and playable frame rate as evaluating metrics, we investigate the efficiency of different error control mechanisms. We develop a throughput analytical model to present system effective throughput for different error control mechanisms under different conditions. For a given packet loss probability, both optimal retransmission times in adaptive ARQ and optimal number of redundant packets in adaptive FEC for each type of frames are derived by keeping the system throughput as a constant value. Also, end to end playable frame rates for the two schemes are computed. Then which error control scheme is the most suitable for which application condition is concluded. Finally empirical simulation experimental results with various data analysis are demonstrated.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5480940"},{"name":"Hardcopy image barcodes via block-error diffusion","snippet":"Error diffusion halftoning is a popular method of producing frequency modulated (FM) halftones for printing and display. FM halftoning fixes the dot size (e.g., to one pixel in conventional error diffusion) and varies the dot frequency according to the intensity of the original grayscale image. We generalize error diffusion to produce FM halftones with user-controlled dot size and shape by using block quantization and block filtering. As a key application, we show how block-error diffusion may be applied to embed information in hardcopy using dot shape modulation. We enable the encoding and subsequent decoding of information embedded in the hardcopy version of continuous-tone base images. The encoding-decoding process is modeled by robust data transmission through a noisy print-scan channel that is explicitly modeled. We refer to the encoded printed version as an image barcode due to its high information capacity that differentiates it from common hardcopy watermarks. The encoding\/halftoning strategy is based on a modified version of block-error diffusion. Encoder stability, image quality versus information capacity tradeoffs, and decoding issues with and without explicit knowledge of the base image are discussed.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1532299"},{"name":"A Feature-Based Flexible Customization Method of Error Modeling for Machine Tool","snippet":"Error model customization for different work type of complex CNC equipments is significant to identify and compensate the errors of CNC integrated manufacturing, and further to achieve process optimization and product quality control. Traditional error modeling method is complicated, takes long time and is difficult to change itself. A flexible customized error modeling method based on feature and multi-body system (MBS) theory is proposed. Firstly, feature space of machine tool and Denavit-Hartenberg (DH) homogeneous transformation matrix space are build separately. Secondly, feature mapping function is deduced. Thirdly, customization method of error model is described. Finally, flexible customization system for error modeling based on feature is developed. Using this system and method to execute error modeling for several typical machine tools of Shenyang Machine Tool Group. It takes about 1minute to get an accurate error model. It can analyze single error's percentage of total error. These are basis of error compensation. The practices indicate the method is effective and feasible. The method provides a new thought for error control, optimization of integrated manufacturing process, digital manufacturing and automation technology.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5700920"},{"name":"Error scope on a computational grid: theory and practice","snippet":"Error propagation is a central problem in grid computing. We re-learned this while adding a Java feature to the Condor computational grid. Our initial experience with the system was negative, due to the large number of new ways in which the system could fail. To reason about this problem, we developed a theory of error propagation. Central to our theory is the concept of an error's scope, defined as the portion of a system that it invalidates. With this theory in hand, we recognized that the expanded system did not properly consider the scope of errors it discovered. We modified the system according to our theory, and succeeded in making it a more robust platform for distributed computing.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1029919"},{"name":"Interactive Error Control for Mobile Video Telephony","snippet":"Error robust video communication for hand-held devices is a delicate task because of limited computational resources and hostile channel conditions in mobile environments. The loss of coded video data on the channel can result in spatio-temporal error propagation in the video. In addition, stringent end-to-end delays for conversational applications make this challenge even more difficult. In this work, we investigate several techniques which exploit feedback from the receiver to enhance the performance of conversational video in realistic mobile communication environments. Specifically, we show how a low-complexity interactive error tracking technique can be combined with multiple reference picture selection (RPS) based on the existing syntax of H.264\/AVC. This technique outperforms other interactive error protection strategies by a margin of more than 2 dB for moderate channel loss rates, with minimal impact on end-to-end delays.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4288971"},{"name":"ED<sup>4<\/sup>I: error detection by diverse data and duplicated instructions","snippet":"Errors in computing systems can cause abnormal behavior and degrade data integrity and system availability. Errors should be avoided especially in embedded systems for critical applications. However, as the trend in VLSI technologies has been toward smaller feature sizes, lower supply voltages and higher frequencies, there is a growing concern about temporary errors as well as permanent errors in embedded systems; thus, it is very essential to detect those errors. Software-implemented hardware fault tolerance (SIHFT) is a low-cost alternative to hardware fault-tolerance techniques for embedded processors: It does not require any hardware modification of commercial off-the-shelf (COTS) processors. ED<sup>4<\/sup>I (error detection by data diversity and duplicated instructions) is a SIHFT technique that detects both permanent and temporary errors by executing two \"different\" programs (with the same functionality) and comparing their outputs. ED<sup>4<\/sup>I maps each number, x, in the original program into a new number x', and then transforms the program so that it operates on the new numbers so that the results can be mapped backwards for comparison with the results of the original program. The mapping in the transformation of ED<sup>4<\/sup>I is x' = kx for integer numbers, where k<sub>f <\/sub> determines the fault detection probability and data integrity of the system. For floating-point numbers, we find a value of k<sub>f<\/sub> for the fraction and k<sub>e<\/sub> for the exponent separately, and use k = k<sub>f<\/sub>2<sup>k<\/sup> for the value of k. We have demonstrated how to choose an optimal value of k for the transformation. This paper shows that, for integer programs, the transformation with k = -2 was the most desirable choice in six out of seven benchmark programs we simulated. It maximizes the fault detection probability under the condition that the data integrity is highest","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=980007"},{"name":"Error Modeling in Network Tomography by Sparse Code Shrinkage (SCS) Method","snippet":"Errors in data measurements for network tomography may cause misleading estimations. This paper presents a novel technique to model these errors by using sparse code shrinkage (SCS) method. SCS is used in the field of image recognition for denoising the image data and we are the first to apply this technique for estimating error free link delays from erroneous link delay data. To make SCS adoptable in network tomography, we have made some changes in the SCS technique such as the use of Non Negative Matrix Factorization (NNMF) instead of independent component analysis (ICA) for the purpose of estimating sparsifying transformation. The estimated (denoised) link delays are compared with the original (error free) link delays based on the data obtained from a laboratory test bed. The simulation results verify the accuracy of the proposed technique.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5684133"},{"name":"Fault injection experiment results in space borne parallel application programs","snippet":"Development of the REE Commercial-Off-The-Shelf (COTS) based space-borne supercomputer requires a detailed knowledge of system behavior in the presence of Single Event Upset (SEU) induced faults. When combined with a hardware radiation fault model and mission environment data in a medium grained system model, experimentally obtained fault behavior data can be used to: predict system reliability, availability and performance; determine optimal fault detection methods and boundaries; and define high ROI fault tolerance strategies. The REE project has developed a fault injection suite of tools and a methodology for experimentally determining system behavior statistics in the presence of application level SEU induced transient faults. Initial characterization of science data application code for an autonomous Mars Rover geology application indicates that this code is relatively insensitive to SEUs and thus can be made highly immune to application level faults with relatively low overhead strategies.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1035379"},{"name":"Fault-tolerant systems design-estimating cache contents and usage","snippet":"Development of the Remote Exploration and Experimentation (REE) Commercial Off The Shelf (COTS) based space-borne supercomputer requires a detailed knowledge of system behavior in the presence of Single Even Upset (SEU) induced faults. When combined with a hardware radiation fault, model and mission environment data in a medium grained system model, experimentally obtained fault behavior data can be used to: predict system reliability, availability and performance; determine optimal fault detection methods and boundaries; and define high Return On Investment (ROI) fault tolerance strategies. The REE project has developed a fault injection suite of tools and a methodology for experimentally determining system behavior statistics in the presence of SEU induced transient faults in application level codes. Where faults cannot be directly injected, analytic means are used in conjunction with experimental data to determine probabilistic system fault response. In many processors, it is not possible to inject faults directly into onboard cache. In this case, a cache contents estimation tool can be used to define probabilistic fault susceptibility which is then combined with direct memory fault injection data to determined fault behavior statistics. In this paper we discuss the structure, function and usage of a PPC-750 cache contents estimator for the REE project.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1035380"},{"name":"Quantifying the effects of placement errors on WSN connectivity in grid-based deployments","snippet":"Device deployment plays a key role in the performance of any Wireless Sensor Network (WSN) application. WSN device deployment (i.e. the numbers and positions of the devices) must consider several design factors, like coverage, connectivity, lifetime, etc. However, connectivity remains the most fundamental factor especially in harsh environments. Extensive work has been applied on connectivity in WSN deployments. However, realistic physical deployment errors have been ignored in the majority of that work. In this paper, we explore an efficient grid-based deployment planning for connectivity when sensors placement is affected by random bounded errors around their corresponding grid vertices. We propose a new approach to evaluate the average connectivity percentage of the deployed sensor nodes. We apply this approach to practical 3D deployment scenario, namely, the cubic grid-based deployment with bounded uniform random errors. The average connectivity percentage is computed numerically and verified by extensive simulation results. Based on the results, quantified effects of placement errors on the connectivity percentage are outlined.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5473002"},{"name":"ReStore: symptom based soft error detection in microprocessors","snippet":"Device scaling and large scale integration have led to growing concerns about soft errors in microprocessors. To date, in all but the most demanding applications, implementing parity and ECC for caches and other large, regular SRAM structures have been sufficient to stem the growing soft error tide. This will not be the case for long, and questions remain as to the best way to detect and recover from soft errors in the remainder of the processor - in particular, the less structured execution core. In this work, we propose the ReStore architecture, which leverages existing performance enhancing checkpointing hardware to recover from soft error events in a low cost fashion. Error detection in the ReStore architecture is novel: symptoms that hint at the presence of soft errors trigger restoration of a previous checkpoint. Example symptoms include exceptions, control flow mis-speculations, and cache or translation look-aside buffer misses. Compared to conventional soft error detection via full replication, the ReStore framework incurs little overhead, but sacrifices some amount of error coverage. These attributes make it an ideal means to provide very cost effective error coverage for processor applications that can tolerate a nonzero, but small, soft error failure rate. Our evaluation of an example ReStore implementation exhibits a 2x increase in MTBE (mean time between failures) over a standard pipeline with minimal hardware and performance overheads. The MTBF increases by 7x if ReStore is coupled with parity protection for certain pipeline structures.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1467777"},{"name":"ReStore: Symptom-Based Soft Error Detection in Microprocessors","snippet":"Device scaling and large-scale integration have led to growing concerns about soft errors in microprocessors. To date, in all but the most demanding applications, implementing parity and ECC for caches and other large, regular SRAM structures have been sufficient to stem the growing soft error tide. This will not be the case for long and questions remain as to the best way to detect and recover from soft errors in the remainder of the processor - in particular, the less structured execution core. In this work, we propose the ReStore architecture, which leverages existing performance enhancing checkpointing hardware to recover from soft error events in a low cost fashion. Error detection in the ReStore architecture is novel: symptoms that hint at the presence of soft errors trigger restoration of a previous checkpoint. Example symptoms include exceptions, control flow misspeculations, and cache or translation look-aside buffer misses. Compared to conventional soft error detection via full replication, the ReStore framework incurs little overhead, but sacrifices some amount of error coverage. These attributes make it an ideal means to provide very cost effective error coverage for processor applications that can tolerate a nonzero, but small, soft error failure rate. Our evaluation of an example ReStore implementation exhibits a 2times increase in MTBF (mean time between failures) over a standard pipeline with minimal hardware and performance overheads. The MTBF increases by 20times if ReStore is coupled with protection for certain particularly vulnerable pipeline structures","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1673379"},{"name":"Middleware for decentralised fault tolerant service execution using replication in pervasive systems","snippet":"Devices in pervasive systems are resource constrained, heterogeneous, mobile and personal. The devices may seek or provide such services as data compression, encryption, image analysis, query processing and other types within a local, but dynamic network. In order to enable sharing of services and resources, it is necessary that the middleware facilitate collaboration amongst devices. Resource and service sharing in such environments is a challenge due to device mobilities, heterogeneity and link failures. In this paper, we propose a middleware that enables decentralised decision making for fault tolerant service execution in pervasive systems. Opportunistic contacts between pairs of devices are exploited to locate service-device mappings and initiate replicas of the required service. Redundancy is an unnecessary, but unavoidable consequence of any replication based fault tolerance scheme. One of the objectives of the proposed middleware is to minimise the number of replications. Simulation results show that the proposed scheme has lower time overhead as compared to existing schemes.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5470622"},{"name":"2D Photonic Defect Layers in 3D Inverted Opals on Si Platforms","snippet":"Dielectric spheres synthesised for the fabrication of self-organized photonic crystals such as opals offer large opportunities for the design of novel nanophotonic devices. In this paper, we show a hexagonal superlattice monolayer of dielectric spheres inscribed on a 3D colloidal photonic crystal by e-beam lithography. The crystal is produced by a variation of the vertical drawing deposition method assisted by an acoustic field. The structures were chosen after simulations showed that a hexagonal super-lattice monolayer in air exhibits an even photonic band gap below the light cone if the refractive index of the spheres is higher than 1.93","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4013720"},{"name":"Atmospheric correction of AMSR-E brightness temperatures for dry snow cover mapping","snippet":"Differences between the brightness temperatures (spectral gradient) collected by the Advanced Microwave Scanning Radiometer for EOS (AMSR-E) at 18.7 and 36.5 GHz are used to map the snow-covered area (SCA) over a region including the western U.S. The brightness temperatures are corrected to take into account for atmospheric effects by means of a simplified radiative transfer equation whose parameters are stratified using rawinsonde data collected from a few stations. The surface emissivity is estimated from the model, and the brightness temperatures at the surface are computed as the product of the surface temperature and the computed emissivity. The SCA derived from microwave data is compared with that obtained from the Moderate Resolution Imaging Spectroradiometer for both cases of corrected and noncorrected brightness temperatures. The improvement to the SCA retrievals based on the corrected brightness temperatures shows an average value around 7%","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1657997"},{"name":"Tropospheric heterogeneities corrections in differential radar interferometry","snippet":"Differential radar interferometry (DInSAR) has been used more and more widely to monitor crustal deformations due to underground mining and oil extraction, earthquakes, volcanoes, landslides, and so on. However, tropospheric heterogeneities have been identified as one of the major errors in DInSAR, which can be up to 40 cm as derived from dual-frequency GPS measurements in the example given in this paper. Therefore, it is crucial to correct the tropospheric heterogeneities in the DInSAR results for monitoring crustal deformation. These corrections from several GPS stations in the radar imaging area can be interpolated and applied to the DInSAR results. The discussions are based on data from the Tower Colliery test site southwest Sydney, Australia.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1026241"},{"name":"Adaptive Correction of Errors from Segmented Digital Ink Texts in Chinese Based on Context","snippet":"Digital ink texts in Chinese can neither be converted into users' desired layouts nor be recognized until their characters, lines, and paragraphs are correctly extracted. There are many errors in automatically segmented digital ink texts in Chinese because they are free forms and mixed with other languages, as well as their Chinese characters have small gaps and complex structures. Paragraphs, lines, and characters (recognizable language symbols) in digital ink may be wrongly extracted. An adaptive approach based on context is proposed to correct wrongly extracted these objects. Each extracted object is first adaptively visualized by color and shape labels according to relations between it and its neighbors. Users use simple gestures naturally and easily to merge and split wrongly extracted objects. Contexts are constructed from users' gestures and objects invoked by them, where users' intensions are identified. We have conducted experiments using real-life segmented digital ink texts in Chinese and compared the proposed approach with others. Experimental results demonstrate that the proposed approach is feasible, flexible, effective, and robust.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5557338"},{"name":"Software solution for fault record analysis in power transmission and distribution","snippet":"Digital protection relays provide the functionality of recording network disturbances during faults. Meanwhile the digital share in the installed relay number is a substantial one, so the utilities can gather valuable information with a large coverage of their grid and can start to enjoy the additional benefits of modem technology beyond the functions built into the devices. After a fault the operating personnel wants to obtain a most precise fault location to narrow the search for possible damage on the line. The fault locator precision of a single relay is limited by physics and by the grid conditions of mixed lines, load taps etc. But an easy-to-use software system for relay fault records can provide the desired precision to the utility personnel. The system is open to fault records of any relay, which is accomplished via the Comtrade data format. It also contains the parameters of segmented or untransposed lines. Furthermore it uses sophisticated self-adapting algorithms for analysis beyond those used at the protection relays.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1364839"},{"name":"Impact of configuration errors on DNS robustness","snippet":"During the past twenty years the Domain Name System (DNS) has sustained phenomenal growth while maintaining satisfactory user-level performance. However, the original design focused mainly on system robustness against physical failures, and neglected the impact of operational errors such as mis-configurations. Our measurement efforts have revealed a number of mis-configurations in DNS today: delegation inconsistency, lame delegation, diminished server redundancy, and cyclic zone dependency. Zones with configuration errors suffer from reduced availability and increased query delays up to an order of magnitude. The original DNS design assumed that redundant DNS servers fail independently, but our measurements show that operational choices create dependencies between servers. We found that, left unchecked, DNS configuration errors are widespread. Specifically, lame delegation affects 15% of the measured DNS zones, delegation inconsistency appears in 21% of the zones, diminished server redundancy is even more prevalent, and cyclic dependency appears in 2% of the zones. We also noted that the degrees of mis-configuration vary from zone to zone, with the most popular zones having the lowest percentage of errors. Our results indicate that DNS, as well as any other truly robust large-scale system, must include systematic checking mechanisms to cope with operational errors.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4808472"},{"name":"Offset based leaky prediction for error resilient ROI coding","snippet":"During the period of transmission, video data usually suffer from transmission errors inevitably. Intra update is a common approach to stop error propagation. However, damaged images cannot recover until next update in case of errors, which often leads to annoying effect. In this paper, we propose an enhanced leaky prediction approach that enables the region-of-interest (ROI) of images to recover gently from the immediate succeeding frame of erroneous ones in favor of better human perception. Moreover, an optimized offset compensation technique is designed to improve coding performance. Experimental results show that the proposed scheme can achieve better image quality for ROI and the fluctuation of bit rate is greatly reduced, compared to the intra update method.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5202457"},{"name":"An improved DMVE temporal error concealment","snippet":"During video transmission over error-prone networks, the compressed bit stream is often corrupted by channel errors which may cause the video quality degrading suddenly. In this paper, we present a novel temporal error concealment technique as a post-processing tool at the decoder side for recovering the lost information. In order to recover the lost motion vector, an improved decoder motion vector estimation (DMVE) criterion is introduced which considers temporal correlation and motion trajectory together. We utilize pixels in the two previous frames as well as surrounding pixels of the lost block. The best motion vector is determined according to the criterion, and then the lost pixels are recovered using motion compensation. Simulations show that the proposed technique can achieve remarkable objective (PSNR) and subjective gains in the quality of the recovered video.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4517877"},{"name":"A Dynamic Binary Translation Framework Based on Page Fault Mechanism in Linux Kernel","snippet":"Dynamic binary translation and optimization is one of the most important essential techniques for computing system virtualization. This paper proposes a new dynamic translation framework for co-designed virtual machines. It generates and handles translation requests based on page fault mechanism provided in Linux kernel. In this new framework, the translation of guest codes and the execution of translated codes can be performed on different processors in parallel. The framework support the coprocessor translating guest code pages and the host CPU executing translated pages simultaneously, thus the translator becomes more efficient. The paper also presents a qualitative analysis of the time cost in our framework on an x86-ARM co-designed dynamic binary translation system, and suggests that the performance of this framework can be further improved if shared memory between host CPU and coprocessor is used. The framework can also be used in a dynamic binary translator on multi-core platforms.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5578325"},{"name":"Sensitivity analysis of modular dynamic fault trees","snippet":"Dynamic fault tree analysis, as currently supported by the Galileo software package, provides an effective means for assessing the reliability of embedded computer-based systems. Dynamic fault trees extend traditional fault trees by defining special gates to capture sequential and functional dependency characteristics. A modular approach to the solution of dynamic fault trees effectively applies Binary Decision Diagram (BOD) and Markov model solution techniques to different parts of the dynamic fault tree model. Reliability analysis of a computer-based system tells only part of the story, however. Follow-up questions such as Where are the weak links in the system?, How do the results change if my input parameters change? and What is the most cost effective way to improve reliability? require a sensitivity analysis of the reliability analysis. Sensitivity analysis (often called Importance Analysis) is not a new concept, but the calculation of sensitivity measures within the modular solution methodology for dynamic and static fault trees raises some interesting issues. In this paper we address several of these issues, and present a modular technique for evaluating sensitivity, a single traversal solution to sensitivity analysis for BOD, a simplified methodology for estimating sensitivity for Markov models, and a discussion of the use of sensitivity measures in system design. The sensitivity measures for both the Binary Decision Diagram and Markov approach presented in this paper is implemented in Galileo, a software package for reliability analysis of complex computer-based systems","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=839462"},{"name":"Fault-tolerant and power-aware scheduling algorithm in hard-real-time distributed systems","snippet":"Dynamic voltage scaling (DVS) technique is being increasingly used in hard-real-time energy-limited embedded systems as a means to conserve energy and prolong their lifetimes. In this paper, we first analyze the interplay between fault-tolerance and energy-saving as well as their quantitative needs on processor slack resource. Then, we extend the traditional fault-tolerant completion time test (FTCTT) to power-aware fault-tolerant completion time test (PAFTCTT). Based on PAFTCTT, a voltage slowdown factor calculation is proposed. These slowdown factors not only guarantee that all hard tasks can be scheduled within their deadlines despite of any single permanent fault, but also effectively reduce energy consumption. Finally, the simulation experiments reveal that slowdown factor technique can achieve the percents of energy-saving up to 31.3% (with an average of 16.3%).","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5565199"},{"name":"Static Var compensators (SVC) required to solve the problem of delayed voltage recovery following faults in the power system of the Saudi electricity company, western region (SEC-WR)","snippet":"Each power system is unique in its load pattern, growth trends and type, generation resources and network configuration. One of the main objectives of the power system operation planners is the operation and control of the power system to satisfy the most secure, and reliable power supply. The power system of the Saudi electricity company in the western region (SEC-WR) faced a high load growth during the past few years. This load increase gave rise to a very high loading of the transmission system elements mainly power transformers and cables. The western region load is mainly composed of air conditioner (AC) during high load season. In case of faults this nature of load induces delayed voltage recovery following fault clearing on the transmission system. The sustained low voltage following transmission line faults could cause customer interruptions and may be equipment damage. The integrity of the transmission system may also be affected. The transient stability of the system may be affected. This may also influence the stability of the generating units in the system. The existing dynamic model of SEC-WR System has been described. The response of the model to the actual faults is compared with actual records obtained from the dynamic system monitor (DSM) installed in several locations in the SEC-WR System. To avoid as much as possible brown and black outs following system faults, SVC systems will be installed. An automatic under voltage load shedding scheme has been set up and optimized as an additional security and backup measures to cater for sever disturbance such as three phase and single phase faults.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1304771"},{"name":"Faults Detection and Isolation Based On Neural Networks Applied to a Levels Control System","snippet":"Each time more grows the necessity of guaranteeing itself security and trustworthiness of the equipment during the execution of the industrials processes. Then, it is very important that faults in the processes can be detected and isolated. This paper presents an approach to process fault detection and isolation (FDI) system applied to a levels control system connected with an industrial network Foundation Fieldbus. The FDI system was developed using artificial neural networksfi (ANN) and tested in real environment.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4371241"},{"name":"On combining fault classification and error propagation analysis in RT-Level dependability evaluation","snippet":"Early analysis of the functional impact of faults aims either at classifying the faults according to their main potential effect, or at analyzing more in depth the error propagation paths in the circuit. This paper presents the results of extensive SEU-like fault injections performed on a VHDL model of the 8051 micro-controller. The advantage of combining the two types of analyses and the impact of the workload are discussed.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1319692"},{"name":"Agent Model for Human Expert Trend Analysis Technique for Real Time Fault Simulation in Integrated Fault Diagnostic System","snippet":"Early fault detection is critical for safe and optimum plant operation and maintenance in any chemical plant. Quick corrective action can help in minimizing quality and productivity offsets and can assist in averting hazardous consequences in abnormal situations. In this paper, fault diagnosis based on trends analysis is considered where integrated equipment behaviors and operation trajectory are analyzed using a trend-matching approach. A qualitative representation of these trends using IF- THEN rules based on neuro-fuzzy approach is used to find root causes and possible and consequences for any detected abnormal situation. Experimental plant is constructed to provide real time fault simulation data for fault detection method verification.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4427749"},{"name":"Defect detection of bearing surfaces based on machine vision technique","snippet":"Due to the high demands for productivity and quality of bearing and the shortage of traditional detection methods, this paper proposes an automatic detection system based on machine vision technique. The detection system uses digital image processing technology to process the images collected by CCD camera and finish identification for the surfaces of bearing quickly and accurately. Firstly, least squares fitting and annulus scan are used to locate the bearing and the regions which will be detected. Secondly, contrast enhancement and low-pass filtering are used to improve the quality of images. Next, object inspection is applied to determine whether defects exist. Finally, the shape feature is used to finish recognition of defects. Experiments show that the detection system has the features of high efficiency, high accuracy and ease of use. This research has a certain practical value.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5620311"},{"name":"Study on method of single-phase-to-earth fault section location in neutral point resonant grounded system","snippet":"Due to the large amount of branches and high grounding resistance of the distribution network, the earth fault location has not been solved effectively. A live location method is proposed for the single-phase-to-earth fault section in the neutral point resonant grounded system. By this method, the compensation current of arc-suppression coil is adjusted after the metallic earth fault occurs. And the fault section can be confirmed by the variation of zero-sequence current measured by FTU equipped in the lines, for the zero-sequence current in the non-fault path is proportional to the zero-sequence voltage. When the single-phase-to-earth fault via resistance occurs, zero-sequence voltage changes after the arc-suppression coil is adjusted. The method is also effective as long as the zero-sequence current is converted by zero-sequence voltage. The site experiments have been carried out to prove that the method is feasible.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5617537"},{"name":"Modeling method for information model of fault tree diagnosis based on UML","snippet":"Due to the problem of complex relation between different data types existing in information model of fault tree diagnosis, the modeling method for information model of fault tree diagnosis based on UML is proposed. Firstly, the requirement of integrated diagnosis and AI-ESTATE to information in test and diagnostic environment and the shortage of information model of fault tree diagnosis is analyzed in the paper. Secondly, key structure, elements, data type and their restricting relation in the diagnostic information model of fault tree based on XML Schema is described. Afterwards, the mapping relation and the binding rule between XML Schema and UML class is researched. Finally, the UML class diagram about information model of fault tree diagnosis is created.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5565179"},{"name":"Practical Diagnostic Approach of Energy Consumption and Systematic Fault of AC System","snippet":"Due to the vast amount of energy consumption of AC (Air Conditioning) systems, the reasonable design, optimal operation and efficient management of AC systems are inevitable and necessary to the whole city energy management (CEM). To achieve these objectives, the verification of systematic characteristics, practical approaches of energy consumption and systematic fault diagnosis of AC systems should be taken as the premises and the most elementary works. Based on AC commissioning principle, a practical diagnosis approach of energy consumption and systematic fault of AC system is established. The framework and general procedures of the practical approach is presented and introduced in this paper.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5577264"},{"name":"On the Risk of Fault Coupling over the Chip Substrate","snippet":"Duplication and comparison has proven to be an efficient method for error detection. Based on this generic principle dual core processor architectures with output comparison are being proposed for safety critical applications. Placing two instances of the same (arbitrary) processor on one die yields a very cost efficient \"single chip\" implementation of this principle. At the same time, however, the physical coupling of the two replica creates the potential for certain types of faults to affect both cores in the same way, such that the mutual checking will fail. The key question here is how this type of coverage leakage relates to other imperfections of the duplication and comparison approach that would also be found using two cores on separate dies (such as coupling over a common power supply or clock). In this paper we analyze several of the relevant physical coupling mechanisms and elaborate a model to decompose the genesis of a common cause fault into several steps. We present an experimental study showing that a very tight local and temporal coincidence of the fault effect in both replica is a crucial prerequisite for a common cause fault. Based on this quantitative input we can conclude from our decomposition model that the risk of common cause faults is low for physical coupling mechanisms with relatively slow propagation speed, such as thermal and mechanical effects. The role of asymmetry for mitigating common cause faults is discussed in the light of these findings.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5350057"},{"name":"A new method for fault detection during power swing in distance protection","snippet":"During a power swing, currents and voltages behave such as a fault. Therefore, power swing blocking function in distance relays is necessary to discriminate between a power swing and a fault. Otherwise power swings can be considered as a fault and causes relay trip. The main problem happens when during power sings a fault occurs. In this case, distance relays should be unblocked. In this paper, a new method based on the DC component of fault currents will be proposed to detect a fault during power swing blocking. The proposed method can detect single-phase to ground, two-phase to ground and three-phase fault. Applying the new method on a sample network reveals the features of the method.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5136999"},{"name":"Deformation correction in ultrasound images using contact force measurements","snippet":"During an ultrasound scan, contact between the probe and the skin deforms the underlying tissue. This can be considered a feature (as in elastography), but is in general undesirable, particularly for 3D scanning. In this paper we propose a novel system to correct this deformation by measuring the contact force at the time of the ultrasound scan and then using an elastic model to predict the tissue deformation. The inverse of this deformation is then applied to the image, generating the image that would have been seen had there been no contact with the probe. A prototype system has been implemented using an a priori finite element model to predict the deformation. This has been tested on gelatine phantoms and shown to remove the contact deformation and so give improved 3D reconstructions","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=991700"},{"name":"Test Case Mutation in Hybrid State Space for Reduction of No-Fault-Found Test Results in the Industrial Automation Domain","snippet":"During development of a device a large set of testcases is executed to ensure the qualitative requirements. Nevertheless because of timing issues it is not possible to perform all possible test cases and therefore it is not possible to guarantee a product that works always as expected by the end user.Finding the root cause of failures in returned devices is still largely manual work by an expert because the exact system and environment state is not known.In this paper we present an approach which allows automatic mutation of test cases for hybrid systems to reproduce failures based on vague user descriptions.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5254081"},{"name":"Correction strategy for view maintenance anomaly after schema and data updating concurrently","snippet":"During maintaining the materialized view in the data warehouse, how to efficiently handle the concurrent updates is an important and intractable problem. The paper discusses typical situations that schema changes mix with data updates concurrently. And the reasons why concurrent updates result in view maintenance anomaly are analyzed. Based on the analysis, an enhanced commit agent is designed for dealing with non-order commit problem. Thus, the consistency between data warehouse and data source is guaranteed.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1504240"},{"name":"Some myths and common errors in simulation experiments","snippet":"During the more than fifty years that Monte Carlo simulation experiments have been performed on digital computers, a wide variety of myths and common errors have evolved. We discuss some of them, with a focus on probabilistic and statistical issues","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=977244"},{"name":"A fault diagnosis system for heat pumps","snippet":"During the operation of heat pumps, faults like heat exchanger fouling, component failure, or refrigerant leakage reduce the system performance. In order to recognize these faults early, a fault diagnosis system has been developed and verified on a test bench. The parameters of a heat pump model are identified sequentially and classified during operation. For this classification, several `hard' and `soft' clustering methods have been investigated, while fuzzy inference systems or neural networks are created automatically by newly developed software. Choosing a simple black-box model structure, the number of sensors can be minimized, whereas a more advanced grey-box model yields better classification results","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=973840"},{"name":"Synergistic coordination between software and hardware fault tolerance techniques","snippet":"Describes an approach for enabling the synergistic coordination between two fault-tolerance protocols to simultaneously tolerate software and hardware faults in a distributed computing environment. Specifically, our approach is based on a message-driven confidence-driven (MDCD) protocol that we have devised for tolerating software design faults, and a time-based (TB) checkpointing protocol that was developed by N. Neves and W.K. Fuchs (1996) for tolerating hardware faults. By carrying out algorithm modifications that are conducive to synergistic coordination between volatile-storage and stable-storage checkpoint establishments, we are able to circumvent the potential interference between the MDCD and TB protocols, and to allow them to effectively complement each other to extend a system's fault tolerance capability. Moreover, the protocol coordination approach preserves and enhances the features and advantages of the individual protocols that participate in the coordination, keeping the performance cost low.","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=941421"},{"name":"The reliability of diverse systems: a contribution using modelling of the fault creation process","snippet":"Design diversity is a defence against design faults causing common-mode failure in redundant systems, but we badly lack knowledge about how much reliability it will buy in practice, and thus about its cost-effectiveness, the situations in which it is an appropriate solution and how it should be taken into account by assessors and safety regulators. Both current practice and the scientific debate about design diversity depend largely on intuition. More formal probabilistic reasoning would facilitate critical discussion and empirical validation of any predictions: to this aim, we propose a model of the generation of faults and failures in two separately-developed program versions. We show results on: (i) what degree of reliability improvement an assessor can reliably expect from diversity; and (ii) how this reliability improvement may change with higher-quality development processes. We discuss the practical relevance of these results and the degree to which they can be trusted.","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=941385"},{"name":"Emulation-based design errors identification","snippet":"Design verification has a large impact on the final testability of a system. The identification and removal of design errors from the initial design steps increases the testing quality of the entire design flow. We propose in this paper to exploit the potentialities of an emulator to accelerate a validation methodology for RTL designs. Alternative emulator configurations are compared in order to evaluate the performance speed-up of the presented methodology. The RTL design functionalities are compared with a System C executable specification model.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1173533"},{"name":"Improving the Performance of Fault-Aware Scheduling Policies for Desktop Grids (Be Lazy, Be Cool)","snippet":"Desktop Grids have proved to be a suitable platform for the execution of Bag-of-Tasks applications but, being characterized by a high resource volatility, require the availability of scheduling techniques able to effectively deal with resource failures and\/or unplanned periods of unavailability. Fault-aware scheduling, proposed in [2], can be considered a promising approach, yielding to both performance improvements for Bag-of-Task-Applications and increased utilization for Desktop Grids. The best fault-aware scheduling strategy available at the moment uses on-line scheduling, that is it starts a task as soon as a machine becomes available. In this paper we present a machine selection policy based on the idea that sometimes is better to wait for another machine rather than greedily exploit an immediately available one. An extensive simulation study, carried on for a variety of realistic Desktop Grid configurations and Bag-of-Task workloads, has revealed that the new scheduling strategy further improve application performance and machine utilization with respect to the best fault- aware scheduling strategy among those proposed in [2].","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4407160"},{"name":"Evaluating the impact of Undetected Disk Errors in RAID systems","snippet":"Despite the reliability of modern disks, recent studies have made it clear that a new class of faults, UndetectedDisk Errors (UDEs) also known as silent data corruption events, become a real challenge as storage capacity scales. While RAID systems have proven effective in protecting data from traditional disk failures, silent data corruption events remain a significant problem unaddressed by RAID. We present a fault model for UDEs, and a hybrid framework for simulating UDEs in large-scale systems. The framework combines a multi-resolution discrete event simulator with numerical solvers. Our implementation enables us to model arbitrary storage systems and workloads and estimate the rate of undetected data corruptions. We present results for several systems and workloads, from gigascale to petascale. These results indicate that corruption from UDEs is a significant problem in the absence of protection schemes and that such schemes dramatically decrease the rate of undetected data corruption.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5270353"},{"name":"A first design for CANsistant: A mechanism to prevent inconsistent omissions in CAN in the presence of multiple errors","snippet":"Despite the significant advantages of the controller area network (CAN) there is an extended belief that CAN is not suitable for critical applications, mainly because of several dependability limitations. One of them is its limited data consistency. Several solutions to this problem have been previously proposed but they are not able to efficiently ensure consistent broadcasts in the presence of multiple channel errors. This paper introduces a circuit called CANsistant, that detects all scenarios potentially leading to the inconsistent omission of a frame in the presence of up to 4 channel errors and, if necessary, retransmits the affected frame.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5347236"},{"name":"Wavelet Coherence and Fuzzy Subtractive Clustering for Defect Classification in Aeronautic CFRP","snippet":"Despite their high specific stiffness and strength, carbon fiber reinforced polymers, stacked at different fiber orientations, are susceptible to interlaminar damages. They may occur in the form of micro-cracks and voids, and leads to a loss of performance. Within this framework, ultrasonic tests can be exploited in order to detect and classify the kind of defect. The main object of this work is to develop the evolution of a previous heuristic approach, based on the use of Support Vector Machines, proposed in order to recognize and classify the defect starting from the measured ultrasonic echoes. In this context, a real-time approach could be exploited to solve real industrial problems with enough accuracy and realistic computational efforts. Particularly, we discuss the cross wavelet transform and wavelet coherence for examining relationships in time-frequency domains between. For our aim, a software package has been developed, allowing users to perform the cross wavelet transform, the wavelet coherence and the Fuzzy Inference System. Since the ill-posedness of the inverse problem, Fuzzy Inference has been used to regularize the system, implementing a data-independent classifier. Obtained results assure good performances of the implemented classifier, with very interesting applications.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5447413"},{"name":"Influence of localized defect on transmission in a coaxial Bragg structure","snippet":"Detailed simulations are presented for the effect of localized defect on the transmission of a coaxial Bragg structure. It is found that if localized defect is introduced to both the outer-wall and the inner-rod corrugations, two pass-bands will generate in the initial stop-band associated with the inter-coupling of the operating mode with spurious modes, which are quite narrow and thus may provide promising applications in high-Q resonators, tunable narrow-band filters, and pulse compressors.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5475857"},{"name":"Design Techniques for Streamlined Integration and Fault Tolerance in a Distributed Sensor System for Line-crossing Recognition","snippet":"Distributed sensor system applications (e.g., wireless sensor networks) have been studied extensively in recent years. Such applications involve resource-limited embedded sensor nodes that communicate with each other through self-organizing protocols. Depending on application requirements, distributed sensor system design may include protocol and prototype implementation. Prototype implementation is especially useful in establishing and maintaining system functionality as the design is customized to satisfy size, energy, and cost constraints. In this paper, we present a streamlined, application-specific approach to incorporating fault tolerance into a TDMA-based distributed sensor system for line-crossing recognition. The objective of this approach is to prevent node failures from translating into failures in the overall system. Our approach is specialized and light-weight so that fault tolerance is achieved without significant degradation in energy efficiency. We also present an asynchronous handshaking approach for providing synchronization between the transceiver and digital processing subsystem in sensor node. This provides a general method for achieving such synchronization with reduced hardware requirements and reduced energy consumption compared to conventional approaches, which rely on generic interface protocols. We demonstrate the capabilities of our approaches to fault tolerance and transceiver-processor integration through experiments involving a complete prototype wireless sensor network test-bed, and a distributed line-crossing recognition application that runs on this test-bed.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4318007"},{"name":"A Skeletal-Based Approach for the Development of Fault-Tolerant SPMD Applications","snippet":"Distributing applications over PC clusters to speed-up or size-up the execution is now commonplace. Yet efficiently tolerating faults of these systems is a major issue. To ease the addition of checkpoint-based fault tolerance at the application level, we introduce a {em Model for Low-Overhead Tolerance of Faults}(MoLOToF) which is based on structuring applications using {em fault-tolerant skeletons}. MoLOToF also encourages collaborations with the programmer and the execution environment. The skeletons are adapted to specific parallelization paradigms and yield what can be called {em fault-tolerant algorithmic skeletons}. The application of MoLOToF to the SPMD parallelization paradigm results in our proposed FT-SPMD framework. Experiments show that the complexity for developing an application is small and the use of the framework has a small impact on performance. Comparisons with existing system-level checkpoint solutions, namely LAM\/MPI and DMTCP, point out that FT-SPMD has a lower runtime overhead while being more robust when a higher level of fault tolerance is required.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5704425"},{"name":"Analytically redundant controllers for fault tolerance: Implementation with separation of concerns","snippet":"Diversity or redundancy based software fault tolerance encompasses the development of application domain specific variants and error detection mechanisms. In this regard, this paper presents an analytical design strategy to develop the variants for a fault tolerant real-time control system. This work also presents a generalized error detection mechanism based on the stability performance of a designed controller using the Lyapunov Stability Criterion. The diverse redundant fault tolerance is implemented with an aspect oriented compiler to separate and thus reduce this additional complexity. A Mathematical Model of an Inverted Pendulum System has been used as a case study to demonstrate the proposed design framework.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5531539"},{"name":"DNA error correcting codes: No crossover.","snippet":"DNA error correcting codes over the edit metric create embeddable markers for sequencing projects that are tolerant of sequencing errors. When a sequence library has multiple sources for its sequences, use of embedded markers permit tracking of sequence origin. Evolutionary algorithms are currently the best known technique for optimizing DNA error correcting codes. In this study we resolve the question of the utility of the crossover operator used in earlier studies on optimizing DNA error correcting codes. The crossover operator in question is found to be substantially counterproductive. A majority of crossover events produce results that violate minimum-distance constraints required for error correction. A new algorithm, a form of modified evolution strategy, is tested and is found to locate codes with record size. The table of best know sizes for DNA-error correcting codes is updated.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4925705"},{"name":"Mask contribution on CD & OVL errors budgets for Double Patterning Lithography","snippet":"Double Patterning Technology (DPT) is now considered as the mainstream technology for 32 nm node lithography. The main DPT processes have been developed according targeted applications: spacer and pitch splitting either by dual line or dual trench approaches. However, the successful implementation of DPT requires overcoming certain technical challenges in terms of exposure tool capability, process integration, mask performance and finally metrology. For pitch splitting process, the mask performance becomes critical as the technique requires a set of two masks. This paper will focus on the mask impact to the global critical dimension (CD) and overlay (OVL) errors for DPT. The mask long-distance and local off-target CD variation and image placement were determined on DP features at 180 nm and 128 nm pitches, dedicated to 45 nm and 32 nm nodes respectively. The mask data were then compared to the wafer CD and OVL results achieved on same DP patterns. Edge placement errors have been programmed on DP like-structures on reticle in order to investigate the offsets impact on CD and image placement. The CD lines increases with asymmetric spaces adjacent to the drawn lines for offsets higher than 12 nm, and then have been compared to the corresponding density induced by individual dense and sparse symmetric edges and have been correlated to the simulated prediction. The single reticle trans-X offsets were then compared to the impact on CD by OVL errors in the double patterning strategy. Finally, the pellicle-induced reticle distortions impact on image placement errors was investigated. The mechanical performance of pellicle was achieved by mask registration measurements before and after pellicle removal. The reticle contribution to the overall wafer CD and OVL errors budgets were addressed to meet the ITRS requirements.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5760004"},{"name":"Analysis of Fault-Tolerant Performance of a Doubly Salient Permanent-Magnet Motor Drive Using Transient Cosimulation Method","snippet":"Doubly salient permanent-magnet (DSPM) motors offer the advantages of high power density and high efficiency. In this paper, it is examined that the DSPM motor is a new class of fault-tolerant machines, a potential candidate for many applications where reliability and power density are of importance. Fault analysis is performed in a DSPM motor drive, including internal and external faults. Due to the fact that the experimentation on a true motor drive for such a purpose is impractical because of its high cost and difficulty to make, a new cosimulation model of a DSPM motor drive is developed using coupled magnetic and electric circuit solvers. Last, to improve the performance of a DSPM motor drive with an open-circuited fault, a fault compensation strategy is proposed. Simulation and experimental results are presented, showing the effectiveness of the proposed cosimulation method and the high performance of the fault-tolerant characteristic of DSPM motor drives.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4441340"},{"name":"Image-space Correction of AR Registration Errors Using Graphics Hardware","snippet":"directly on top of physical objects in a video scene. Registration accuracy is a serious problem in these cases since any imprecisions are immediately apparent as virtual and physical edges and features coincide. We present a hardware-accelerated image-based post-processing technique that adjusts rendering of virtual geometry to better match edges present in images of a physical scene, reducing the visual effect of registration errors from both inaccurate tracking and oversimplified modeling. Our algorithm is easily integrable with existing AR applications, having no dependency on the underlying tracking technique. We use the advanced programmable capabilities of modern graphics hardware to achieve high performance without burdening the CPU.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1667650"},{"name":"Neural net and expert system diagnose transformer faults","snippet":"Dissolved gas-in-oil analysis (DGA) is a common practice in transformer incipient fault diagnosis. The analysis techniques include the conventional key gas method, ratio methods, and artificial intelligence methods. Application of artificial intelligence (Al) techniques have shown very promising results. The methods include fuzzy logic, expert systems (EPS), evolutionary algorithms (EA), and artificial neural networks (ANN). A transformer incipient fault diagnosis system (ANNEPS) was developed over a period of 5 years at Virginia Tech, collaborating with Doble Engineering Company. The system can detect thermal faults (distinguishing overheating of oil from that of cellulose and between four overheating stages and overheating of oil), low-energy discharge (partial discharge), high-energy discharge (arcing), and cellulose degradation","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=814667"},{"name":"Double circuit transmission line Fault Distance Location using Artificial Neural Network","snippet":"Distance relays used for protection of transmission lines have problems of under-reach, over-reach and maloperation due to high impedance faults. Further the problem is compounded when the distance relays are used for protection of double circuit transmission lines due to effect of zero sequence mutual coupling. Different types of faults on a protected transmission line should be located correctly. This paper presents a single neural network for fault distance location for all the ten types of faults (3 LG, 3 LLG, 3 LL, 1 LLL) in both the circuits of a double circuit transmission line fed from sources at both the end. This technique uses only one end data and accurate fault distance location is achieved after one cycle from the inception of fault. The proposed Artificial Neural Network (ANN) based Fault Distance Locator uses fundamental components of three phase current signals of both the circuits & three phase voltage signals to learn the hidden relationship in the input patterns. An improved performance is obtained once the neural network is trained suitably, thus performing correctly when faced with different system parameters and conditions i.e. varying fault type, fault location, fault resistance, fault inception angle, presence of mutual coupling and remote source infeed.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5393593"},{"name":"Designing a fault-tolerant architecture for real-time distributed control system","snippet":"Distributed control systems play major roles in real-time applications. In some circumstances human lives may depend on these systems. Hence, they should be highly dependable. Since there is no specific way to forecast a failure, the systems should have fault-tolerant features to allow them to continue to operate in the presence of fault. In this research, a fault-tolerant architecture for real-time distributed control system was developed. A fault-tolerant node was designed and its performance was evaluated using the Markov chains model. All these nodes communicate via a network. To choose the most suitable network for this system, several networks were analyzed qualitatively. Several medium access control protocols were also compared quantitatively using OMNET++. From the results obtained, a new fault-tolerant architecture is proposed. This system possesses high reliability not only at the node stage but also at the network stage, hence increasing the reliability of the overall system.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1033046"},{"name":"A Distributed Fault-Tolerant Algorithm for Event Detection Using Heterogeneous Wireless Sensor Networks","snippet":"Distributed event detection using wireless sensor networks has received growing interest in recent years. In such applications, a large number of inexpensive and unreliable sensor nodes are distributed in a geographical region to make firm and accurate local decisions about the presence or absence of specific events based on their sensor readings. However, sensor readings can be unreliable, due to either noise in the sensor readings or hardware failures in the devices, and may cause nodes to make erroneous local decisions. We present a general fault-tolerant event detection scheme that allows nodes to detect erroneous local decisions based on the local decisions reported by their neighbors. This detection scheme does not assume homogeneity of sensor nodes and can handle cases where nodes have different accuracy levels. We prove analytically that the derived fault-tolerant estimator is optimal under the maximum a posteriori (MAP) criterion. An equivalent weighted voting scheme is also derived. Further, we describe two new error models that take into account the neighbor distance and the geographical distributions of the two decision quorums. These models are particularly suitable for detection applications where the event under consideration is highly localized. Our fault-tolerant estimator is simulated using a network of 1024 nodes deployed randomly in a square region and assigned random probability of failures","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4177877"},{"name":"Intelligent fault-tolerant CORBA service on real-time CORBA","snippet":"Distributed object applications can be made fault tolerant by replicating their constituent objects, and by distributing these replicas across the different computers in the network. The idea behind object replication is that the failure of one replica of an object can be masked from a client of the object because the other replicas can continue to perform any operation that the client requires. We propose IFTS (Intelligent Fault Tolerant CORBA Service) for handling faults of server object replica using a replication concept to support fault tolerance. It can choose the fastest primary replica using the multicast mechanism. It also introduces passive replication for secure fault tolerance. Furthermore, we propose the design and implementation of IFTS service to provide reliability and faster service using multicast technology by extending existing CORBA ORB","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=992721"},{"name":"ACCE: Automatic correction of control-flow errors","snippet":"Detection of control-flow errors at the software level has been studied extensively in the literature. However, there has not been any published work that attempts to correct these errors. Low-cost correction of CFEs is important for real-time systems where checkpointing is too expensive or impossible. This paper presents automatic correction of control-flow errors (ACCE), an efficient error correction algorithm involving addition of redundant code to the program. ACCE has been implemented by modifying GCC, a widely used C compiler, and performance measurements show that the overhead is very low. Fault injection experiments on SPEC and MiBench benchmark programs compiled with ACCE show that the correct output is produced with high probability and that CFEs are corrected with a latency of a few hundred instructions.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4437639"},{"name":"A new method for fault section estimation in distribution network","snippet":"Determination of fault section is a necessary step for locating the fault in the distribution power system. In this paper a new practical based method is presented for fault section estimation in distribution system. In the proposed method, at first different zones is defined using impedance classifier. Then, the suitable locations for installing the cutout fuses are determined using expert of designer. After that, special settings for cutout fuse links are determined in such a way that they operate coordinately. Finally, current waveforms are used to distinguish which cutout fuse operated or in which section fault occurred.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5666632"},{"name":"Defect identification of lumber through correlation technique with statistical and textural feature extraction method","snippet":"Feature extraction is an important component of a pattern recognition system. A well-defined feature extraction algorithm makes the identification process more effective and efficient. Several techniques exist for the quality checking of wooden materials. However, image based quality checking of wooden materials still remains a challenging task. Although trivial quality checking methods are available, they do not give useful results in most situations. This paper addresses the issue of quality checking of wooden materials using statistical and textural feature extraction techniques with high accuracy and reliability. In our work, a wood defect identification system has been designed based on pre-processing techniques, feature extraction and by correlating the features of those wood species for their classification. The most popular technique used for the textural classification is Gray-level Co-occurrence Matrices (GLCM). The features from the enhanced images are thus extracted using the GLCM is correlated, which determines the classification between the various wood species. Experiments conducted under the proposed conditions showing significant results are presented.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5738784"},{"name":"Automated Diagnosis of Product-Line Configuration Errors in Feature Models","snippet":"Feature models are widely used to model software product-line (SPL) variability. SPL variants are configured by selecting feature sets that satisfy feature model constraints. Configuration of large feature models can involve multiple stages and participants, which makes it hard to avoid conflicts and errors. New techniques are therefore needed to debug invalid configurations and derive the minimal set of changes to fix flawed configurations. This paper provides three contributions to debugging feature model configurations: (1) we present a technique for transforming a flawed feature model configuration into a constraint satisfaction problem (CSP) and show how a constraint solver can derive the minimal set of feature selection changes to fix an invalid configuration, (2) we show how this diagnosis CSP can automatically resolve conflicts between configuration participant decisions, and (3) we present experiment results that evaluate our technique. These results show that our technique scales to models with over 5,000 features, which is well beyond the size used to validate other automated techniques.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4626856"},{"name":"Adding fault tolerance mechanisms to Interbus-S","snippet":"Field bus technology is now a reality in industrial environments. There are many field bus systems commercially available, and each is suitable for particular kinds of applications. In this scenario the Interbus-S system is playing a leading role, due to the efficiency of its protocol. However, a drawback of this communication system is the centralisation of the mono-master arbitration scheme. The presence of a single device to co-ordinate communication activities makes the Interbus-S protocol vulnerable to fault occurrences in the master. Maintaining full compatibility with the existing standard, the authors have defined a protocol extension which allows the whole communication system to continue working after the occurrence of a fault in the master node","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=882545"},{"name":"Adding Integrity Verification Capabilities to the LDPC-Staircase Erasure Correction Codes","snippet":"File distribution is becoming a key technology, in particular in large scale content broadcasting systems like DVB-H\/SH. They largely rely on Application Level FEC codes (AL-FEC) in order to recover from transmission erasures. We believe that sooner or later, content integrity and source authentication security services will be required in these systems. In order to save the client terminal resources, which can be a handheld autonomous device, we have designed a hybrid system that merges the AL-FEC decoding and content integrity\/source authentication services. More precisely our system can detect a random object corruption triggered by a deliberate attack with a probability close to 100% almost for free in terms of computation overhead. The case of intelligent corruptions is also addressed and counter measures proposed.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5426110"},{"name":"Simulation-Based Bug Trace Minimization With BMC-Based Refinement","snippet":"Finding the cause of a bug can be one of the most time-consuming activities in design verification. This is particularly true in the case of bugs discovered in the context of a random-simulation-based methodology, where bug traces, or counterexamples, may be several hundred thousand cycles long. In this paper, BUg TRAce MINimization (Butramin), which is a bug trace minimizer, is proposed. Butramin considers a bug trace produced by a random simulator or semiformal verification software and produces an equivalent trace of shorter length. Butramin applies a range of minimization techniques, deploying both simulation-based and formal methods, with the objective of producing highly reduced traces that still expose the original bug. Butramin was evaluated on a range of designs, including the publicly available picoJava microprocessor, and bug traces up to one million cycles long. Experiments show that in most cases, Butramin is able to reduce traces to a very small fraction of their initial sizes, in terms of cycle length and signals involved. The minimized traces can greatly facilitate bug analysis and reduce regression runtime","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4039508"},{"name":"Fault Detection Structures for the Montgomery Multiplication over Binary Extension Fields","snippet":"Finite field arithmetic is used in applications like cryptography, where it is crucial to detect the errors. Therefore, concurrent error detection is very beneficial to increase the reliability in such applications. Multiplication is one of the most important operations and is widely used in different applications. In this paper, we target concurrent error detection in the Montgomery multiplication over binary extension fields. We propose error detection schemes for two Montgomery multiplication architectures. First, we present a new concurrent error detection scheme using the time redundancy and apply it on semi-systolic array Montgomery multipliers. Then, we propose a parity based error detection scheme for the bit-serial Montgomery multiplier over binary extension Fields.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4318983"},{"name":"Evolutionary design and adaptation of digital filters within an embedded fault tolerant hardware platform","snippet":"Finite impulse response filters (FIRs) are crucial device for robust data communication and manipulation. Multiplierless filters have been shown to produce high performance systems with fast signal processing and reduced area. Furthermore, the distributed architecture inherent in multiplierless filters makes it a suitable candidate for fault tolerant design. Alternative approaches to the design of fault tolerant systems have been proposed using evolutionary algorithms (EAs) and the concept of evolvable hardware (EHW). This paper presents an evolvable hardware platform for the automated design and adaptation of multiplierless digital filters. Filters are realised within a dedicated programmable logic array (PLA). The platform employs a genetic algorithm to autonomously configure the PLA for a give set of coefficients. The ability of the platform to adapt to increasing numbers of faults was investigated through the evolution of a 31-tap low-pass FIR filter. Results show that the functionality of filters evolved on the PLA was maintained despite an increasing number of faults covering up to 25% of the PLA area. Additionally, three PLA initialisation methods were investigated to ascertain which produced the fastest fault recovery times. It was shown that seeding a population of random configuration-strings with the best configuration currently obtained resulted in a 6 fold increase in fault recovery speed over other methods investigated","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=937954"},{"name":"Adaptive FMO selection strategy for error resilient H.264 coding","snippet":"Flexible macroblock ordering (FMO) is one of the effective error resilient tools in H.264\/AVC video coding standard. Nevertheless the issue of how to suitably arrange the macroblocks in suitable FMO mapping type for different video applications is yet to be clarified and investigated. In this paper, we are analyzing the tradeoff and effectiveness of the six fixed FMO types, and based these six fixed FMO types, using the joint source-channel rate distortion optimization (RDO) principle to propose an adaptive FMO type selection strategy for different video scenes and applications. The experiment results shows that our method has more compatibility and flexibility than the six fixed FMO types, and better error resilience than most of them.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4589969"},{"name":"Implementation of reconfiguration management in fault-adaptive control systems","snippet":"Fault adaptive systems must adapt and reconfigure themselves to the changes in the environment or the system itself, and have to maintain operation even in case of system failures. In order to avoid performance degradation due to system reconfigurations, adequate reconfiguration management is necessary. This paper describes a fault-adaptive control system with multilayer control and a reconfiguration management system.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1006826"},{"name":"Comparison of the four configurations of the inductive Fault Current Limiter","snippet":"Fault current limiters (FCLs) are expected to play an important role in the protection of the future power networks, since the increase of loads and expansion of the power networks lead to much higher short-circuit power. This paper presents the comparison of four different configurations of inductive FCL, with respect to the FCL weight (magnetic core and winding material) and losses during both the nominal and the fault state of operation. Two main challenges in the inductive FCL design are reduction of the material weight and reduction of the induced dc winding over-voltage during the fault period. So far, solutions (core configurations) proposed in the literature are: decoupling of the dc and the ac magnetic circuits to avoid high voltages across the dc winding during a fault and the so-called open- core configuration. The presented results reveal the merits and drawbacks of each of the configurations and compare them to the conventional inductive FCL design characteristics. The results are obtained through the simulations in SaberDesinger and by experiments.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4592574"},{"name":"Broken rotor bar fault detection in induction motors using starting current analysis","snippet":"Fault detection based on a common steady-state analysis technique, such as FFT, is known to be significantly dependant on the loading conditions of induction motors. At light load, it is difficult to distinguish between healthy and faulty rotors because the characteristic broken rotor bar fault frequencies are very close to the fundamental component and their amplitudes are small in comparison. As a result, detection of the fault and classification of the fault severity under light load is almost impossible. In order to overcome this problem, this paper investigates the detection of rotor faults in induction machines by analysing the starting current using a newly developed quantification technique based on the wavelet transform. The analysis technique applies the wavelet transform to the envelope of the starting current. The envelope extraction is used to remove the strong fundamental component, which overshadows the characteristic differences between a healthy motor and a faulty motor with broken rotor bars. The results are then verified using tests on a machine with a varying numbers of broken bars. The effects of initial rotor position, supply imbalance and loading are also investigated","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1665592"},{"name":"Detecting faults in four symmetric key block ciphers","snippet":"Fault detection in encryption algorithms is gaining in importance since fault attacks may compromise even recently developed cryptosystems. We analyze the different operations used by various symmetric ciphers and propose possible detection codes and frequency of checking. Several examples (i.e., AES, RC5, DES and IDEA) are presented to illustrate our analysis.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1342476"},{"name":"A new approach for fault detection in digital relays-based power system using Petri nets","snippet":"Fault detection in power systems, from the viewpoint of its required speed and accuracy, needs to be investigated yet. The hidden faults resulted from malfunctioning of circuit breakers and incorrect warnings of relays and happening of several faults, are the main difficulties in monitoring of power system. In this paper, Petri nets have been used for modeling and location detection of faults in power systems. In this deductive method, using the information of protection system's situation, the estimation of the faulted sections has been modeled using Petri nets. This deductive process can be represented graphically based on Petri nets and executed using matrix operations. The inputs of fault diagnosis models are the acquired data by the Remote Terminal Units (RTU) of the Supervisory Control and Data Acquisition (SCADA) system including the relay's trip signals and the signal of circuit breaker's situation. Logical operand information of digital protection relays such as pickup and operation of protection devices are more reliable than that of SCADA in reflection of relays trip condition. They can be added to the information of SCADA based fault diagnosis models. By using Petri nets, the processing time of information is reduced and the precise of fault detection procedure is increased. Also the proposed approach provides hierarchical monitoring of power systems.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5712527"},{"name":"A fault classification model of modern automotive infotainment system","snippet":"Fault detection, analyzing and fixing are major challenges in automotive field especially in modern complex infotainment system. To diagnose and fix a fault, it is essential to find a proper fault classification model. This paper presents a fault classification model of modern automotive infotainment system which consists of electronic control units. After investigating automotive infotainment system from different possible aspects, a fault classification scheme is proposed which best fits in this field.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5289263"},{"name":"Synthesis Of Optimal-Cost Dynamic Observers for Fault Diagnosis of Discrete-Event Systems","snippet":"Fault diagnosis consists in synthesizing a diagnoser that observes a given plant through a set of observable events, and identifies faults which are not observable as soon as possible after their occurrence. Existing literature on this problem has considered the case of static observers, where the set of observable events does not change during execution of the system. In this paper, we consider dynamic observers, where the observer can switch sensors on or off, thus dynamically changing the set of events it wishes to observe. We define a notion of cost for such dynamic observers and show that (i) the cost of a given dynamic observer can be computed and (ii) an optimal dynamic observer can be synthesized.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4239975"},{"name":"Ventilator Fault Diagnosis Based on Fuzzy Theory","snippet":"Fault diagnosis has been the research hotspot in the industry fields. It has a practical significance to discuss the effective fault diagnosis methods. Aiming at the fuzzy and random features of the occurrence probabilities, this paper presents a hybrid method that combines the fault tree with fuzzy set theory.In this approach, fuzzy aggregation and defuzzification are adopted and this method is used in ventilator fault diagnosis. The research shows that this method is feasible and effective and can be applied to the other rotating machinery fault diagnosis.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5365808"},{"name":"Exploratory analysis of massive data for distribution fault diagnosis in smart grids","snippet":"Fault diagnosis in power distribution systems is critical to expedite the restoration of service and improve the reliability. With power grids becoming smarter, more and more data beyond utility outage database are available for fault cause identification. This paper introduces basic methodologies to integrate and analyze data from different sources. Geographic information system (GIS) provides a framework to integrate these data through spatial and temporal relations. Features extracted from raw data provide different discriminant powers, which can be evaluated by the likelihood measure. A fault cause classifier is then trained to learn the relations between fault causes and the features. Two statistical methods, linear discriminant analysis (LDA) and logistic regression (LR), are introduced. The assumptions, general approaches and performances of these two techniques are discussed and evaluated on a real-world outage dataset.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5275689"},{"name":"Introducing dynamics in a fault diagnostic application using Bayesian Belief Networks","snippet":"Fault diagnostic techniques are required to determine whether a fault has occurred in a system and to identify the component failures that may have caused it. This task can be complicated when dealing with complex systems and dynamic behaviour, in particular, introduces further difficulties. This paper presents a method for fault detection on dynamic systems using Bayesian Belief Networks (BBNs). Possible trends are identified for the variables in the systems that are monitored by the sensors. Fault Trees (FTs) are built to represent the causality of the trends and these are then converted into BBNs. The networks developed for different sections are connected together to form a unique concise network. For a combination of sensors which deviate from the expected trends, calculating the updated probability enables a list of potential causes for the system scenarios to be obtained. A simple water tank system has been used to validate the method.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5270213"},{"name":"Parsifal: A Generic and Configurable Fault Emulation Environment with Non-Classical Fault Models","snippet":"Fault emulation has become an important tool for test evaluation. However, until now fault models other than the stuck-at fault model have rarely been used in emulation. In this paper, we propose non-classical fault models for emulation and a generic fault emulation environment capable of supporting these and other fault models and different emulation modes in a common support framework. Although different in logical implementation and physical abstraction level, all fault models are administered and applied together and can even be mixed in a single fault grading campaign. The proposed fault emulation environment is not restricted in its use to a certain emulator. The modular approach proposed in this paper allows an easy adaption for different emulation systems and reuse of all key components including the fault models. Those may be applied during fault grading campaigns as well as in-circuit emulation. We will present results obtained on the emulation system Mercury+ by Cadence","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4100982"},{"name":"Evaluating the Use of Reference Run Models in Fault Injection Analysis","snippet":"Fault injection (FI) has been shown to be an effective approach to assessing the dependability of software systems. To determine the impact of faults injected during FI, a given oracle is needed. Oracles can take a variety of forms, including (i) specifications, (ii) error detection mechanisms and (iii) golden runs. Focusing on golden runs, in this paper we show that there are classes of software which a golden run based approach can not be used to analyse. Specifically, we demonstrate that a golden run based approach can not be used in the analysis of systems which employ a main control loop with an irregular period. Further, we show how a simple model, which has been refined using FI experiments, can be employed as an oracle in the analysis of such a system.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5368242"},{"name":"An improved fault locating system of distribution network based on fuzzy identification","snippet":"Fault locating system, which is designed for the fast power recovery, is very important in the economical operating of the distribution network. But, for the uncertainty of the fault information, the incorrect conclusion may be obtained by the traditional fault location calculation, so the most fault locating system can not be employed in the distribution network. In this paper, an improved fault locating system is proposed, which is composed of the fault signal acquisition unit and the fault location analysis center. Fuzzy identification is employed in the fault location analysis center to deal with the uncertainty of fault information. The failure and mistake rates of indicator action are used as the fuzzy parameters to calculate the fuzzy difference of the fault sequence and the standard fault set. The fault indicator is the primary device of fault information acquisition. The radio frequency and GPRS technology construct the communication channel of fault signal acquisition unit, which cuts down the construction cost and also ensures the obtaining accuracy of the fault information. The fault location system is working on the distribution network and operating well. With the accurate fault location, power supply recovers fast. The loss of power failure is reduced effectively.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5735873"},{"name":"Using PQ Monitoring and Substation Relays for Fault Location on Distribution Systems","snippet":"Fault location is of considerable interest for utilities to improve their reliability and speed storm restorations. Power quality recorders, relays, and other monitors can provide information to help locate faults. In this paper, some basic impedance-based fault-location methods are evaluated on utility measurement data with known fault locations. The main finding is that reasonably accurate fault locations are possible on a wide range of distribution circuits with either feeder-level or bus-level substation monitoring. Another important finding described is how monitoring can be used to estimate the parameters of the fault arc. This can improve fault locations and help with accident investigations, equipment failure forensics, and other hazards related to the power and energy created by the arc","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4225382"},{"name":"Fault location using traveling wave for power networks","snippet":"Fault location using traveling wave has been applied in extra-high voltage power grids successfully. Due to its complication and high cost, it is not easy for this technique to be accepted for use in distribution system. A new traveling wave fault location system is developed simply in a cost-effective way for power networks (especially for distribution system) in this paper. Two traveling wave sensors are developed to capture the current traveling wave flowing from the capacitive equipment to earth and the voltage traveling waves in all three phases. The outputs of the sensors are then applied to the trigger and time tagging by using Global Position System (GPS) receiver. The fault position is calculated by the traveling wave arrival times in every power station where only one fault locator is installed. The fault location system is tested in the power system. Testing results show that the fault locator has high precision and robustness.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1348815"},{"name":"Current fault management trends in NASA's planetary spacecraft","snippet":"Fault management for today's space missions is a complex problem, going well beyond the typical safing requirements of simpler missions. Recent missions have experienced technical issues late in the project lifecycle, associated with the development and test of fault management capabilities, resulting in both project schedule delays and cost overruns. Symptoms seem to become exaggerated in the context of deep space and planetary missions, most likely due to the need for increased autonomy and the limited communications opportunities with Earth-bound operators. These issues are expected to cause increasing challenges as the spacecraft envisioned for future missions become more capable and complex. In recognition of the importance of addressing this problem, the Discovery and New Frontiers Program Office hosted a Fault Management Workshop on behalf of NASA's Science Mission Directorate, Planetary Science Division, to bring together experts in fault management from across NASA, DoD, industry and academia. The scope of the workshop was focused on deep space and planetary robotic missions, with full recognition of the relevance of, and subsequent benefit to, Earth-orbiting missions. Three workshop breakout sessions focused the discussions to target three topics: 1) fault management architectures, 2) fault management verification and validation, and 3) fault management development practices, processes and tools. The key product of this three-day workshop is a NASA White Paper that documents lessons learned from previous missions, recommended best practices, and future opportunities for investments in the fault management domain. This paper summarizes the findings and recommendations that are captured in the white paper.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4839530"},{"name":"Fault Management Using the CONMan Abstraction","snippet":"Fault management in networks is difficult. We argue that a major contributor to the difficulty of debugging network faults is the sheer volume of semantically anemic details exposed by protocols. Unlike past approaches that try to cope with the deluge of information exposed, in this paper we explore how to reduce and structure the management information exposed by data-plane protocols and devices to make them more amenable to fault management. To this effect, we delineate two conditions that the management interface of data-plane protocols should satisfy: it should provide a structured description of protocol reality and it should support what we call a \"conservation of bytes\" invariant. Based on this, we propose an architecture wherein data- plane protocols expose management information satisfying these conditions. This allows management applications to detect, localize and (possibly) resolve faults in a structured fashion. We discuss the detection of a representative set of real-world faults to illustrate our approach. We implemented these fault management features into three protocols and built a management application that uses the features to debug faults. Apart from serving as a proof of concept, this exercise indicates that our proposal does indeed simplify debugging of a large fraction of network faults.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5061985"},{"name":"Case study of designing a fault recorder system","snippet":"Fault recorders are reckoned among the most important components of protection systems within high voltage substations which through continuous monitoring the power system parameters and diagnosing fault situations as well as recording the parameters status before and after the occurrence of a faults, assisting substation engineers to identifying the cause of the failures and eliminating them. Designing the fault recorders using innovative and breakthrough hardware and software components such as high-speed processors, high capacity mass storages, accurate analog to digital converters, real time operating systems etc, enables them to collect a huge amount of precise information from the status of power network. This information can subsequently be used by evaluating software to analysis the circumstances of the fault to avoid repeating it in the future. In this paper the designing a fault recorder system is discussed as a case study for designing an embedded system, considering its specific requirements and exceptions.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1364837"},{"name":"A new distributed approach for building balanced ring for fault tolerance in mesh architecture","snippet":"Fault ring (f-ring) is a popular model for the fault tolerance in grid based architectures such as 2D-mesh and torus. In the work proposed by Huxi Gu et al. introduced the concept of a balanced ring( b-ring) to reduce the traffic load on the fault ring to achieve fault tolerance in a mesh architecture. However, central to their work is the formation of the balanced ring that surrounds a fault ring. In this paper, we propose a new distributed approach for the formation of the balanced ring. Our approach is based on eight-neighborhood property that requires only the local information of the faulty nodes in contrast to the global knowledge as needed by the algorithm.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5397940"},{"name":"Perturbation-based Fault Screening","snippet":"Fault screeners are a new breed of fault identification technique that can probabilistically detect if a transient fault has affected the state of a processor. We demonstrate that fault screeners function because of two key characteristics. First, we show that much of the intermediate data generated by a program inherently falls within certain consistent bounds. Second, we observe that these bounds are often violated by the introduction of a fault. Thus, fault screeners can identify faults by directly watching for any data inconsistencies arising in an application's behavior. We present an idealized algorithm capable of identifying over 85% of injected faults on the SpecInt suite and over 75% overall. Further, in a realistic implementation on a simulated Pentium-III-like processor, about half of the errors due to injected faults are identified while still in speculative state. Errors detected this early can be eliminated by a pipeline flush. In this paper, we present several hardware-based versions of this screening algorithm and show that flushing the pipeline every time the hardware screener triggers reduces overall performance by less than 1%","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4147658"},{"name":"A Modified BCE Algorithm for Fault-Tolerance Scheduling of Periodic Tasks in Hard Real-Time Systems","snippet":"Fault tolerance is an important aspect of real-time control systems, due to unavoidable timing constraints. In this paper, the timing problem of a set of concurrent periodic tasks is considered where each task has primary and alternate versions. In the literature, probability of fault in the alternate version of a task is assumed to be zero. Here, a fault probability with uniform distribution has been used. In addition, to cover the situations in which both versions are scheduled with some time overlapping, a criterion is defined for prioritizing primary version against the alternate version. A new scheduling algorithm is proposed based on the defined criterion. Simulation results show that an increase in the number of executed primary tasks which improves the efficiency of processor utilization, hence prove the efficiency of the proposed algorithm.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5071998"},{"name":"An optimal point in scheduling real-time tasks process based on fault tolerant imprecise computation model","snippet":"Fault tolerance is an important issue due to the critical nature of the supported tasks of real-time computer systems, since timing constraints must not be violated. The imprecise computation technique has been proposed as a way to handle transient overload and to enhance fault tolerant of real-time systems. This paper introduces an exact theoretical analysis for the imprecise computation model based on three principles of maximize reward-based test, minimize response-time test, and minimize errors test, then finds optimal-point in scheduling process to satisfy three scheduling conditions. Further this is also demonstrated by the simulation results.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1175401"},{"name":"Modeling fault-tolerant mobile agent execution as a sequence of agreement problems","snippet":"Fault tolerance is fundamental to the further development of mobile agent applications. In the context of mobile agents, fault tolerance prevents a partial or complete loss of the agent, i.e. ensures that the agent arrives at its destination. Simple approaches such as checkpointing are prone to blocking. Replication can in principle improve solutions based on checkpointing. However existing solutions in this context either assume a perfect failure detection mechanism (which is not realistic in an environment such as the Internet), or rely on complex solutions based on leader election and distributed transactions, where only a subset of solutions prevents blocking. The paper proposes a novel approach to fault tolerant mobile agent execution, which is based on modeling agent execution as a sequence of agreement problems. Each agreement problem is one instance of the well understood consensus problem. Our solution does not require a perfect failure detection mechanism, while preventing blocking and ensuring that the agent is executed exactly once","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=885388"},{"name":"FATOMAS-a fault-tolerant mobile agent system based on the agent-dependent approach","snippet":"Fault tolerance is fundamental to the further development of mobile agent applications. In the context of mobile agents, fault-tolerance prevents a partial or complete loss of the agent, i.e., it ensures that the agent arrives at its destination. We present FATOMAS, a Java-based fault-tolerant mobile agent system based on an algorithm presented in an earlier paper (2000). Contrary to the standard \"place-dependent\" architectural approach, FATOMAS uses the novel \"agent-dependent\" approach. In this approach, the protocol that provides fault tolerance travels with the agent. This has the important advantage to allow fault-tolerant mobile agent execution without the need to modify the underlying mobile agent platform (in our case ObjectSpace's Voyager). In our performance evaluation, we show the costs of our approach relative to the single, non-replicated agent execution. Pipelined mode and optimized agent forwarding are two optimizations that reduce the overhead of a fault-tolerant mobile agent execution.","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=941407"},{"name":"Doubly Fed Induction Generator Model-Based Sensor Fault Detection and Control Loop Reconfiguration","snippet":"Fault tolerance is gaining interest as a means to increase the reliability and availability of distributed energy systems. In this paper, a voltage-oriented doubly fed induction generator, which is often used in wind turbines, is examined. Furthermore, current, voltage, and position sensor fault detection, isolation, and reconfiguration are presented. Machine operation is not interrupted. A bank of observers provides residuals for fault detection and replacement signals for the reconfiguration. Control is temporarily switched from closed loop into open-loop to decouple the drive from faulty sensor readings. During a short period of open-loop operation, the fault is isolated using parity equations. Replacement signals from observers are used to reconfigure the drive and reenter closed-loop control. There are no large transients in the current. Measurement results and stability analysis show good results.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4773265"},{"name":"Fault diversity among off-the-shelf SQL database servers","snippet":"Fault tolerance is often the only viable way of obtaining the required system dependability from systems built out of \"off-the-shelf\" (OTS) products. We have studied a sample of bug reports from four off-the-shelf SQL servers so as to estimate the possible advantages of software fault tolerance - in the form of modular redundancy with diversity - in complex off-the-shelf software. We checked whether these bugs would cause coincident failures in more than one of the servers. We found that very few bugs affected two of the four servers, and none caused failures in more than two. We also found that only four of these bugs would cause identical, undetectable failures in two servers. Therefore, a fault-tolerant server, built with diverse off-the-shelf servers, seems to have a good chance of delivering improvements in availability and failure rates compared with the individual off-the-shelf servers or their replicated, nondiverse configurations.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1311908"},{"name":"CARRIAGE: fault tolerant CORBA system based on portable interceptors","snippet":"Fault tolerance requirement, which demands the reliability and consistency in compute systems, becomes a hot issue in current distributed object application field. Based on standardized portable interceptors mechanism, CARRIAGE system has successfully integrated ORBUS, a CORBA implementation developed by the authors and EDEN, a fault-tolerant framework into a whole new fault tolerant CORBA system, which uses active replication style to enhance fault tolerance service in CORBA domain with low cost and high efficiency. Practice has identified that this software prototype conforms to the standard specification thoroughly and provides a feasible and convenient way to glue the legacy systems without modifying the original systems or the application programs either.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1194660"},{"name":"Formal fault tree analysis of state transition systems","snippet":"Fault tree analysis (FTA) is a traditional deductive safety analysis technique that is applied during the system design stage. However, traditional FTA does not consider transitions between states, and it is difficult to decompose complex system fault events that are composed of multiple normal components' states rather than individual component failures. To solve these problems, we first propose two different fault events of fault trees, and then present a formal fault tree construction model by introducing the concept of transition rules for event decomposition, in which the semantics of gates and minimal cut sets of fault trees are revised compared with traditional FTA.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1579128"},{"name":"Formal static fault tree analysis","snippet":"Fault tree analysis (FTA) is a traditional informal reliability and safety analysis technique. FTA is basically a combinational model in which standard Boolean logic constructs, such as AND and OR gates, are used to decompose the fault events. Several dynamic constructs, such as Functional Dependency (FDEP) and Priority AND (PAND) gates, are also proposed to handle dynamic behaviors of system failure mechanisms. In this article, we focus on some paradoxes and constraints of the traditional FDEP and PAND gates, and present our static solutions to these dynamic gates. The proposed static fault tree model is formalized with Maude, an executable algebraic formal specification language. Two example fault tolerant parallel processor (FTPP) configurations are used to demonstrate our static fault tree model.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5674869"},{"name":"Adaptive partition size temporal error concealment for H.264","snippet":"Existing temporal error concealment methods for H.264 often decide the partition size of the lost macroblock (MB) before recovering the motion information, without actual quality comparison between different partition modes. In this paper, we propose to select the best partition mode by minimizing the Weighted Double-Sided External Boundary Matching Error (WDS-EBME), which jointly measures the inter-MB boundary discontinuity, inter-partition boundary discontinuity and intrapartition block artifacts in the recovered MB. The proposed method estimates the best motion vectors for each of the candidate partition modes, calculates the overall WDS-EBME values for them, and selects the partition mode with the smallest overall WDS-EBME to recover the lost MB. We also propose a progressive concealment order for the 4times4 partition mode. Test results show that the adaptive partition size method always outperforms the fixed partition size methods. Both the adaptive and fixed partition size methods are much superior to the temporal error concealment (TEC) method in the H.264 reference software.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4746376"},{"name":"Shared Data from a Study of Measurement Uncertainty in Fault Injection","snippet":"Experimental dependability studies usually produce an amount of data substantially greater than what can be presented in a research paper or a technical report. For this reason, authors condensate the results into more succinct forms that allow them to convey their message. Since a large amount of the original data is left unexplored, sharing it allows other teams to discover additional facts (as well as to compare the results to other studies). In a previous paper, we investigated sources of uncertainty in measurement results obtained using three different fault injection techniques. The resulting experimental data was shared in the AMBER raw data repository. This paper gives an overview of the study and makes an attempt at further exploring the shared data.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5628855"},{"name":"Trustworthy Evaluation of a Safe Driver Machine Interface through Software-Implemented Fault Injection","snippet":"Experimental evaluation is aimed at providing useful insights and results that constitute a confident representation of the system under evaluation. Although guidelines and good practices exist and are often applied, the uncertainty of results and the quality of the measuring system is rarely discussed. To complement such guidelines and good practices in experimental evaluation, metrology principles can contribute in improving experimental evaluation activities by assessing the measuring systems and the results achieved. In this paper we present the experimental evaluation by software-implemented fault injection of a safe train-borne driver machine interface (DMI), to evaluate its behavior in presence of faults. The measuring system built for the purpose and the results obtained on the assessment of the DMI are scrutinized along basic principles of metrology and good practices of fault injection. Trustfulness in results has been estimated satisfactory and the experimental campaign has shown that the safety mechanisms of the DMI correctly identify the faults injected and that a proper reaction is executed.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5368539"},{"name":"Efficiency analysis of illumination correction methods for face recognition performance","snippet":"Face recognition is an important task in the computer vision community leading to multiple applications such as building control access, video surveillance or forensics, to mention only a few. Face images are acquired in the enrollment process to form a database, which is the first stage usually performed off line. Second stage implies real-time test procedure where a new unseen face is captured and the face recognition system authorizes or does not, or recognize the identity of the person, based on similarity matching between the new acquired face image and the ones existing in the database, providing a matching score. When the acquiring conditions provided by the enrollment process highly differ by test environmental conditions, some preprocessing steps may be required. An instance of such step is given by illumination correction. The paper aims at analyzing the efficiency of five recent state-of-the-art normalization approaches in term of illumination correction and their effect on face recognition. Surprisingly, to the best of our knowledge, no systematic comparison exist in the literature to date, fact that had motivated us to carry out such analysis. We should also note that, apart from the face recognition task, other domains, such as medical imaging, could benefit from those preprocessing techniques.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5606436"},{"name":"Algorithm-Based Fault Tolerance for Fail-Stop Failures","snippet":"Fail-stop failures in distributed environments are often tolerated by checkpointing or message logging. In this paper, we show that fail-stop process failures in ScaLAPACK matrix-matrix multiplication kennel can be tolerated without checkpointing or message logging. It has been proved in previous algorithm-based fault tolerance that, for matrix-matrix multiplication, the checksum relationship in the input checksum matrices is preserved at the end of the computation no mater which algorithm is chosen. From this checksum relationship in the final computation results, processor miscalculations can be detected, located, and corrected at the end of the computation. However, whether this checksum relationship can be maintained in the middle of the computation or not remains open. In this paper, we first demonstrate that, for many matrix matrix multiplication algorithms, the checksum relationship in the input checksum matrices is not maintained in the middle of the computation. We then prove that, however, for the outer product version algorithm, the checksum relationship in the input checksum matrices can be maintained in the middle of the computation. Based on this checksum relationship maintained in the middle of the computation, we demonstrate that fail-stop process failures (which are often tolerated by checkpointing or message logging) in ScaLAPACK matrix-matrix multiplication can be tolerated without checkpointing or message logging.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4492768"},{"name":"hFT-FW: Hybrid Fault-Tolerance for Cluster-Based Stateful Firewalls","snippet":"Failures are a permanent menace for the availability of Internet services. During the last decades, numerous fault-tolerant approaches have been proposed for the wide spectrum of Internet services, including stateful firewalls. Most of these solutions adopt reactive approaches to mask failures by replicating state-changes between replicas. However, reactive replication is a resource consuming task that reduces scalability and performance: the amount of computational and bandwidth resources to propagate state-changes among replicas might be high. On the other hand, more and more commercial off-the-shelf platforms provide integrated hardware error-detection facilities. As a result, some current fault-tolerance research works aim to replace the reactive fault-handling with proactive fault-avoidance. However, pure proactive approaches are risky and they currently face serious limitations. In this work, we propose a hybrid proactive and reactive model that exploits the stateful firewall semantics to increase the overall performance of cluster-based fault-tolerant stateful firewalls. The proposed solution reduces the amount of resources involved in the reactive state-replication by means of Bayesian techniques to perform lazy replication while, at the same time, benefits from proactive fault-tolerance. Preliminary experimental results are also provided.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4724361"},{"name":"Building a requirement fault taxonomy: experiences from a NASA verification and validation research project","snippet":"Fault-based analysis is an early lifecycle approach to improving software quality by preventing and\/or detecting pre-specified classes of faults prior to implementation. It assists in the selection of verification and validation techniques that can be applied in order to reduce risk. This paper presents our methodology for requirements-based fault analysis and its application to National Aeronautics and Space Administration (NASA) projects. The ideas presented are general enough to be applied immediately to the development of any software system. We built a NASA-specific requirement fault taxonomy and processes for tailoring the taxonomy to a class of software projects or to a specific project. We examined requirement faults for six systems, including the International Space Station (ISS), and enhanced the taxonomy and processes. The developed processes, preliminary tailored taxonomies for critical\/catastrophic high-risk (CCHR) systems, preliminary fault occurrence data for the ISS project, and lessons learned are presented and discussed.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1251030"},{"name":"Comparative Study of Fault-Proneness Filtering with PMD","snippet":"Fault-prone module detection is important for assurance of software quality. We have proposed a novel approach for detecting fault-prone modules using spam filtering technique, named Fault-proneness filtering. In order to show the effectiveness of fault-proneness filtering, we conducted comparative study with a static code analysis tool, PMD. In the study, Fault-proneness filtering obtains higher F<sub>1<\/sub> than PMD.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4700354"},{"name":"Modified feedback configuration for sensor fault tolerant control","snippet":"Faults in process control systems can cause undesired reactions and shut-down of a plant, and could be damaging to components, to personnel or the environment. The improvement of reliability, safety and efficiency of the system has become increasingly important. The fundamental purpose of a FTCS scheme is to ensure that faults do not result in system breakdown and although at a lower degree of system performance. The correlative action or prevention measures can be taken to eliminate or minimize the effect of the fault. This paper demonstrates how a feedback control structure can tolerate sensor faults in a temperature control system. The proposed fault-tolerant control design consists of two parts: a nominal performance controller and a model based element to provide sensor fault compensating signals. The nominal controller can have any given structure that satisfies the performance specification, such as a PID controller. When a sensor fault is present, the controller input is augmented to compensate the fault. Results of real time implementation for temperature control are presented to demonstrate the applicability of the proposed FTCS scheme. Sensor faults and disturbance were included to test the proposed design. Deteriorated sensor performance is considered as fault.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4795844"},{"name":"An efficient fault-tolerant scheme for mobile agent execution","snippet":"Fault-tolerance is one of the main problems that must be resolved to improve the adoption of the agent's computing paradigm. In this paper, we develop a pragmatic framework for agent systems fault-tolerance. The developed framework deploys an independent checkpointing strategy with cooperating agent and passive replication to offer a low-cost, application-transparent model for reliable agent-based computing that covers all possible faults that might invalidate reliable agent execution, migration and communication and maintains the exactly-once and non-blocking properties. At the end, we will present some performance results that show the effectiveness of the proposed fault-tolerance scheme","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1627526"},{"name":"Robot fault-tolerance using an embryonic array","snippet":"Fault-tolerance, complex structure management and reconfiguration are seen as valuable characteristics. Embryonic arrays represent one novel approach that takes inspiration from nature to improve upon standard techniques. An existing BAE SYSTEMS RASCALTM robot has been augmented so as to improve the motor control system reliability through two biologically-inspired systems: an embryonic array and an artificial immune system. This paper is concerned with the embryonic array; this is novel in that it supports datapath-wide arithmetic and logic functions. The array is configured to provide an autonomous self-repairing hardware motor controller and is realized using a standard Xilinx Virtex FPGA. As with previous embryonic systems, the logic requirement of the array is greater than that of a conventional FPGA or standard modular-redundancy approach. However, the array offers the advantages of both conventional FPGAs and modular-redundancy techniques. It is a reconfigurable computing platform that provides inherent fault-tolerance through its distributed self-repair mechanism.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1217651"},{"name":"Improvement of Temporal-Replication Mechanism in Mobile Agent System Fault-Tolerant Model","snippet":"Fault-tolerant is one of the most important content of mobile-agent system. After studying the existing mobile-agent system fault-tolerant mechanisms both in domestic and overseas, this paper is concerned with temporal-replication methodology. On the basis of witness agent approach, which was proposed by Michael R.Lyu etc, in this paper a progressed mechanism was introduced. There are two aspects of improvement, the one is to keep address and timestamp of the nodes that agent has traveled; the other is to employ the agent creation node as a fixed backup. This approach can handle server failures, place failures, and failures in message passing. It is capable of detecting and recovering most failure scenarios in mobile agent systems. We describe the design of our fault-tolerant approach to mobile agent systems, and conduct reliability evaluation for our approach by using simulation tools of C-Sim and Matlab. The evaluation results show our approach is a promising technique in achieving mobile agent system reliability.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4425550"},{"name":"Using program analysis to identify and compensate for nondeterminism in fault-tolerant, replicated systems","snippet":"Fault-tolerant replicated applications are typically assumed to be deterministic, in order to ensure reproducible, consistent behavior and state across a distributed system. Real applications often contain nondeterministic features that cannot be eliminated. Through the novel application of program analysis to distributed CORBA applications, we decompose an application into its constituent structures, and discover the kinds of nondeterminism present within the application. We target the instances of nondeterminism that can be compensated for automatically, and highlight to the application programmer those instances of nondeterminism that need to be manually rectified. We demonstrate our approach by compensating for specific forms of nondeterminism and by quantifying the associated performance overheads. The resulting code growth is typically limited to one extra line for every instance of nondeterminism, and the runtime overhead is minimal, compared to a fault-tolerant application with no compensation for nondeterminism.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1353026"},{"name":"Virtual fault simulation of distributed IP-based designs","snippet":"Fault simulation and testability analysis are major concerns in design flows employing intellectual-property (IP) protected virtual components. In this paper we propose a paradigm for the fault simulation of IP-based designs that enables testability analysis without requiring IP disclosure, implemented within the JavaCAD framework for distributed design. As a proof of concept, stuck-at fault simulation has been performed for combinational circuits containing virtual components","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=840023"},{"name":"Model for fault tolerance and policy from RM-ODP expressed in UML\/OCL","snippet":"Fault tolerance (FT) is a topic of major concern in achieving dependable systems, for both real time as well as non real time systems. The paper provides a model of achieving fault tolerance, based on the ISO\/ITU Reference Model for Open Distributed Processing (RM-ODP). This reference model provides a system software engineering methodology for fault tolerance, an object based model of fault tolerance, system requirements for achieving fault tolerance in an open manner, modeling constructs and rules to enable a proper system specification of fault tolerance, and business rules in terms of policies to achieve a well formed system specification. All these aspects are discussed at some depth, but the author primarily focuses on how certain behavior can be specified and achieved in an object based system, the constructs of the Unified Modeling Language (UML) and the Object Constraint Language (OCL)","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=839528"},{"name":"Safety verification of fault tolerant goal-based control programs with estimation uncertainty","snippet":"Fault tolerance and safety verification of control systems that have state variable estimation uncertainty are essential for the success of autonomous robotic systems. A software control architecture called mission data system, developed at the Jet Propulsion Laboratory, uses goal networks as the control program for autonomous systems. Certain types of goal networks can be converted into linear hybrid systems and verified for safety using existing symbolic model checking software. A process for calculating the probability of failure of certain classes of verifiable goal networks due to state estimation uncertainty is presented. A verifiable example task is presented and the failure probability of the control program based on estimation uncertainty is found.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4586461"},{"name":"An Efficient Algorithm To Analyze New Imperfect Fault Coverage Models","snippet":"Fault tolerance has been an essential architectural attribute for achieving high reliability in many critical applications of digital systems. Automatic recovery and reconfiguration mechanisms play a crucial role in implementing fault tolerance because an uncovered fault may lead to a system or subsystem failure even when adequate redundancy exists. In addition, an excessive level of redundancy may even reduce the system reliability. Therefore, an accurate analysis must account for not only the system structure but also the system fault and error handling behavior. The models that capture the fault and error handling behavior are called coverage models. The appropriate coverage modeling approach depends on the type of fault tolerant techniques used. Recent research emphasizes the importance of two new categories of coverage models: Fault Level Coverage (FLC) models and one-on-one level coverage (OLC) models. However, the methods for solving FLC and OLC models are much more limited, primarily because of the complex nature of the dependency introduced by the reconfiguration mechanisms. In this paper, we propose an efficient algorithm for solving FLC and OLC models.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4126388"},{"name":"A Combined Approach for Information Flow Analysis in Fault Tolerant Hardware","snippet":"Fault tolerance in information security devices is difficult to establish due to the large number of possible interactions in the device (e. g. embedded code, boolean logic, electromagnetic interference, etc.) In previous work we examined information flow as a graph problem by composing orthogonal views of the device under analysis. In other work we used fault-tree analysis to reason about information flow as a systemic failure arising from certain configurations (or faults) in either the control logic or data flow 'backbone'. In this paper we combine these approaches by taking advantage of an alternative representation of fault trees as reliability block diagrams.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4276308"},{"name":"Self-Adaptation of Fault Tolerance Requirements Using Contracts","snippet":"Fault tolerance is a constant concern in data centers where servers have to run with a minimal level of failures. Changes on the operating conditions or on server demands, and variations of the systems own failure rate have to be handled in such a way that SLAs are honored and services are not interrupted. We present an approach to handle fault tolerance requirements, based on component replication, which is supported by a context-aware infrastructure and guided by contracts that describe adaptation policies for each application. At run-time the infrastructure autonomically manages the deployment, the monitoring of resources, the maintenance of the fault tolerance requirements described in the contract, and reconfigures the application when necessary, to maintain compliance. An example with an Apache web server and replicated Tomcat servers is used to validate the approach.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5284167"},{"name":"A Framework for Proactive Fault Tolerance","snippet":"Fault tolerance is a major concern to guarantee availability of critical services as well as application execution. Traditional approaches for fault tolerance include checkpoint\/restart or duplication. However it is also possible to anticipate failures and proactively take action before failures occur in order to minimize failure impact on the system and application execution. This document presents a proactive fault tolerance framework. This framework can use different proactive fault tolerance mechanisms, i.e., migration and pause\/un-pause. The framework also allows the implementation of new proactive fault tolerance policies thanks to a modular architecture. A first proactive fault tolerance policy has been implemented and preliminary experimentations have been done based on system-level virtualization and compared with results obtained by simulation.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4529406"},{"name":"A Low-Cost Fault-Tolerant Real, Reactive, and Apparent Power Measurement Technique Using Microprocessor","snippet":"Errors may creep in when measuring power by conventional methods due to the inductance and capacitance of the coils and the induced eddy current in the metal parts of the instruments through the alternating magnetic field of the current coil. Apart from these, if a fault occurs in any of the potential transformer secondary circuits or the potential coil of the measuring equipment, a conventional meter cannot detect it, which results in underregistration. In this paper, a microprocessor-based threephase real, reactive, and apparent power measurement system is developed, which displays the power being fed to a load under both normal and faulty conditions. The microprocessor provides a simple, accurate, reliable, and economical solution to these problems. A framework of the hardware circuitry and the assembly language program for the evaluation of power values is given, and the problems to which attention should be paid to execute the proposed algorithm using the microprocessor are discussed. Illustrative laboratory test results confirm the validity and accurate performance of the proposed method in real-time.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4389141"},{"name":"Assessing Failure of Bridge Construction Using Fuzzy Fault Tree Analysis","snippet":"Estimating exact probabilities of occurrence of bridge failure for the use in the conventional fault tree analysis (FTA) is difficult when fault events are imprecise such as human error. A fuzzy FTA model employing fuzzy sets and possibility theory to tackle this problem is proposed. An example of the collapse of cantilever gantry during construction demonstrates the capability of this approach that can assist safety engineer to better evaluate bridge performance.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4405896"},{"name":"Evanescent microwave sensor scanning for detection of sub-surface defects in wires","snippet":"Evanescent microwave probe (EMP) scanning is used to detect sub-surface defects in copper wire with high dielectric coatings. The primary interest is in winding wire used in the construction of high voltage motors, generators and transformer applications although this technique can be applied to other aspects of stress and flaw detection. Evanescent microwave probes have the unique ability to image subsurface features under poorly conducting or dielectric materials. This nondestructive evaluation technique will allow fast and highly accurate subsurface scans of armature or stator wire in inspecting for various surface anomalies","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=965618"},{"name":"Using loop invariants to fight soft errors in data caches","snippet":"Ever scaling process technology makes embedded systems more vulnerable to soft errors than in the past. One of the generic methods used to fight soft errors is based on duplicating instructions either in the spatial or temporal domain and then comparing the results to see whether they are different. This full duplication based scheme, though effective, is very expensive in terms of performance, power, and memory space. In this paper, we propose an alternate scheme based on loop invariants and present experimental results which show that our approach catches 62% of the errors caught by full duplication, when averaged over all benchmarks tested. In addition, it reduces the execution cycles and memory demand of the full duplication strategy by 80% and 4%, respectively.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1466586"},{"name":"The secret life of bugs: Going past the errors and omissions in software repositories","snippet":"Every bug has a story behind it. The people that discover and resolve it need to coordinate, to get information from documents, tools, or other people, and to navigate through issues of accountability, ownership, and organizational structure. This paper reports on a field study of coordination activities around bug fixing that used a combination of case study research and a survey of software professionals. Results show that the histories of even simple bugs are strongly dependent on social, organizational, and technical knowledge that cannot be solely extracted through automation of electronic repositories, and that such automation provides incomplete and often erroneous accounts of coordination. The paper uses rich bug histories and survey results to identify common bug fixing coordination patterns and to provide implications for tool designers and researchers of coordination in software development.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5070530"},{"name":"Application of a fault injection based dependability assessment process to a commercial safety critical nuclear reactor protection system","snippet":"Existing nuclear power generation facilities are currently seeking to replace obsolete analog Instrumentation and Control (I&C) systems with contemporary digital and processor based systems. However, as new technology is introduced into existing and new plants, it becomes vital to assess the impact of that technology on plant safety. From a regulatory point of view, the introduction or consideration of new digital I&C systems into nuclear power plants raises concerns regarding the possibility that the fielding of these I&C systems may introduce unknown or unanticipated failure modes. In this paper, we present a fault injection based safety assessment methodology that was applied to a commercial safety grade digital Reactor Protection System. Approximately 10,000 fault injections were applied to the system. This paper presents a overview of the research effort, lessons learned, and the results of the endeavor.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5544285"},{"name":"A PIN-Based Dynamic Software Fault Injection System","snippet":"Fault injection plays a critical role in the verification of fault-tolerant mechanism, software testing and dependability benchmarking for computer systems. In this paper, according to the characteristics of software faults, we propose a new fault injection design pattern based on the PIN framework provided by Intel company, and develop a PIN-based dynamic software fault injection system (PDSFIS). Faults can be injected by PDSFIS without the source code of target applications under assessment, nor does the injection process involve interruption or software traps. Experimental assessment results of an Apache Web server obtained by the dependability benchmarking are presented to demonstrate the potentials of PDSFIS.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4709308"},{"name":"Multi-cycle Fault Injections in Error Detecting Implementations of the Advanced Encryption Standard","snippet":"Fault injections can easily break a cryptosystem: hence, many dedicated error detection schemes have been proposed, relying on various forms of redundancy (e.g., temporal redundancy). In this paper, we analyze the error detection coverage of two AES implementations, based on the double-data-rate computation template, with emulated faults of several durations.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4437419"},{"name":"Novel fault localization approach for ATPG \/ scan-fault failures in complex sub-nano FPGA\/ASIC debugging","snippet":"Fault isolation in automated test pattern generation (ATPG) \/ scan-fault has been increasingly challenging in today's advanced integrated circuit (IC) as diagnosis results usually point to extensive faulty nets which can be physically widespread on the die. This work highlights a novel approach in understanding electrical fault data with promising physical failure analysis (PFA) results in FPGA\/ASIC debugging.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5532246"},{"name":"Optimization for Fault Localization in All-Optical Networks","snippet":"Fault localization is a critical issue in all-optical networks. The limited-perimeter vector matching (LVM) protocol is a novel fault-localization protocol proposed for localizing single-link failures in all-optical networks. In this paper, we study the optimization problems in applying the LVM protocol in static all- optical networks. We consider two optimization problems: one is to optimize the traffic distribution so that the fault-localization probability in terms of the number of localized links is maximized, and the other is to optimize the traffic distribution so that the time for localizing a failed link is minimized. We formulate the two problems into an integer linear programming problem, respectively, and use the CPLEX optimization tool to solve the formulated problems. We show that by optimizing the traffic distribution the fault-localization probability can be maximized and the fault-localization time can be minimized. Moreover, a heuristic algorithm is proposed to evaluate the optimization results through simulation experiments.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5153289"},{"name":"Exploiting Quasiperiodicity in Motion Correction of Free-Breathing Myocardial Perfusion MRI","snippet":"Free-breathing image acquisition is desirable in first-pass gadolinium-enhanced magnetic resonance imaging (MRI), but the breathing movements hinder the direct automatic analysis of the myocardial perfusion and qualitative readout by visual tracking. Nonrigid registration can be used to compensate for these movements but needs to deal with local contrast and intensity changes with time. We propose an automatic registration scheme that exploits the quasiperiodicity of free breathing to decouple movement from intensity change. First, we identify and register a subset of the images corresponding to the same phase of the breathing cycle. This registration step deals with small differences caused by movement but maintains the full range of intensity change. The remaining images are then registered to synthetic references that are created as a linear combination of images belonging to the already registered subset. Because of the quasiperiodic respiratory movement, the subset images are distributed evenly over time and, therefore, the synthetic references exhibit intensities similar to their corresponding unregistered images. Thus, this second registration step needs to account only for the movement. Validation experiments were performed on data obtained from six patients, three slices per patient, and the automatically obtained perfusion profiles were compared with profiles obtained by manually segmenting the myocardium. The results show that our automatic approach is well suited to compensate for the free-breathing movement and that it achieves a significant improvement in the average Pearson correlation coefficient between manually and automatically obtained perfusion profiles before ( <b>0.87<\/b> <b>0.18<\/b>) and after (<b>0.96<\/b> <b>0.09<\/b>) registration.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5458069"},{"name":"Full coverage location of logic resource faults in A SOC co-verification technology based FPGA functional test environment","snippet":"Full coverage location of logic resource faults is vital for FPGA design and fabrication, rather than only detecting whether there are faults or not. Taking advantage of flexibility and observability of software in conjunction with high-speed simulation of hardware, SOC co-verification technology based in-house FPGA functional test environment embedded with an in-house computerized tool, ConPlacement, can locate logic resources automatically, exhaustively and repeatedly. The approach to implement full coverage location of configurable logic block (CLB) faults by the FPGA functional test environment is presented in the paper. Experimental results of XC4010E demonstrate that full coverage location of logic resource faults as well as multi-faults position can be realized.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5351202"},{"name":"Software-Based Algorithm for Modeling and Correction of Gradient Nonlinearity Distortions in Magnetic Resonance Imaging","snippet":"Functional radiosurgery is a noninvasive stereotactic technique that requires magnetic resonance image (MRI) sets with high spatial resolution. Gradient nonlinearities introduce geometric distortions that compromise the accuracy of MRI-based stereotactic localization. We present a gradient nonlinearity correction method based on a cubic phantom MRI data set. The approach utilizes a sum of spherical harmonics to model the geometrically warped planes of the cube and applies the model to correct arbitrary image sets acquired with the same scanner. In this paper, we give a detailed description of the Matlab distortion correction program, report on its performance in stereotactic localization of phantom markers, and discuss the possibility to accelerate the code using general-purpose computing on graphics processing units (GPGPU) techniques.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5233196"},{"name":"RTL-based functional test generation for high defects coverage in digital SOCs","snippet":"Functional test is long viewed as unfitted for production test. The purpose of this contribution is to propose a RTL-based test generation methodology which can be rewardingly used both for design validation and to enhance the test effectiveness of classic, gate-level test generation. Hence, a RTL-based defect-oriented test generation methodology is proposed, for which a high defects coverage (DC) and a relatively short test sequence can be derived, thus allowing low-energy operation in test mode. The test effectiveness, regarding DC, is shown to be weakly dependent on the structural implementation of the behavioral description. The usefulness of the methodology is ascertained using the VeriDOS simulation environment and the CMUDSP ITC'99 benchmark circuit","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=873785"},{"name":"Design Fault Directed Test Generation for Microprocessor Validation","snippet":"Functional validation of modern microprocessors is an important and complex problem. One of the problems in functional validation is the generation of test cases that has higher potential to find faults in the design. We propose a model based test generation framework that generates tests for design fault classes inspired from software validation. There are two main contributions in this paper. Firstly, we propose a microprocessor modeling and test generation framework that generates test suites to satisfy modified condition decision coverage (MCDC), a structural coverage metric that detects most of the classified design faults as well as the remaining faults not covered by MCDC. Secondly, we show that there exists good correlation between types of design faults proposed by software validation and the errors\/bugs reported in case studies on microprocessor validation. We demonstrate the framework by modeling and generating tests for the microarchitecture of VESPA, a 32-bit microprocessor. In the results section, we show that the tests generated using our framework's coverage directed approach detects the fault classes with 100% coverage, when compared to model-random test generation","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4211892"},{"name":"Further Results on Prony Approximation for Evaluation of the Average Probability of Error","snippet":"Further results on a Prony approximation for efficient evaluation of the average probability of error over fading channels are presented. A generic definition of the average probability of error for a communication system is given. The Prony approximation method is shown to be an extension of the Chernoff bound and the MGF method. An improved algorithm for obtaining the parameters of the Prony approximation is developed. New, simple and highly accurate Prony approximations for the conditional probability of bit-error of M-ary modulations assuming parameter quantization to only 3 significant figures are presented. Furthermore, the rank and determinant design criteria for space-time block codes in Gaussian channels are shown to be valid for the exact pairwise error probability. Numerical results indicate that the relative approximation error using the Prony approximation method is less than 5% of the exact average probability of error for all practical values of the signal-to-noise ratio.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4533298"},{"name":"Enhancing Fault Tolerance And Reliability In GAIAOS Through Structured Overlay Network","snippet":"GAIAOS event manager is a distributed event service, based on CORBA event service with a centralized entry point, resulting in limited fault resilience and scalability. In this paper, we have proposed a decentralized event service for GAIAOS through the use of DHT based structured overlay network to overcome these problems. The proposed architecture provides a completely distributed event communication mechanism without any centralized entry point. Incorporation of the structured overlay network in GAIAOS results in higher degree of fault resilience and scalability","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4115537"},{"name":"GMPLS fault management and impact on service resilience differentiation","snippet":"Generalized Multi-Protocol Label Switching (GMPLS) is currently under standardization. It basically reuses the MPLS control plane (IP routing and signaling) for various technologies such as fiber switching, DWDM, SONET, and packet MPLS. Since GMPLS runs in core networks, fault management is of major concern. However, fast fault recovery and backup capacity assignments are very expensive and not all customers need this or are willing to pay for it. Therefore, we propose in this paper to use several protection and bandwidth-sharing schemes on the same network in order to provide differentiated services in the resilience space. This means an operator can offer and provide several customized services. The service management system implementing, the schemes is built on top of a GMPLS network management system developed in our lab.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1194218"},{"name":"A Vitual Instrument for Sensors Nonlinear Errors Calibration","snippet":"Generally, the input-output characteristic of many sensors are nonlinear. To improve the measure precision, the sensors nonlinear errors need to be calibrated. At present many calibration methods have been researched in term of theoretical analysis but no effective software be developed from the points of engineering actual applications. In this paper, a virtual instrument for sensors nonlinear errors calibration, based on Labview6.1, is presented. It consists of three kinds of nonlinear errors calibration methods: linear fit calibration, polynomial fit calibration and artificial neural network calibration. Tests and performance analysis of the VI were performed with the data of fibre-optic displacement sensor","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1604544"},{"name":"PTGC: A parallel triangular geometric correction algorithm for remote sensing images","snippet":"Geometric correction with the compute intensive characteristic is the critical step in the processing of remote sensing image. As is known, the correction based on polynomial transform will lead to significant computing errors when applied to the region with fluctuant terrain. In this paper we present a novel and more precise parallel triangular geometric correction algorithm to deal with the image of the fluctuant terrain with large amount of data. In the algorithm, dynamic cut-off points are adopted to achieve good load balance, and the triangle scan line structure is utilized to preserve the index of the triangle, which is hashed by the pixel. Hence, the redundant calculation is avoided. The algorithm is equipped with good scalability, and is able to handle arbitrary geometric distortion. The accuracy of our approach is approximate 80% higher than the polynomial geometric correction algorithm with similar computational overhead.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5293445"},{"name":"Fusing hard and soft computing for fault management in telecommunications systems","snippet":"Global telecommunication systems are at the heart of the Internet revolution. To support Internet traffic they have built-in redundancy to ensure robustness and quality of service. This requires complex fault management. The traditional hard approach is to reduce the number of alarm events (symptoms) presented to the operating engineer through monitoring, filtering and masking. The goal of the soft approach is to automate the analysis fully so that the underlying fault is determined from the evidence available and presented to the engineer. This paper describes progress toward automated fault identification through a fusion between these soft and hard computing approaches.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1039194"},{"name":"Optimization of the Alberty and Hespelt carrier frequency error detection algorithm","snippet":"Following a literature survey, the Alberty and Hespelt frequency error detection (FED) algorithm was chosen for software implementation in an all-digital demodulator. For the correct operation of this algorithm, it is desired to have symmetric bandpass filters. In this paper a simple method will be presented to ensure that the bandpass filters are fully symmetrical. Simulations show that even with symmetric bandpass filters, there is a substantial amount of tracking jitter. To overcome this problem, a smart filter has been implemented to ensure that the tracking performance is almost jitter free without increasing the error acquisition time","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1557323"},{"name":"Spherical Near-Field Antenna Measurements: A Review of Correction Techniques","snippet":"Following an introductory review of spherical near-field scanning measurements, with emphasis on the general applicability of the technique, we present a survey of the various methods to improve measurement accuracy by correcting the acquired data before performing the transform and by special processing of the resulting data following the transform. A post-processing technique recently receiving additional attention is the IsoFiltertrade technique that assists in suppressing extraneous stray signals due to scattering from antenna range apparatus.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4459042"},{"name":"Joint Source-Channel Rate-Distortion Optimization for H.264 Video Coding Over Error-Prone Networks","snippet":"For a typical video distribution system, the video contents are first compressed and then stored in the local storage or transmitted to the end users through networks. While the compressed videos are transmitted through error-prone networks, error robustness becomes an important issue. In the past years, a number of rate-distortion (R-D) optimized coding mode selection schemes have been proposed for error-resilient video coding, including a recursive optimal per-pixel estimate (ROPE) method. However, the ROPE-related approaches assume integer-pixel motion-compensated prediction rather than subpixel prediction, whose extension to H.264 is not straightforward. Alternatively, an error-robust R-D optimization (ER-RDO) method has been included in H.264 test model, in which the estimate of pixel distortion is derived by simulating decoding process multiple times in the encoder. Obviously, the computing complexity is very high. To address this problem, we propose a new end-to-end distortion model for R-D optimized coding mode selection, in which the overall distortion is taken as the sum of several separable distortion items. Thus, it can suppress the approximation errors caused by pixel averaging operations such as subpixel prediction. Based on the proposed end-to-end distortion model, a new Lagrange multiplier is derived for R-D optimized coding mode selection in packet-loss environment by taking into account of the network conditions. The rate control and complexity issues are also discussed in this paper","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4130384"},{"name":"Fault-based attack of RSA authentication","snippet":"For any computing system to be secure, both hardware and software have to be trusted. If the hardware layer in a secure system is compromised, not only it would be possible to extract secret information about the software, but it would also be extremely hard for the software to detect that an attack is underway. In this work we detail a complete end-to-end fault-attack on a microprocessor system and practically demonstrate how hardware vulnerabilities can be exploited to target secure systems. We developed a theoretical attack to the RSA signature algorithm, and we realized it in practice against an FPGA implementation of the system under attack. To perpetrate the attack, we inject transient faults in the target machine by regulating the voltage supply of the system. Thus, our attack does not require access to the victim system's internal components, but simply proximity to it. The paper makes three important contributions: first, we develop a systematic fault-based attack on the modular exponentiation algorithm for RSA. Second, we expose and exploit a severe flaw on the implementation of the RSA signature algorithm on OpenSSL, a widely used package for SSL encryption and authentication. Third, we report on the first physical demonstration of a fault-based security attack of a complete microprocessor system running unmodified production software: we attack the original OpenSSL authentication library running on a SPARC Linux system implemented on FPGA, and extract the system's 1024-bit RSA private key in approximately 100 hours.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5456933"},{"name":"A pattern defect inspection method by parallel grayscale image comparison without precise image alignment","snippet":"For automatic visual inspection of patterns on printed wiring boards and\/or patterned wafers, this paper presents a new defect detection method for grayscale images without precise image alignment. Most of the conventional visual inspection algorithms based on grayscale reference comparison require precise image alignment with precision of subpixel or within 1 pixel; however, it is difficult to succeed in the precise image alignment in every image. While a defect inspection method without precise image alignment has been previously proposed for binary images, the expansion to grayscale images we discuss is indispensable for detecting more minute defects. We propose dynamic tolerance control based on grayscale morphology to reduce false defects on pattern edges, and use gray dilation operation so that a weakness of the original method for binary images, an inability to detect the absence of minute patterns, is overcome. Theoretical analysis and experimental results show that the proposed method is capable of detecting subpixel-sized defects, and has practical detection performance.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1185315"},{"name":"Performance evaluation of a fault-tolerant mechanism based on replicated distributed objects for CORBA","snippet":"For future applications, it is important to develop systems using small objects. Such systems must be able to provide uninterrupted services even when some small objects stop. Replicating such objects is one way to do this. However, introducing some type of redundancy into systems generally adds some overhead. Our proposed model reduces this overhead. It is implemented with multi-threaded execution for applications in actual systems. We measured the performance of an implementation for applications connected to databases. The results show that the overhead for ordinary execution and the time required to switch the replicas are both acceptably small. This technique will therefore play an important role in future systems","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=922823"},{"name":"Geometric correction of scanned topographic maps using capable input information","snippet":"For making digital maps by using the raster-vector conversion of printed binary topographic maps, one of the problems is how to correct geometric distortion that originates in the habit of individual scanner. To use innumerable resources of printed binary topographic maps effectively, we propose an interactive interface and geometric correction algorithms, which uses peculiar coordinates information in individual map. The examples prove that the interactive interface using a cross cursor is useful and efficient for getting accurate input coordinates, and the proposed correction algorithm demonstrates high accuracy of corrected coordinates.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1219659"},{"name":"Random double-bit error-correcting decomposable codes","snippet":"For odd m, a family of decomposable [3(2<sup>m<\/sup>-1), 3(2<sup>m<\/sup>-1)-3m, 5] codes, based on |a+x|b+x|a+b+x| construction, are proposed. A simple high-speed decoding algorithm for these codes suitable for implementation in combinational circuits is described","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=907536"},{"name":"Modeling of the induction machine for the diagnosis of rotor defects. Part. II. Simulation and experimental results","snippet":"For part I see ibid., p.Z001665-Z001672. Two softwares are coupled for the parameter calculation and the differential integration of the dynamic model of the squirrel cage rotor induction machine, in part. II of this paper. From theoretical study of the induction machine by an approach of multiple coupled circuits, we worked out the necessary various blocks to simulation. The calculation of machine inductances (with and without rotor defects) is carried out by the tools of software MATLAB before the beginning of simulation under Software SIMULINK. The machine parameters are then computed by using the concepts of the construction of electric machines. Simulation and experimental results are presented to confirm the validity of proposed model.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1569147"},{"name":"CT Based Attenuation Correction for PET Brain Imaging","snippet":"For research and clinical PET brain studies performed on PET\/CT systems, the CT image is often of little benefit beyond attenuation correction. The goal of this work is both to investigate the quantitative accuracy of CT-based attenuation correction (CTAC) for PET brain studies and to determine the lowest-dose protocol that performs adequately. Measurements were performed on a GE Discovery ST PET\/CT scanner using the GE kVp-dependent CTAC algorithm. The effects of pitch on CT images were investigated by acquiring CT images of a Defrise disk phantom. The effects of pitch and X-ray tube voltage and current were investigated using a human skull phantom encased in plastic filled with F-18 radioactivity. This phantom was imaged and CT-based attenuation correction factors (CTACF) were created for several permutations of pitch (0.562:1, 0.938:1, 1.375:1, 1.75:1), voltage (80, 100, 120, 140 kVp) and current (10, 100mA). Emission images were acquired and reconstructed using the 3-D reprojection algorithm with the various CTACFs. The measured mean activity concentration is independent of pitch, kVp, and mA and accurate on an absolute scale to ~5%. Anatomically sized regional differences in the brain region indicate that tube voltages less than 100 kVp may not perform adequately (10% of values have a discrepancy greater than 5%). Results indicate that the higher pitch, lower current, and tube voltages down to 100 kVp perform equivalently to higher-dose configurations.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4179758"},{"name":"A novel fault-tolerant control method of wireless sensor networks","snippet":"For sensor distribution of wireless sensor networks, a neural network fault-tolerant control strategy is proposed: first the control law in various fault conditions is designed using the method of system reconstruction, and then the control law features is learned based a neural network. After learning, the neural network as a controller system can be controlled. Simulation results show that: neural network controller can replace the original controller system, and for an unknown fault in the system, it can have the same fault tolerant.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5543713"},{"name":"A length compensation method to eliminate the varying length defect in one dimensional fisheye views","snippet":"Graphical fisheye view is an effective technique for visualizing and navigating large information structures. However, there are still technical difficulties that hinder its broader applications. One of the prominent problems is the varying length effect seen in most fisheye views. The varying length effect refers to a phenomenon that the length (or height) of a fisheye component is not fixed, but it varies with the location of the focal point. This effect may bring some disadvantages that reduce the usability of a fisheye component. To overcome this defect, sporadical solutions have been seen for specific implementations, but a systematic method has not been seen yet. This paper proposes a length compensation method to eliminate the varying length defect for one dimensional fisheye components. The method provides solutions for handling both discrete and continuous magnifications respectively. The mathematical foundation of the method is given, and the implemented prototype proves that it is effective.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5512457"},{"name":"Gravity measurement from moving platform by second order Kalman Filter and position and velocity corrections","snippet":"Gravity measurement is an effective tool for oil and gas exploring. It is particularly important for gravity observation from moving platforms, especially for remote areas and offshore fields by aircraft, boat, ship, submarine, satellite, vehicle, etc. The measurement is complicated by difficulty to discern gravity from platform accelerations, nonlinearity, drift and dynamic angular movement. Presented solution is second order Kalman Filter, a recursive estimator that is applied to highly nonlinear measurement problems. The filter optimally combines data of three-axis gyros, accelerometers and platform position and velocity signals to provide accurate attitude and gravity measurement. Extensive simulations verified accuracy and robustness of proposed method for measurement form different vehicles in various dynamic environments.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4957130"},{"name":"Design of Tone-Dependent Color-Error Diffusion Halftoning Systems","snippet":"Grayscale error diffusion introduces nonlinear distortion (directional artifacts and false textures), linear distortion (sharpening), and additive noise. Tone-dependent error diffusion (TDED) reduces these artifacts by controlling the diffusion of quantization errors based on the input graylevel. We present an extension of TDED to color. In color-error diffusion, which color to render becomes a major concern in addition to finding optimal dot patterns. We propose a visually meaningful scheme to train input-level (or tone-) dependent color-error filters. Our design approach employs a Neugebauer printer model and a color human visual system model that takes into account spatial considerations in color reproduction. The resulting halftones overcome several traditional error-diffusion artifacts and achieve significantly greater accuracy in color rendition","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4032821"},{"name":"A fault-tolerance mechanism in grid","snippet":"Grid appears as an effective technology coupling geographically distributed resources for solving large-scale problems in the wide area network. Fault tolerance in grid system is a significant and complex issue to secure a stable and reliable performance. Until now, various techniques exist for detecting and correcting faults in distributed computing systems. Unfortunately, few energy focus on fault-tolerance in grid environment, especially with the emergence of OGSA. A new fault-tolerant mechanism is needed to detect and recover service faults and nodes crash. Based on our previous work on Java threads state capturing and existing mobile agent techniques, we put forward a fault-tolerant mechanism providing effective fault-handling and recovering methods.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1300379"},{"name":"Managing Faults for Distributed Workflows over Grids","snippet":"Grid applications composed of multiple, distributed jobs are common areas for applying Web-scale workflows. Workflows over grid infrastructures are inherently complicated due to the need to both functionally assure the entire process and coordinate the underlying tasks. Often, these applications are long-running, and fault tolerance becomes a significant concern. Transparency is a vital aspect to understanding fault tolerance in these environments.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5427403"},{"name":"DDGrid: A Grid Computing Environment with Massive Concurrency and Fault-Tolerance Support","snippet":"Grid Computing is an effective computing paradigm widely used in solving complex problems. There are a variety of existing grid middleware systems which support operation of grid infrastructures, including CNGrid GOS, EGEE gLite, Globus Toolkit, and OSG Condor etc. These grid infrastructures focus on encapsulating underlying computing and storage resources and providing necessary basic services such as batch job service, information service, scheduling service, and cross-domain security, etc. Some other features such as fault-tolerance, massive concurrency support are vital to the success of real applications, especially complex and long running applications. These features have not been the focus point of the current grid systems. DDGrid, a key project supported by CNGrid (China National Grid), is aiming at establishing a grid computing environment that can utilize computing resources scattered over the Internet to carry out virtual-screening operations which requires computing power that a single institute or company can't afford. In our design and implementation of DDGrid, we propose a master\/worker mode which effectively utilizes computing resources that the underlying grid infrastructure provides and tries to provide additional features of fault-tolerance and massive concurrency support that are essential to the real applications.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4662836"},{"name":"Fault tolerant mechanism in grid based on Backup Node","snippet":"Grid is a very efficient technology to performing heavy processing with distributed resources. These resources at geographically distributed widely and without central control unit. One of the important challenges in grid is fault tolerant, because the grid resources in this environment are not reliable. And to avoid duplication with processing done by a resource, the Check Pointing mechanism has been proposed. The state of application at particular point of time can be store and in case of failure the status information can restore on new node and computation can continue. Check Pointe storage location is a major challenge in the fault tolerant techniques. In this paper we presented a new method by using a backup node. Adding a component to GRAM to improve efficiency and fault tolerant. With clear Backup Node the Check point information can save on it. This method increases the efficiency and system reliability.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5689756"},{"name":"Achieving Fault Tolerance on Grids with the CPPC Framework and the GridWay Metascheduler","snippet":"Grids have brought a significant increase in the number of available resources that can be provided to applications. In the last decade, an important effort has been made to develop middleware that provides grids with functionalities related to application execution. However, support for fault-tolerant executions is either lacking or limited. This paper presents an experience to endow with fault tolerance support parallel executions on grids through the integration of CPPC, a check pointing tool for parallel applications, and Grid Way, a well-known met scheduler provided with the Globus Toolkit. Since both tools are not immediately compatible, a new architecture, called CPPC-GW, has been designed and implemented to allow for the transparent execution of CPPC applications through Grid Way. The performance of the solution has been evaluated using the NAS Parallel Benchmarks. Detailed experimental results show the low overhead of the approach.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5644958"},{"name":"A broadband network fault distribution model","snippet":"Growth of the telecommunications market brings more and more types of broadband services, and also the total amount of users who use broadband services is growing. All of this leads to increasing number of user interference, which may be caused by various reasons. Among the most common causes of errors may be faults in the access network, the failures of the customer equipment, errors in the core network, errors in the access devices, etc. For the telecom operators it is very important to well manage the removal of those errors, because the quality of customer services depends on it. This article deals with the exploration of the most common error places and describes time distribution appearing of these errors. As part of this work was created the fault generator, which tries to truly predict the appearance of user interferences. Generator is modeled by using the method of Fourier series and a quantile function.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5306881"},{"name":"Temporal error concealment algorithm for H.264\/AVC using omnidirectional motion similarity","snippet":"H.264\/AVC is the newest one among several video compression standards. The main goals of H.264\/AVC are to achieve efficient compression performance and a network friendly video coding. However, if an error occurs when transmitting compressed video, error concealment is needed to prevent error propagation and to improve the video quality. In this paper, we propose the temporal error concealment algorithm which provides high performance for H.264\/AVC. The proposed algorithm uses the property that the motion vectors (MVs) between the error macroblock (MB) and the neighboring MB have high similarity to select a group of candidate MVs, when an error occurs in the inter-coded frame. Next, weighted overlapped boundary matching algorithm using the credibility of information selects the best candidate MV among a group of candidate MVs. The experimental results show that the proposed algorithm improves PSNR up to 3.02 dB compared with the boundary matching algorithm (BMA).","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5586725"},{"name":"Physical defect modeling for fault insertion in system reliability test","snippet":"Hardware fault-insertion test (FIT) is a promising method for system reliability test and diagnosis coverage measurement. It improves the speed of releasing a quality diagnostic program before manufacturing and provides feedbacks of fault tolerance of a very complicated large system. Certain level insufficient fault tolerance can be fixed in the current system but others may require ASIC or overall system architectural modifications. The FIT is achieved by introducing an artificial fault (defect modeling) at the pin level of a module to mimic any physical defect behavior within the module, such as SEU (single event upset) or escaped delay defect. We present a hardware architectural solution for pin fault insertion. We also present a simulation framework and optimization techniques for a subset of module pin selection for FIT, such that desired coverage are obtained under the constraints of limited FIT pins due to the costs of the associated implementation. Experimental results are presented for selected ISCAS and OpenCore benchmarks, as well as for an industrial circuit.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5355715"},{"name":"Clustering Intelligent Sensor Nodes for Distributed Fault Detection & Diagnosis","snippet":"Having studied distributed process monitoring using intelligent nodes in a cluster, we now extend the study to include using multiple clusters of nodes to monitor multiple process units interconnected to form a process plant. Our case study is that of a reaction process consisting of two CSTRs and auxiliary equipments such as tanks, heat exchangers etc. The study aims to explore the possibility of the new framework of distributed (and collaborative) process monitoring, in which fault detection and diagnosis is performed at the physical sensor and sensor cluster level. The resulting framework should be a better advisory tool for situation analysis and fault diagnosis by plant operator; while potential extension includes prognosis and the inclusion in control and fault recovery.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4053565"},{"name":"Fault Tolerant ICAP Controller for High-Reliable Internal Scrubbing","snippet":"High reliable reconfigurable applications today require system platforms that can easily and quickly detect and correct single event upsets. This capability, however, can be costly for FPGAs. This paper demonstrates a technique for detecting and repairing SEUs within the configuration memory of a Xilinx Virtex-4 FPGA using the ICAP interface. The internal configuration access port (ICAP) provides a port internal to the FPGA for configuring the FPGA device. An application note demonstrates how this port can be used for both error injection and scrubbing (L. Jones, 2007). We have extended this work to create a fault tolerant ICAP scrubber by triplicating the internal ICAP circuit using TMR and block memory scrubbing. This paper will describe the costs, benefits, and reliability of this fault-tolerant ICAP controller.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4526471"},{"name":"Hardware\/software solution for high precision defect correction in digital image sensors","snippet":"High resolution image sensors are now standard in imaging devices such as mobile phones with camera functionality. Resolution improvement in very small sensors is obtained by decreasing the pixel size but, in CMOS sensors, the likelihood of defective pixels also augments. Hence, sophisticated processing is necessary for achieving high quality images despite of noise and defects. This paper presents a hardware\/software solution for high precision correction of defective pixels in an image sensor. The method maintains an always up-to-date map of the defective pixels and also allows detection of new defects as they show up during the lifetime of the sensor. The reliability of the map is assured by tracking the history of pixels defectiveness. The map is updated automatically and in real time without user intervention.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4559487"},{"name":"Geometric Correction of High Resolution Satellite Imagery and its Residual Analysis","snippet":"High resolution satellite images are prone to geometric distortions. To correct these, the process of geometric correction becomes vital. Only knowledge of satellite altitude, attitude, position and the information of the digital elevation model (DEM) will not be adequate for the geometric correction requirements. Therefore the authors designed an algorithm for removal of geometric distortions in satellite imagery. In that a new method of geo-referencing called pixel projection method was applied along with selection of precise ground control points (GCPs). In pixel projection method vertices of remotely sensed image is geo-located based on ancillary data. For precision of GCP least square method was used to cater for instrument bias. GCPs were selected from Google Earth's software. Though with that approach precise geo-referencing of satellite imagery was achieved and a level-1 image was successfully converted to level-3 geometrically corrected image. In this paper the authors carried out residual analysis of our new proposed method. In first step an image to image matching was performed and their MSE (mean square error) was calculated. In second step 8 points in the original image and geo-referenced images were identified and their MSE was calculated. It is observed that with new approach of geo-referencing more precise geo-referencing has been done and image is found to be accurately geometrically corrected","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4106430"},{"name":"Improvement of bit error rate using channel interleaving for channel binding WLAN prototype","snippet":"High speed wireless LAN prototype of 324 Mbit\/sec has been developed. Six channel binding of 802.11 a signal on frequency domain was employed for increasing PHY data rate. Frame aggregation was employed to improve MAC-SAP throughput. Individual adaptive data rate setting on each channel, so called adaptive dynamic channel assign was implemented in order to achieve maximum MAC-SAP throughput at any channel condition. In this paper, multiple channel interleaving over all frequency channels for randomization of burst error was presented. Interleaving can be carried out over all channels or inside individual channel. Bit and packet error rate were measured using implemented WLAN equipements with\/without multiple channel interleaving. It was confirmed that improvement using multiple channel interleaving were measured to be 2 dB.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4699826"},{"name":"High-impedance fault detection using discrete wavelet transform and frequency range and RMS conversion","snippet":"High-impedance faults (HIFs) are faults which are difficult to detect by overcurrent protection relays. Various pattern recognition techniques have been suggested, including the use of wavelet transform . However this method cannot indicate the physical properties of output coefficients using the wavelet transform. We propose to use the Discrete Wavelet Transform (DWT) as well as frequency range and rms conversion to apply a pattern recognition based detection algorithm for electric distribution high impedance fault detection. The aim is to recognize the converted rms voltage and current values caused by arcs usually associated with HIF. The analysis using discrete wavelet transform (DWT) with the conversion yields measurement voltages and currents which are fed to a classifier for pattern recognition. The classifier is based on the algorithm using nearest neighbor rule approach. It is proposed that this method can function as a decision support software package for HIF identification which could be installed in an alarm system.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1375120"},{"name":"Simple correction of chemical shift changes in magnetic resonance spectroscopy quantitation","snippet":"High-resolution magic angle spinning (HRMAS) <sup>1<\/sup>H spectroscopy is playing an increasingly important role for diagnosis. This technique enables setting up metabolite profiles of ex vivo pathological and healthy tissue. Automatic quantitation of HRMAS signals provides reliable reference profiles to monitor diseases and pharmaceutical follow-up. Nevertheless, for several metabolites chemical shifts may slightly differ according to the micro-environment in the tissue or cells, in particular its pH. This hampers accurate estimation of the metabolite concentrations mainly when using quantitation algorithms based on a metabolite basis-set. In this work, we propose a user-friendly way to circumvent this problem based on stretching of the metabolite basis-set signals and maximization of the correlation between the HRMAS and basis-set spectra prior to quantitation.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5548476"},{"name":"Rotor cage fault diagnosis in induction motors based on spectral analysis of current Hilbert modulus","snippet":"Hilbert transformation is an ideal phase shifting tool in data signal processing. Being Hilbert transformed, the conjugated one of a signal is obtained. The Hilbert modulus is defined as the square of a signal and its conjugation. This work presents a method by which rotor faults of squirrel cage induction motors, such as broken rotor bars and eccentricity, can be diagnosed. The method is based on the spectral analysis of the stator current Hilbert Modulus of the induction motors. Theoretical analysis and experimental results demonstrate that has the same rotor fault detecting ability as the extended Park' vector approach. The vital advantage of the former is the smaller hardware and software spending compared with the existing ones.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1373123"},{"name":"Coordinated application of multiple description scalar quantization and error concealment for error-resilient MPEG video streaming","snippet":"Historically, multiple description coding (MDC) and postprocessing error concealment (ECN) algorithms have evolved separately. In this paper, we propose a coordinated application of multiple description scalar quantizers (MDSQ) and ECN, where the smoothness of the video signal helps to compensate for the loss of descriptions. In particular, we perform a reconstruction that is consistent with the data received at the decoder. When only a single description is available, the video is reconstructed in such a way that: 1) if we were to regenerate two descriptions (from the reconstructed video), one of them would be equivalent to the received description and 2) the reconstructed video is spatiotemporally smooth. Experimental results with several video sequences demonstrated a peak signal-to-noise ratio (PSNR) improvement of 0.9-2.8 dB for intracoded frames. The PSNR improvements for intercoded frames were negligible. However, for both cases, the visual improvements were much more striking than what the PSNR improvement suggested.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1413266"},{"name":"Fault tolerant analysis of multi-agent manufacturing systems based Petri nets","snippet":"Holonic manufacturing system (HMS) is a paradigm based on multi-agent system theory that allows fast reconfiguration of available resources to cope with changing product demands in highly uncertain manufacturing environment. The advantage and flexibility offered by HMS poses new challenges and complexities in modeling and design of production control systems. For example, HMS is largely based on multi-agent system (MAS). Contract net protocol is a well-known negotiation and task distribution mechanism for MAS. However, production processes cannot be modeled with contract net protocol. Instead, most existing literatures model production processes based on Petri net theory. The lack of integration among different modeling tools makes it deficient to apply existing tools to model, planning and control HMS. A promising solution to model HMS is to combine the flexibility and robustness of multi-agent theory with the modeling and analytical power of Petri net. This paper focuses on development of a framework to model HMS by extending the contract net protocol with timed Petri net model. The main results include: (1) a nominal collaborative Petri net (CPN) agent model for HMS. (2) Liveness conditions for (CPN) and (3) a resource unavailability model to capture the effects of resource failures. (4) Fault tolerant conditions to test whether a certain type of resource failures are allowed based on Petri net agent model of HMS and (5) a collaborative algorithm to test the feasibility of a solution.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1571902"},{"name":"Haloperidol Impairs Learning and Error-related Negativity in Humans","snippet":"Humans are able to monitor their actions for behavioral conflicts and performance errors. Growing evidence suggests that the error-related negativity (ERN) of the event-related cortical brain potential (ERP) may index the functioning of this response monitoring system and that the ERN may depend on dopaminergic mechanisms. We examined the role of dopamine in ERN and behavioral indices of learning by administering either 3 mg of the dopamine antagonist (DA) haloperidol (<italic>n<\/italic> = 17); 25 mg of diphenhydramine (<italic>n<\/italic> = 16), which has a similar CNS profile but without DA properties; or placebo (<italic>n<\/italic> = 18) in a randomized, double-blind manner to healthy volunteers. Three hours after drug administration, participants performed a go\/no-go Continuous Performance Task, the Eriksen Flanker Task, and a learning-dependent Time Estimation Task. Haloperidol significantly attenuated ERN amplitudes recorded during the flanker task, impaired learning of time intervals, and tended to cause more errors of commission, compared to placebo, which did not significantly differ from diphenhydramine. Drugs had no significant effects on the stimulus-locked P1 and N2 ERPs or on behavioral response latencies, but tended to affect post-error reaction time (RT) latencies in opposite ways (haloperidol decreased and diphenhydramine increased RTs). These findings support the hypothesis that the DA system is involved in learning and the generation of the ERN.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6788520"},{"name":"Topological Correction of Hypertextured Implicit Surfaces for Ray Casting","snippet":"Hypertextures are a useful modelling tool in that they can add three-dimensional detail to the surface of otherwise smooth objects. Hypertextures can be rendered as implicit surfaces, resulting in objects with a complex but well defined boundary. However, representing a hypertexture as an implicit surface often results in many small parts being detached from the main surface, turning an object into a disconnected set. Depending on the context, this can detract from the realism in a scene where one usually does not expect a solid object to have clouds of smaller objects floating around it. We present a topology correction technique, integrated in a ray casting algorithm for hypertextured implicit surfaces, that detects and removes all the surface components that have become disconnected from the main surface. Our method works with implicit surfaces that are C<sup>2<\/sup> continuous and uses Morse theory to find the critical points of the surface. The method follows the separatrix lines joining the critical points to isolate disconnected components.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4273373"},{"name":"Improving the performance of hypervisor-based fault tolerance","snippet":"Hypervisor-based fault tolerance (HBFT), a checkpoint-recovery mechanism, is an emerging approach to sustaining mission-critical applications. Based on virtualization technology, HBFT provides an economic and transparent solution. However, the advantages currently come at the cost of substantial overhead during failure-free, especially for memory intensive applications. This paper presents an in-depth examination of HBFT and options to improve its performance. Based on the behavior of memory accesses among checkpointing epochs, we introduce two optimizations, read fault reduction and write fault prediction, for the memory tracking mechanism. These two optimizations improve the mechanism by 31.1% and 21.4% respectively for some application. Then, we present software-superpage which efficiently maps large memory regions between virtual machines (VM). By the above optimizations, HBFT is improved by a factor of 1.4 to 2.2 and it achieves a performance which is about 60% of that of the native VM.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5470357"},{"name":"Image mosaic method based on the image geometric correction for traffic accident scene","snippet":"Image mosaic is one of important technologies for image processing. It is normally used to make up a seamless and high resolution image. There are some algorithms that deal with the image mosaic. But the most make simply two or more images seamlessly form a large image for a holographic scenic display. The post-processing on the photo from a traffic accident scene is required to reflect and allow inspectors to accurately determine the actual distance between objects in the scene. Therefore the images taken from the traffic accident scene need to be corrected before being spliced to each other. The image correction allows the information on the scene to be correctly displayed. The splicing on the corrected images ensures a thorough view and complete information gain that covers the whole scene.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5646855"},{"name":"Fault data collection in substations according to IEC 61850","snippet":"In a research project a fault data collection device for substations with a communication interface according to IEC 61850 was developed. The IEC 61850 standard is the future of substation automation networking. This standard has no models for signaling faults included up to date. Therefore, specific logical nodes were defined by using attribute types of the standard. These additional nodes were implemented and tested in a self made server.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5282051"},{"name":"Eye gaze correction to guarantee eye contact in videoconferencing","snippet":"In a typical desktop video-conference setup, the camera and the display screen cannot be physically aligned. This problem produces lack of eye contact and substantially degrades the user's experience. Expensive hardware systems using semi-reflective materials are available on the market to solve the eye gazing problem. However, these specialized systems are far away from the mass market. This paper presents an alternative approach using stereo rigs to capture a three-dimensional model of the scene. This information is then used to generate the view from a virtual camera aligned with the conference image the user looks at.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5336642"},{"name":"LEON3 ViP: A Virtual Platform with Fault Injection Capabilities","snippet":"In addition to functional simulation for validation of hardware\/software designs, there are additional robustness requirements that need advanced simulation techniques and tools to analyze the system behavior in the presence of faults. In this paper, we present the design of a fault injection framework for LEON3, a 32bit SPARC CPU based system used by the European Space Agency, described at Transaction Level using System C. First of all an extension of a previous XML formalization of basic binary faults, like memory and CPU registers corruption, is done in order to support TLM2.0transaction's parameters corruptions. Next a novel Dynamic Binary Instrumentation (DBI) technique for C++ binaries is used to insert fault injection wrappers in SystemC transaction path. For binary faults in model components the use of TLM2.0 transport_dbg is proposed. This way each component with fault injection capabilities exposes a standard interface to allow internal component inspection and modification.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5615462"},{"name":"A Method to Aid Recovery and Maintenance of the Input Error Correction Features","snippet":"In an information system, inputs are submitted to the system from its external environment. However, many input errors cannot be detected automatically and therefore result in errors in the effects raised by the system. Hence, the provision of input error correction features to correct these erroneous effects is critical. The recovery and maintenance of these features are complex and tedious. We have discovered some interesting control flow graph properties with regard to input errors and the implementation of their correction features. This paper proposes a method for the automated recovery of after-effect input error correction features from these properties. Based on the recovered information, we further propose a method to aid the maintenance of these features using decomposition slicing. All the empirical properties have been validated statistically. The approach has also been evaluated through some case studies","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4021363"},{"name":"Manga University: web-based correction system for artistic design education","snippet":"In artistic design education, the teacher instructs each student individually face to face. As a result, it is difficult to share coaching with a third party or to teach distantly. To solve these problems, we have developed Manga University, which is a web-based application to aid artistic design education. It enables distant teaching or shared teaching and provides a learning portfolio for collaboration. It is applicable to several other types of artistic design education, such as fashion design or GUI design for software or the web.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1186269"},{"name":"A smart design of coolant tank leak testing equipment in car manufacture using fault detection and isolation observer based method","snippet":"In car manufacture industry (in Shanghai), we need to do leak test for coolant tank. Normally, many coolant tanks e.g. 5 have to be tested at the same time. The traditional test system has only one pressured liquid input with a single pressure sensor and a temperature sensor to monitor and control the coolant tank leak test system. Once a tank leaked, it was difficult to identify which one in testing had been leaking. This paper presents a smart design of the test equipment for a coolant tank leak test using existing testing equipment. This design applied fault detection and isolation observer based method.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4582855"},{"name":"A 180A Phase Shifter With Small Phase Error for Broadband Applications","snippet":"In commonly used multi-bit high-\/low-pass phase shifters, the phase error is mostly due to the first bit, which provides the 180deg phase shift. This paper explores the design of a 180deg phase shifter that combines a high-pass filter with a transmission line to reduce the phase error over a large bandwidth. Compared to the conventional high-\/low-pass phase shifter, the proposed phase shifter shows better performance in both phase error and amplitude balance over a broad bandwidth. To illustrate the principle, a 180degphase shifter using this topology is designed, fabricated and measured. The phase error is measured to be plusmn2deg, plusmn3.5deg and plusmn4.5deg over a bandwidth of 31.8%, 43.4% and 49.3% respectively. The measured amplitude imbalance of the two branches is within 0.4 dB from 840 to 1310 MHz and the return loss is found to be better than 17 dB inclusive of the effect of the switches and the discrete components.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4450295"},{"name":"Multi-bit Error Tolerant Caches Using Two-Dimensional Error Coding","snippet":"In deep sub-micron ICs, growing amounts of on-die memory and scaling effects make embedded memories increasingly vulnerable to reliability and yield problems. As scaling progresses, soft and hard errors in the memory system will increase and single error events are more likely to cause large-scale multi- bit errors. However, conventional memory protection techniques can neither detect nor correct large-scale multi-bit errors without incurring large performance, area, and power overheads. We propose two-dimensional (2D) error coding in embedded memories, a scalable multi-bit error protection technique to improve memory reliability and yield. The key innovation is the use of vertical error coding across words that is used only for error correction in combination with conventional per-word horizontal error coding. We evaluate this scheme in the cache hierarchies of two representative chip multiprocessor designs and show that 2D error coding can correct clustered errors up to 32times32 bits with significantly smaller performance, area, and power overheads than conventional techniques.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4408256"},{"name":"An algorithm for diagnostic fault simulation","snippet":"In diagnostic testing faults detectable by test vectors are partitioned into groups. This partitioning is such that a fault is distinguishable from faults in all other groups, but is indistinguishable from those in its own group. Diagnostic fault coverage (DC) is defined as the number of fault groups divided by the total number of faults. We present a new diagnostic fault simulation algorithm that determines the DC of given test vectors and produces a fault dictionary. For each vector, we begin with detected fault list at each primary output obtained from a convetional fault simulator. For the vector being simulated each fault is assigned a detection index that uniquely specifies its detection status at all primary outputs. Fault list is then partitioned. Faults with different detection index are distinguished by the simulated vector and are kept in separate groups. Any fault in a group by itself is dropped from further simulation with subsequent vectors for which its detection index remains unknown (X). After simulation of each vector, the cumulative DC is obtained by counting the fault groups. Fault dictionary syndrome for a fault is the array of its detection indexes.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5550345"},{"name":"Numerical simulation of propagation and defect reflection of T(0,1) mode guided wave in pipes","snippet":"For the complexity of calculating and analyzing the guided wave propagation and defect reflection in steel pipes, and the instructional role on studying the characters of T(0,1) mode guided wave to experimental studies, a method associating guided wave theory with numerical solution was applied to simulate T(0,1) mode guided wave propagation and defect reflection in steel pipes by building models, imposing surface loads, and calculating in the ANSYS program, and the characters of T(0,1) mode guided wave were studied. The results of numerical calculation prove that: the T(0,1)mode guided wave was basically non-dispersive in reasonable frequencies, the attenuation trend of amplitude was exponential and the amplitude was basically keeping stable after propagating some distance, the T(0,1) mode guide wave was sensitive to both inner and outer circumferential defects. The reflection coefficient of T(0,1) mode guided wave increases linearly with the increase of circumferential length and depth of defects. When defect depth is not through-thickness, axial length has more influence on reflection coefficient. When defect depth is through-thickness, the influence of axial length to reflection coefficient is basically omitted.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5619082"},{"name":"How Much Fault Protection is Enough - A Deep Impact Perspective","snippet":"For the deep impact project, a myriad of fault protection (FP) monitors, symptoms, alarms and responses is engineered into the spacecraft FP software, common and yet custom to the flyby and impactor mother-daughter spacecraft. Device faults and functional faults are monitored, which are mapped 1-to-n into FP symptoms, per instance of the fault. Symptoms are then mapped n-to-1 to FP alarms, further down mapped n-to-1 to FP responses. Though the final statistics of 49 monitors, 921 symptoms, 667 alarms, and 39 responses appear to be staggering, it remains debatable whether the amount of on-board autonomous fault protection is sufficient and friendly to operate","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1559557"},{"name":"Network fault management systems using multiple mobile agents for multihomed networks","snippet":"For the increasing of the network complexity, it is not easy to exactly determine where the network fault is. Using the characteristic of multihoming, the paper proposes a scheme to identify the network faults, where six types of mobile agents are developed to cooperate to provide fault management functions. Moreover, the characteristics of mobility, intelligence and flexibility help the proposed scheme to identify the faults quickly. Besides, the proposed scheme is implemented on National Broadband Experimental Network (NBEN) and Taiwan Research Network (TANet2), where the two interconnected networks have their own routing policies, but are managed by the common mobile-agent-based network management network. Experimental results show that the ping-monitoring agent implemented in the proposed network fault management system has a 59.66% reduction in the time of monitoring the whole NBEN network.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1204250"},{"name":"Efficient Memory Error Coding for Space Computer Applications","snippet":"For the secure transaction of data between the central processing unit (CPU) of a satellite on board-computer and its local random access memory (RAM), the program memory has been usually designed with triple modular redundancy (TMR), which is a hardware implementation that includes replicated memory circuits and voting logic to detect and correct a faulty value. TMR error correction technique allows single correction of one error bit per stored word. For computers on board a satellite, there is however a definite risk of two error bits occurring within one byte of stored data. In this paper, the application of the quasi-cyclic codes to the routine error protection of SRAM program memory for satellites in low Earth orbit is described and implemented in field programmable gate array (FPGA) technology. The proposed device is transparent to the routine transfer of data between CPU and its local RAM","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1684773"},{"name":"Incorporating imperfect debugging into software fault processes","snippet":"For the traditional SRGMs, it is assumed that a detected fault is immediately removed and is perfectly repaired with no new faults being introduced. In reality, it is impossible to remove all faults from the fault correction process and have a fault-free effect on the software development environment. In order to relax this perfect debugging assumption, we introduce the possibility of imperfect debugging phenomenon. Furthermore, most of the traditional SRGMs have focused on the failure detection process. Consideration of fault correction process in the existing models is limited. However, to achieve desired level of software quality, it is very important to apply powerful technologies for removing the errors in the fault correction process. Therefore, we divide these processes into different two nonhomogeneous Poisson processes (NHPPs). Moreover, these models are considered to be more practical to depict the fault-removal phenomenon in software development.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1414597"},{"name":"A fault-tolerant system for Java\/CORBA objects","snippet":"Frameworks like CORBA facilitate the development of distributed applications through the use of off-the-shelf components. Though the use of distributed components allows faster building of applications, it also reduces the application availability as failure of any component can make the application unavailable. In this paper we present the design and implementation of a fault-tolerant system for CORBA objects implemented in Java. The proposed fault tolerant system employs object replication. We use a three tier architecture in which the middle tier manages replication and acts as a proxy for replicated objects. The proxy ensures consistency and transparency. In the current implementation, the proxy uses the primary-site approach to ensure strong consistency. Saving and restoring of objects' state is done transparently and it does not require object implementation to have special functions implemented for this purpose.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4536152"},{"name":"Fault-tolerant scheduling in distributed real-time systems","snippet":"In distributed systems, a real-time task has several subtasks which need to be executed at different nodes. Some of these subtasks can be executed in parallel on different nodes without violating their precedence relationships, if any, among them. To better exploit the parallelism, it becomes necessary to assign separate deadlines to subtasks and schedule them independently. We use three subtask deadline assignment policies which we have introduced earlier to develop a bidding-based fault-tolerant scheduling algorithm for distributed real-time systems. A local scheduler which resides on each node, tries to determine a schedule for each subtask according to the primary-backup approach. In this paper we discuss the algorithm and present the results of simulation studies conducted to establish the efficacy of our algorithm","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=962608"},{"name":"New Method for Detecting Low Current Faults in Electrical Distribution Systems","snippet":"In electrical distribution systems, low current faults may be caused by a high impedance fault or by the fault current limitation caused by the neutral to ground connection. In the former case, an indirect contact or insulation degradation give a high value of the fault impedance. In the latter, the neutral grounding may be either isolated or compensated. Nevertheless, these types of faults do not produce enough current so that the traditional overcurrent relays or fuses are not able to detect the fault. This paper presents a new methodology, based on the superposition of voltage signals of certain frequency, for the detection of low current single phase faults in radial distribution systems. The simulation analysis and laboratory tests carried out have proved the validity of the methodology for any type of grounding method.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4302565"},{"name":"A comparison of techniques to optimize measurement of voltage changes in electrical impedance tomography by minimizing phase shift errors","snippet":"In electrical impedance tomography, errors due to stray capacitance may be reduced by optimization of the reference phase of the demodulator. Two possible methods, maximization of the demodulator output and minimization of reciprocity error have been assessed, applied to each electrode combination individually, or to all combinations as a whole. Using an EIT system with a single impedance measuring circuit and multiplexer to address the 16 electrodes, the methods were tested on resistor-capacitor networks, saline-filled tanks and humans during variation of the saline concentration of a constant fluid volume in the stomach. Optimization of each channel individually gave less error, particularly on humans, and maximization of the output of the demodulator was more robust. This method is, therefore, recommended to optimize systems and reduce systematic errors with similar EIT systems.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1021934"},{"name":"Machining accuracy improvement of five-axis machine tools by geometric error compensation","snippet":"In five-axis machining, geometric error of the machine tool is an important error source reducing machining accuracy. Through geometric error compensation, the machining accuracy can be improved. A software-based error compensation strategy is discussed in this study. All individual geometric error components of five-axis machine tools are identified by a laser interferometer system and a ball bar system. To acquire synthetic error, a generalized error model was established based on muti-body system (MBS) and homogeneous transfer matrix (HTM). Finally, the geometric error was compensated by correcting the NC code. Corresponding error compensation software has been developed and an experiment has shown the feasibility of the proposed compensation method.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6139041"},{"name":"A Resource Management System for Fault Tolerance in Grid Computing","snippet":"In grid computing, resource management and fault tolerance services are important issues. The availability of the selected resources for job execution is a primary factor that determines the computing performance. The failure occurrence of resources in the grid computing is higher than in a tradition parallel computing. Since the failure of resources affects job execution fatally, fault tolerance service is essential in computational grids. And grid services are often expected to meet some minimum levels of quality of service (QoS) for desirable operation. However Globus toolkit does not provide fault tolerance service that supports fault detection service and management service and satisfies QoS requirement. Thus this paper proposes fault tolerance service to satisfy QoS requirement in computational grids. In order to provide fault tolerance service and satisfy QoS requirements, we expand the definition of failure, such as process failure, processor failure, and network failure. And we propose resource scheduling service, fault detection service and fault management service and show implement and experiment results.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5283868"},{"name":"Reference node selection algorithm and localization error analysis for indoor sensor networks","snippet":"In indoor environment, one of the major challenges for researchers is to localize the sensor nodes with relatively high localization precision. Many traditional positioning algorithms dealt with the node localization issues, such as two-phase positioning (TPP) algorithm, without taking into account the \"reference node\" parameter, however which also strongly affects the quality of spatial localization. We analyze the localization error and draw the conclusion that the localization error is the least when three reference nodes form an equilateral triangle. Therefore, we propose reference node selection algorithm based on trilateration (RNST). The simulation results show that our algorithm can meet real-time localization requirement of the mobile nodes in indoor environment, and make the localization error less than that of the traditional algorithm.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4401671"},{"name":"SVM Classifier for Impulse Fault Identification in Transformers using Fractal Features","snippet":"Improper or inadequate insulation may lead to failure during impulse tests of a transformer. It is important to identify the type and the exact location of insulation failure within the winding of power transformers. This paper describes a new approach using fractal theory for extraction of features from the impulse test response of a transformer and Support Vector Machine (SVM) in regression mode to classify the fault response patterns. A variety of algorithms are available for the computation of Fractal Dimension (FD). In the present work, Box counting and Higuchi's algorithm for the determination of FD, Lacunarity, and Approximate Entropy (ApEn) has been used for the extraction of fractal features form time domain impulse test response. The analysis has been performed on both Analog and Digital Models of a 3 MVA, 33\/11 kV transformer. A noticeable finding is that the SVM tool trained with the simulated data only is capable of identifying the location and fault classes of analog model data accurately within a tolerance limit of plusmn3.37% .","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4401238"},{"name":"Evaluation of probabilistic-based selectivity technique for earth fault protection in MV networks","snippet":"In a Bayesian selectivity technique has been introduced to identify the faulty feeder in compensated medium voltage (MV) networks. The proposed technique has been based on a conditional probabilistic method applied on transient features extracted from the residual currents only using the Discrete Wavelet Transform (DWT). In this paper, the performance of this selectivity technique is evaluated when the current transformers (CTs) impacts are considered. The CTs are modeled considering their frequency characteristics. Furthermore, network noises are added to the simulated signals. Therefore, the algorithm can be tested at different practical conditions, such as nonlinear characteristics of the measuring devices and the impact of noise as well. The fault cases occurring at different locations in a compensated 20 kV network are simulated by ATP\/EMTP. Results show a reduction in the algorithm sensitivity with considering CT and noise effectiveness.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5282189"},{"name":"Fast Recovery and QoS Assurance in the Presence of Network Faults for Mission-Critical Applications in Hostile Environments","snippet":"In a hostile military environment, systems must be able to detect and react to catastrophes in a timely manner in order to provide assurance that critical tasks will continue to meet their timeliness requirements. Our research focuses on achieving network quality of service (QoS) assurance using a Bandwidth Broker in the presence of network faults in layer-3 networks. Passive discovery techniques using the link-state information from routers provide for rapid path discovery which, in turn, leads to fast failure impact analysis and QoS restoration. In addition to network fault tolerance, the Bandwidth Broker must be fault tolerant and must be able to recover quickly. This is accomplished using a modified commercially available and open-source in- memory database cluster technology.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4296863"},{"name":"Fault injection in distributed Java applications","snippet":"In a network consisting of several thousands computers, the occurrence of faults is unavoidable. Being able to test the behaviour of a distributed program in an environment where we can control the faults (such as the crash of a process) is an important feature that matters in the deployment of reliable programs. In this paper, we investigate the possibility of injecting software faults in distributed Java applications. Our scheme is by extending the FAIL-FCI software. It does not require any modification of the source code of the application under test, while retaining the possibility to write high level fault scenarios. As a proof of concept, we use our tool to test FreePastry, an existing Java implementation of a distributed hash table (DHT), against node failures","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1639507"},{"name":"Job-Site Level Fault Tolerance for Cluster and Grid environments","snippet":"In order to adopt high performance clusters and grid computing for mission critical applications, fault tolerance is a necessity. Common fault tolerance techniques in distributed systems are normally achieved with checkpoint-recovery and job replication on alternative resources, in cases of a system outage. The first approach depends on the system's MTTR while the latter approach depends on the availability of alternative sites to run replicas. There is a need for complementing these approaches by proactively handling failures at a job-site level, ensuring the system high availability with no loss of user submitted jobs. This paper discusses a novel fault tolerance technique that enables the job-site recovery in Beowulf cluster-based grid environments, whereas existing techniques give up a failed system by seeking alternative resources. Our results suggest sizable aggregate performance improvement during an implementation of our method in Globus-enabled HA-OSCAR. The technique called ''smart failover\" provides a transparent and graceful recovery mechanism that saves job states in a local job-manager queue and transfers those states to the backup server periodically, and in critical system events. Thus whenever a failover occurs, the backup server is able to restart the jobs from their last saved state","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4154086"},{"name":"Soft error assessments for servers","snippet":"In order to assess the soft error rate (SER) of a server, it is important to not only quantify the soft error contribution of the individual semiconductor components, but also to account for derating and for SER mitigation like hardening and shielding. Derating describes the fact that not every soft error has an impact. A large number of soft errors vanish based on electrical, logical or timing considerations. They have no impact. Additionally, a server can, to a large degree, be protected from the impact of soft errors by implementing error detection and correction means. In these cases the impact of the soft error is limited to the extra compute time needed for the correction. Summing up the SER contributions from transistors and circuits results in the so-called raw soft error rate, a rate which describes just the bottom layer of the system stack. Powerful protection mechanisms at higher layers can reduce that rate by several orders of magnitude. Awareness of this vertical interaction across the different layers in the system stack leads to servers optimized for robustness.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5488799"},{"name":"Automated post-fault diagnosis of power system disturbances","snippet":"In order to automate the analysis of SCADA and digital fault recorder (DFR) data for a transmission network operator in the UK, the authors have developed an industrial strength multi-agent system entitled protection engineering diagnostic agents (PEDA). The PEDA system integrates a number of legacy intelligent systems for analyzing power system data as autonomous intelligent agents. The integration achieved through multi-agent systems technology enhances the diagnostic support offered to engineers by focusing the analysis on the most pertinent DFR data based on the results of the analysis of SCADA. Since November 2004 the PEDA system has been operating online at a UK utility. In this paper the authors focus on the underlying intelligent system techniques, i.e. rule-based expert systems, model-based reasoning and state-of-the-art multi-agent system technology, that PEDA employs and the lessons learnt through its deployment and online use","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1709651"},{"name":"Improving access to relevant data on faults, errors and failures in real systems","snippet":"In order to be able to test the effectiveness and verify proposed techniques for enhanced availability based on field data from systems it is important to have reliability data of the components and the information necessary to characterize or model the system. This includes inter alia the type and number of components, their protection and dependency relations as well the automatic recovery mechanisms built into the system. An important benefit of making system models and logs available to the research community in a standard format is that it opens up the possibility for creating tools to assess and optimize deployed as well as hypothetical system configurations. Specialized tools for on-line and off-line analysis and classification of reliability data also become viable. Availability modeling tools could be benchmarked against actual data. Depending on the usefulness of such tools and the level of adoption of standard models and formats in the industry a market for reliability data analysis tools could emerge over time. These tools could be used during the design, deployment and operation phases of a system in order to predict or enhance the availability of the services it provides","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4020835"},{"name":"Development of image-based scatter correction for brain perfusion SPECT study: comparison with TEW method","snippet":"In order to convert scatter uncorrected into corrected SPECT image, an image-based scatter correction (IBSC) method has been developed. The aim of this study was validation of its role as image converter from scatter uncorrected into corrected images equivalent to image corrected by conventional TEW method. IBSC method is executed in the postreconstruction process and only requires an attenuation corrected main photopeak image with broad  value, I<sub>AC<\/sub><sup>b<\/sup>. The scatter component image is estimated by convolving I<sub>AC<\/sub><sup>b<\/sup> with a scatter function followed by multiplying with an image-based scatter fraction (SF) function. The IBSC method was evaluated with Monte Carlo simulations and <sup>99m<\/sup>Tc-ECD SPECT human brain perfusion studies obtained from five volunteers. The noise property of the scatter corrected image using IBSC method, I<sub>IBSC<\/sub>, was compared with that by TEW method, I<sub>TEW<\/sub>, with simulated brain phantom images. Image contrast between gray with white matter in the human study was also compared between IBSC and TEW method. The global signal-to-noise (S\/N) ratio of I<sub>IBSC<\/sub> was decreased to 14% compared to that of I<sub>AC<\/sub><sup>b<\/sup>, whereas that of I<sub>TEW<\/sub> was decreased to 21%. In human brain imaging, significant difference in image contrast between IBSC and TEW method was not observed (p<0.05). In conclusion, the IBSC method could be applied to clinical brain perfusion SPECT as conversion I<sub>AC<\/sub><sup>b<\/sup> into a scatter corrected image equivalent to I<sub>TEW<\/sub>. This achieves a better noise property than the TEW method.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1352434"},{"name":"Parametric fault tree for the dependability analysis of redundant systems and its high-level Petri net semantics","snippet":"In order to cope efficiently with the dependability analysis of redundant systems with replicated units, a new, more compact fault-tree formalism, called Parametric Fault Tree (PFT), is defined. In a PFT formalism, replicated units are folded and indexed so that only one representative of the similar replicas is included in the model. From the PFT, a list of parametric cut sets can be derived, where only the relevant patterns leading to the system failure are evidenced regardless of the actual identity of the component in the cut set. The paper provides an algorithm to convert a PFT into a class of High-Level Petri Nets, called SWN. The purpose of this conversion is twofold: to exploit the modeling power and flexibility of the SWN formalism, allowing the analyst to include statistical dependencies that could not have been accommodated into the corresponding PFT and to exploit the capability of the SWN formalism to generate a lumped Markov chain, thus alleviating the state explosion problem. The search for the minimal cut sets (qualitative analysis) can be often performed by a structural T-invariant analysis on the generated SWN. The advantages that can be obtained from the translation of a PFT into a SWN are investigated considering a fault-tolerant multiprocessor system example.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1183940"},{"name":"Fabrication error in resonant frequency of microstrip antenna","snippet":"In order to fabricate microstrip antenna with high precision for micromechanical application, we calculated the error in the resonant frequency of a patch antenna. Accuracy of the obtained results is compared with the experimental data. An accurate analysis allows success on first fabrication","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=965219"},{"name":"Application of MATLAB in teaching of Error Theory And Survey Adjustment","snippet":"In order to improve and strengthen the ability of survey data processing, MATLAB software is introduced in the course of Error Theory and Survey Adjustment as the teaching assistance method. Based on its ability formidable calculating function, graph handling ability and the rich toolbox, the MATLAB-based Error Theory and Survey Adjustment experiment has been set up. It improves the students' creative capability and stimulates learning activity, because the abstract theory becomes direct viewing and vivid, so the teaching effect is improved.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5622615"},{"name":"Subpixel Edge Location Using Orthogonal Fourier-Mellin Moments Based Edge Location Error Compensation Model","snippet":"In order to improve the edge location accuracy of vision measurements, this paper presents a compensation model for edge location calculation using the orthogonal Fourier-Mellin moments (OFMMs). The edge location error is the difference between the sampling edge location and the actual one which occurs when the edge location is not within the center pixel and pixel boundaries. We established a look-up table which as the compensation model to eliminate the error for OFMMs.Experimental results showed that the proposed model can be used to correct the error of edge location. The precision of 0.19 pixel by using OFMMs with the compensation model can be achieved. It can be concluded that the proposed method is an efficient approach to satisfy the practical requirements for high accuracy for edge detection.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4696488"},{"name":"Analysis of measuring errors for the visible light phase-shifting point diffraction interferometer","snippet":"In order to improve the measuring accuracy of the visible light phase-shifting point diffraction interferometer (PS\/PDI) for the extreme ultraviolet lithography (EUVL) aspheric mirrors, the main measuring errors will be discussed in this paper. At first, the elementary configuration and measuring principle of the visible light phase-shifting point diffraction interferometer are introduced briefly, then the different errors which are possible to affect the measuring result are summed up, the errors include PZT phase-shifting error, detector nonlinearity error, detector quantization error, wavelength instability error and intensity instability error of the laser source, vibration error, air refractivity instability error and so on. Through detailed analysis and simulation, the magnitude of these errors can be obtained. By analysing the reasons which cause these errors and the relationship between these errors and interferometer configuration parameters, some methods are put forward to avoid or restrain these errors accordingly.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5713588"},{"name":"Efficient High-Speed Interface Verification and Fault Analysis","snippet":"In this article we discuss three challenges of device verification and test for high-speed interfaces, with special focus on latest memory device interface generations as DDR3 \/ GDDR5 \/ XDR working in the GHz clock frequency range. We address how efficient device verification in terms of reaching target coverage fast can be achieved through random methods on ATE, both random operating condition tests and Random Test Pattern (RTP). We present a novel Random Test Pattern generation method suitable for memory device verification. Failure conditions and failing pattern must be transferred to simulation for root-cause understanding, which is not directly possible due to the large gap between pattern length on ATE in the order of 10<sup>6<\/sup> clock cycles and pattern length limitations in simulation of 10<sup>3<\/sup> clock cycles. We present an extraction algorithm on ATE with DUT in the loop to extract the minimum length failing test pattern sequence for simulation and root-cause analysis. An application example is presented, where a large Random Test Pattern revealed a special command sequence leading to device failure and how this sequence has been extracted for simulation. The presented random methods lead to fast detection of device issues by exploring the full coverage space. With the presented automated extraction that replaces manual interactive analysis, fast root-cause understanding with engineering time and ATE test-time reduction from typically few days to below one hour are achievable.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4700559"},{"name":"Application of set membership identification for fault detection of MEMS","snippet":"In this article, a set membership (SM) identification technique is tailored to detect faults in microelectromechanical systems. The SM-identifier estimates an orthotope which contains the system's parameter vector. Based on this orthotope, the system's output interval is predicted. If the actual output is outside of this interval, then a fault is detected. Utilization of this scheme can discriminate mechanical-component faults from electronic component variations frequently encountered in MEMS. For testing the suggested algorithm's performance in simulation studies, an interface between classical control-software (MATLAB) and circuit emulation (HSPICE) is developed","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1641783"},{"name":"Wide band harmonic suppression based on Koch-shaped defected ground structure for a microstrip patch antenna","snippet":"In this article, a wide band harmonic suppression microstrip patch antenna using the Koch-shaped defected ground structure (DGS) is presented. In order to realize the wide band harmonious suppression, the Koch-shaped defected ground structure (DGS) microstrip circuit is designed as the feed part of the proposed antenna. By inserting parallel slots, the antenna can be directly fed by the simple 50 microstrip transmission line. The proposed patch antenna operates at the center frequency of 2.5GHz and the spurious radiations up to 15GHz are properly suppressed. Simulated and measured results indicate that the Koch-shaped defected ground structure (DGS) is effective in suppressing spurious radiations. And the patch antenna considered here can be widely applied in active integrated communication systems.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5524944"},{"name":"Development of neural networks module for fault identification in asynchronous machine using various types of reference signals","snippet":"In this article, the device of automatic diagnostic of asynchronous motor is discussed. This diagnostic system is based on artificial neural network (ANN), in order to find the different defects by classification. The machine health identification process is mainly based on recognition and comparison of real-time captured standard signature as stator current, rotation speed of machine. The features extraction of the instantaneous signals will then input to an artificial neural networks (ANN) for recognition and identification. The output of the neural network was trained to generate a healthy index that indicates the machine health condition. In this work, the entries used in the neural network were the various types of signals: the instantaneous values and the effective values (root mean square) of the machine parameters.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1514041"},{"name":"Analysis of error recovery schemes for networks on chips","snippet":"In this article, we discuss design constraints to characterize efficient error recovery mechanisms for the NoC design environment. We explore error control mechanisms at the data link and network layers and present the schemes' architectural details. We investigate the energy efficiency, error protection efficiency, and performance impact of various error recovery mechanisms.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1511975"},{"name":"Design of Energy-Efficient High-Speed Links via Forward Error Correction","snippet":"In this brief, we show that forward error correction (FEC) can reduce power in high-speed serial links. This is achieved by trading off the FEC coding gain with specifications on transmit swing, analog-to-digital converter (ADC) precision, jitter tolerance, receive amplification, and by enabling higher signal constellations. For a 20-in FR4 link carrying 10-Gb\/s data, we demonstrate: 1) an 18-mW\/Gb\/s savings in the ADC; 2) a 1-mW\/Gb\/s reduction in transmit driver power; 3) up to 6?? improvement in transmit jitter tolerance; and 4) a 25- to 40-mV improvement in comparator offset tolerance with 3?? smaller swing.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5464335"},{"name":"End-to-end defect modeling","snippet":"In this context, computer models can help us predict outcomes and anticipate with confidence. We can now use cause-effect modeling to drive software quality, moving our organization toward higher maturity levels. Despite missing good software quality models, many software projects successfully deliver software on time and with acceptable quality. Although researchers have devoted much attention to analyzing software projects' failures, we also need to understand why some are successful - within budget, of high quality, and on time-despite numerous challenges. Restricting software quality to defects, decisions made in successful projects must be based on some understanding of cause-effect relationships that drive defects at each stage of the process. To manage software quality by data, we need a model describing which factors drive defect introduction and removal in the life cycle, and how they do it. Once properly built and validated, a defect model enables successful anticipation. This is why it's important that the model include all variables influencing the process response to some degree.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1331312"},{"name":"Rateless Codes With Unequal Error Protection Property","snippet":"In this correspondence, a generalization of rateless codes is proposed. The proposed codes provide unequal error protection (UEP). The asymptotic properties of these codes under the iterative decoding are investigated. Moreover, upper and lower bounds on maximum-likelihood (ML) decoding error probabilities of finite-length LT and Raptor codes for both equal and unequal error protection schemes are derived. Further, our work is verified with simulations. Simulation results indicate that the proposed codes provide desirable UEP. We also note that the UEP property does not impose a considerable drawback on the overall performance of the codes. Moreover, we discuss that the proposed codes can provide unequal recovery time (URT). This means that given a target bit error rate, different parts of information bits can be decoded after receiving different amounts of encoded bits. This implies that the information bits can be recovered in a progressive manner. This URT property may be used for sequential data recovery in video\/audio streaming","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4137897"},{"name":"Comments on \"Data Mining Static Code Attributes to Learn Defect Predictors\"","snippet":"In this correspondence, we point out a discrepancy in a recent paper, \"data mining static code attributes to learn defect predictors,\" that was published in this journal. Because of the small percentage of defective modules, using probability of detection (pd) and probability of false alarm (pf) as accuracy measures may lead to impractical prediction models.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4288196"},{"name":"A Lower Bound on the Probability of Undetected Error for Binary Constant Weight Codes","snippet":"In this correspondence, we study the probability of undetected error for binary constant weight codes. First, we derive a new lower bound on the probability of undetected error for binary constant weight codes. Next, we show that this bound is tight if and only if the binary constant weight codes are generated from certain t-designs in combinatorial design theory. This means that these binary constant weight codes generated from certain t-designs are uniformly optimal for error detection. Along the way, we determine the distance distributions of such binary constant weight codes. In particular, it is shown that binary constant weight codes generated from Steiner systems are uniformly optimal for error detection. Thus, we prove a conjecture of Xia, Fu, Jiang, and Ling. Furthermore, the distance distribution of a binary constant weight code generated from a Steiner system is determined. Finally, we study the exponent of the probability of undetected error for binary constant weight codes. We derive some bounds on the exponent of the probability of undetected error for binary constant weight codes. These bounds enable us to extend the region in which the exponent of the probability of undetected error is exactly determined","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1683941"},{"name":"Defects detection and characterization by using cellular neural networks","snippet":"In this document, a new method to detect and to characterize surface defects in mechanical parts is reported. Cellular neural networks are used as tools for the implementation of the stereoscopic vision analysis technique. Suitable applications in microscopic defect analysis (real-time processing), in various fields (e.g. aeronautics applications) are introduced, by means of the reported examples in order to validate the cellular neural networks (CNN) approach","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=921352"},{"name":"A method of probe refinement for fault diagnosis","snippet":"In order to obtain the information of the actual execution sequences, probes are required to be deployed in the system, which may influence the operation of system. In this paper, a method of probe refinement is proposed to reduce the cost of probes. By taking the distinction capacity of component as heuristic information, and considering the actual constraints such as that the probes in different positions impact the system differently, the algorithm of reduction is applied to remove these components which can not affect the recognition of the execution sequences. Then the necessary information of the sequences can be obtained with fewer probes deployed in the rest components. Meanwhile, the strategy is discussed and implemented to ensure the completeness of the result. Finally, the experiment shows that the refinement method can reduce the overheads of introducing probes effectively.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5579044"},{"name":"Bearing fault detection based on order bispectrum","snippet":"In order to process the non-stationary vibration signals such as speed up or speed down vibration signals effectively, the order bispectrum analysis technique is presented. This new method combines computed order tracking technique with bispectrum analysis. Firstly, the vibration signal is sampled at constant time increments and then uses software to resample the data at constant angle increments. Therefore, the time domain transient signal is converted into angle domain stationary one. In the end, the resampled signals are processed by bispectrum analysis technology. The experimental results show that order bispectrum analysis can effectively detect the bearing fault.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5647398"},{"name":"Research of industrial furnace fault diagnosis expert system","snippet":"In order to realize fast location and detection of abnormal status during running of industrial furnace, especially abnormal status of firing, this article studies and designs a fault diagnosis expert system based on fault tree theory. Firstly, formalized definition of industrial furnace fault diagnosis expert system is given in the paper, then all component elements of the expert system are analyzed and designed in detail, finally the principles and methods of design knowledge base are introduced to us by using the fault tree theory, and also reasoning algorithm is put forward, which is used to reason the fault by way of fault tree. The project practice indicates that this system knowledge model has good suitability, and what's more it is used simply and conveniently, also the outcome of fault diagnosis is reliable and steady.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5619377"},{"name":"Graph fitting test method for the interpolation error of moire fringe","snippet":"In order to realize the fast test for the interpolation error, the graph fitting test method for the interpolation error of Moire fringe is put forward in this paper. Firstly, the triangular wave Moire fringe photoelectric signal of the encoder whose phase difference is 90deg are sampled to get the Lissajous graph of the two signals. Secondly, the single wave represented by the founded subsection function is used to fit the practical Moire fringe Lissajous graph. Then, the fitting result is tested to verify whether it satisfies the accuracy requirement. Lastly, the founded subsection function instead of the practical wave function is used to calculate the interpolation error. Using the graph fitting method to sample the Moire fringe single of 15-bits photoelectric encoder to get the interpolation error curve, the tested maximum interpolation error is 70rdquo and the minimum error is - 69rdquo. Comparing with the interpolation error which is received from traditional test method, the change trend of the interpolation error curve is similar, and peak-peak value is almost equality. The results of experiment indicate that: the equipment is convenient and the examination method is efficient and feasible. The measure speed is fast and the manifestation result is intuitionistic. The system can be used in the working field. The method can avoid the speed influence and realize the dynamic interpolation error measure, which is significant for the research of encoder's dynamic accuracy characteristics.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5274511"},{"name":"Use of Faulted Phase Earthing using a custom built earth fault controller","snippet":"In order to reduce customer hours lost (CHL) and customer interruptions (CI), the use of Faulted Phase Earthing (FPE) is being considered on the Irish 20 kV distribution system. The operation of this particular FPE system is enabled by the use of a custom built Earth Fault Controller (EFC) that has the ability to detect high impedance faults of up to 12 k??. The EFC can also successfully identify single pole switching events, which have at times caused the mal-operation of existing protection. FPE involves the earthing of a faulted phase during a single line to ground fault. This ensures that the fault site is made safer and that no customers are interrupted during the fault.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5522145"},{"name":"Study on the Neural Network Model for Shield Construction Faults Diagnosis","snippet":"In order to solve the problem of establishing the mathematic model for shield construction faults diagnosis, an approach to the mathematic model by using BP neural network is presented in this paper. The BP neural network model for diagnosing three familiar shield construction faults based on the data of shield excavation parameters was built. The inputs of the model are respectively nine shield excavation parameters which are correlative with shield construction faults. The outputs of the model are three shield construction faults which are respectively the spewing at screw conveyer, the wear of disc-cutters and the jamming of shield. The case study of a shield project validated that the structure of the established model is practical, the diagnostic results are right and the diagnosis method is effective. The conclusion provides the beneficial guidance for the design of the online diagnosis system of shield construction faults based on the data of shield excavation parameters.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5656576"},{"name":"Research on influences of sampling errors on performances of three-level PWM rectifier","snippet":"In order to solve the problem that the sampling errors depress the control performances of pulse width modulation (PWM) rectifier, this paper focuses on a three-level PWM rectifier with voltage oriented control (VOC) strategy, introduces the structure of sampling process, and analyzes the influences of sampling errors on the control performances quantificationally. A software optimization method aimed for restraining the persistent and random sampling errors is then proposed. Simulation and experimental results verified the validity of the analyses and the feasibility of the proposed method. The performances of the rectifier were remarkably improved by using the proposed optimization method.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5382954"},{"name":"Multilayer Architecture Based on HMM and SVM for Fault Classification","snippet":"In order to solve the problems of current machine learning in fault diagnosing system of the chemical plants, a better and effective multilayer architecture model is used in this paper. Hidden Markov model (HMM) is good at dealing with dynamic continuous data and support vector machine (SVM) shows superior performance for classification, especially for limited samples. Combining their respective virtues, we propose a new multilayer architecture model to improve classification accuracy for a fault diagnosis example. The simulation result shows that this two level architecture framework combining HMM and SVM is better than the single HMM method in high classification accuracy with small training samples.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5412442"},{"name":"Adaptive Fault-Tolerance by Exposing Service Request Process as First-Class Object in Pervasive Computing","snippet":"In the open and dynamic pervasive computing environment, it is challenging to detect and handle the frequently occurred failures of service request. The widely used transparent mechanisms Remote Procedure Call and Object Request Broker have a great impact on the adaptive fault-tolerance, because they make it difficult for service requester to sense, configure and control the service request process. In this paper, we explicitly encapsulate and expose the service request process as the first-class object. By the exposed service request object, both the pervasive computing platform and the user applications are able to acquire the adaptive and systematic fault-tolerance ability during the service request process.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5482747"},{"name":"A Cellular Approach to Fault Detection and Recovery in Wireless Sensor Networks","snippet":"In the past few years wireless sensor networks have received a greater interest in application such as disaster management, border protection, combat field reconnaissance and security surveillance. Sensor nodes are expected to operate autonomously in unattended environments and potentially in large numbers. Failures are inevitable in wireless sensor networks due to inhospitable environment and unattended deployment. The data communication and various network operations cause energy depletion in sensor nodes and therefore, it is common for sensor nodes to exhaust its energy completely and stop operating. This may cause connectivity and data loss. Therefore, it is necessary that network failures are detected in advance and appropriate measures are taken to sustain network operation. In this paper we extend our cellular architecture and proposed a new mechanism to sustain network operation in the event of failure cause of energy-drained nodes. In our solution the network is partitioned into a virtual grid of cells to perform fault detection and recovery locally with minimum energy consumption. Specifically, the grid based architecture permits the implementation of fault detection and recovery in a distributed manner and allows the failure report to be forwarded across cells. The proposed failure detection and recovery algorithm has been compared with some existing related work and proven to be more energy efficient.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5210889"},{"name":"Optimizing Cauchy Reed-Solomon Codes for Fault-Tolerant Network Storage Applications","snippet":"In the past few years, all manner of storage applications, ranging from disk array systems to distributed and wide-area systems, have started to grapple with the reality of tolerating multiple simultaneous failures of storage nodes. Unlike the single failure case, which is optimally handled with RAID level-5 parity, the multiple failure case is more difficult because optimal general purpose strategies are not yet known. Erasure coding is the field of research that deals with these strategies, and this field has blossomed in recent years. Despite this research, the decades-old Reed-Solomon erasure code remains the only space-optimal (MDS) code for all but the smallest storage systems. The best performing implementations of Reed-Solomon coding employ a variant called Cauchy Reed-Solomon coding, developed in the mid 1990's. In this paper, we present an improvement to Cauchy Reed-Solomon coding that is based on optimizing the Cauchy distribution matrix. We detail an algorithm for generating good matrices and then evaluate the performance of encoding using all implementations Reed-Solomon codes, plus the best MDS codes from the literature. The improvements over the original Cauchy Reed-Solomon codes are as much as 83% in realistic scenarios, and average roughly 10% over all cases that we tested","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1659489"},{"name":"Modeling Errors in Small Baseline Stereo for SLAM","snippet":"In the past few years, there has been significant advancement in localization and mapping using stereo cameras. Despite the recent successes, reliably generating an accurate geometric map of a large indoor area using stereo vision still poses significant challenges due to the accuracy and reliability of depth information especially with small baselines. Most stereo vision based applications presented to date have used medium to large baseline stereo cameras with Gaussian error models. Here we make an attempt to analyze the significance of errors in small baseline (usually <0.1m) stereo cameras and the validity of the Gaussian assumption used in the implementation of Kalman filter based SLAM algorithms. Sensor errors are analyzed through experimentations carried out in the form of a robotic mapping. Then we show that SLAM solutions based on the extended Kalman filter (EKF) could become inconsistent due to the nature of the observation models used","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4150352"},{"name":"New attenuation correction for the HRRT using transmission scatter correction and total variation regularization","snippet":"In the standard software for the Siemens HRRT PET scanner the most commonly used segmentation in the A-map reconstruction for human brain scans is MAP-TR. Problems with bias in the lower cerebellum and pons in HRRT brain images have been reported. The main source of the problem is poor bone \/ soft tissue segmentation in these regions and the lack of scatter correction in the A-map reconstruction. In this paper we describe and validate the new TXTV segmentation method (included in the HRRTU 1.0 and 1.1 user software) aimed at solving the bias problem.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5401730"},{"name":"Detection of rotor faults in torque controlled induction motor drives","snippet":"In the supervision of electrical equipment, the task of diagnostic system is to detect an upcoming machine fault as soon as possible, in order to save expensive manufacturing processes or to replace fault parts. An important issue in such effort is the modelling of the induction machine (IM) including rotor bar and end-ring faults, with a minimum of computational complexity. In this paper, a simpler method is employed in the simulation of an induction motor with rotor asymmetries. Simulation of classical and dynamic space vector models, Finite Element Analysis and experimental results are presented to support the proposed model. The need for detection of rotor faults at an earlier stage has pushed the development of monitoring methods with increasing sensitivity and noise immunity. Addressing diagnostic techniques based on current signatures analysis (MCSA), the characteristic components introduced by specific faults in the current spectrum are investigated and a diagnosis procedure correlates the amplitudes of such components to the fault extent. The impact of feedback control on asymmetric rotor cage induction machine behavior is also analyzed. It is shown that the variables usually employed in diagnosis procedures assuming open-loop operation are no longer effective under closed-loop operation. Simulation results show that signals already present at the drive are suitable to effective diagnostic procedure. The utilization of the current regulator error signals in rotor failure detection are the aim of the present work. The use of a band-pass filter bank to detect the presence of sidebands is also proposed.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4417754"},{"name":"Strategies to Attend Destructive and Nondestructive Faults in Power Transmission Substations","snippet":"In this document, the reliability evaluation of line circuits and the development of restoration strategies for ten transmission substations from the most important Colombian power transmission utility (ISA) are presented. There are two types of strategies to restore transmission substations: Fault procedures and contingency plans. The Fault procedures are used to attend non-destructive faults of the switching, control and protection systems on the transmission substation, while contingency plans help to attend destructive faults on power transformers and breakers.By applying the fault procedures and contingency plans, it causes the reduction of the economical consequences of the transmission equipment malfunctioning. The proposed restoration strategies help to accomplish the availability goals imposed by the market regulator and also to maintain the service continuity to the end user.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1468650"},{"name":"Zech Logarithmic Decoding of Triple-Error-Correcting Binary Cyclic Codes","snippet":"In this letter, Zech logarithmic decoding method is proposed for triple-error-correcting binary cyclic codes, which have not been developed before, whose generator polynomials have at most three irreducible factors.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4640951"},{"name":"Harmonics mitigation and power factor correction with a modern three-phase four-leg shunt active power filter","snippet":"In this paper a compensating system using four-leg shunt active power filter (SAPF) in a three-phase four-wire distribution network which will be able to mitigate harmonics, absorb or generate reactive power, and improve the power factor on supply side, is presented. Two control approaches based on p-q theory and load current detection using phase locked loop (PLL) are proposed. To validate the compensation performance of SAPF the distribution network with nonlinear loads is simulated using MATLAB\/Simulink software. Simulation results have proved and validated the performance of SAPF minimizing the total harmonics distortion (THD) and neutral current.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5697574"},{"name":"Statistical error modeling of CNN-UM architectures: the binary case","snippet":"In this paper a detailed error model is analyzed of the CNN-UM in a general statistical manner. The locally regular template class is considered and the possibility of erroneous output is expressed from the component nonlinearity and parameter deviation.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1035085"},{"name":"A method for intellectualized detection and fault diagnosis of vacuum circuit breakers","snippet":"In this paper a method for intellectualized detection and fault diagnosis of vacuum circuit breakers is introduced. The system consists of sensors, single-chips, measuring circuits, processing circuits, controlling circuits, extended ports, communication interface, etc. It can monitor on-line the condition of a vacuum circuit breaker, analyze its change tendency, identify and locate and display the detectable faults. This paper describes the main detecting principles and diagnostic foundations. The hardware structure and software design are also given","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=879101"},{"name":"Robust Data Hiding Technique for Video Error Concealment over DVB-H channel","snippet":"In this paper a method for providing additional information for error concealment during the transmission of a video file over an error-prone channel is presented. The transmission of the additional information is performed without increasing the bandwidth occupation, by hiding this information into the video itself at the encoder side. The considered transmission system is the DVB-H system. The performance of the method has been investigated by simulations and experimental tests in order to evaluate the perceived quality of the processed video and the robustness of the data hiding method against the H.264\/AVC coding.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4412908"},{"name":"A procedure to correct the error in the structure function based thermal measuring methods","snippet":"In this paper a methodology is presented to correct the systematic error of structure function based thermal material parameter measuring methods. This error stems from the fact that it is practically impossible to avoid parallel heat-flow paths in case of forced one-dimensional heat conduction. With the presented method we show how to subtract the effect of the parallel heat-flow paths from the measured structure function. With this correction methodology the systematic error of structure function based thermal material parameter measuring methods can be practically eliminated. Application examples demonstrate the accuracy increase obtained with the use of the method.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1320458"},{"name":"Performance Increase of Error Control Operation on Data Transmission","snippet":"In this paper a new approach is proposed to increase the performance of the operation of error control on data transmission. Specifically, a hardware structure for parallel cyclic redundancy check (CRC) calculation is developed to speed up the error control operation of data transmission. Based on a study of the properties of both CRC and check sum (CS) a new error detecting scheme is developed which combines CRC and CS. Also it is shown that the proposed error detecting scheme ensures high reliability and performance of the error control operation on data transmission in comparison to CRC alone.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5384741"},{"name":"High impedance fault detection in distribution networks using support vector machines based on wavelet transform","snippet":"In this paper a new pattern recognition based algorithm is presented to detect high impedance fault (HIF) in distribution networks. In this method, using wavelet transform (WT), the time-frequency based features of the current waveform up to 6.25 kHz are calculated. To extract the best feature set of the generated time frequency features, two methods including principle component analysis (PCA) and linear discriminant analysis (LDA) are used and then support vector machines (SVM) is used as a classifier to distinguish the HIFs considering with and without broken conductor from other similar phenomena such as capacitor banks switching, no load transformer switching, load switching and harmonic loads considering induction motors, arc furnaces. The results show high accuracy of the proposed method in the detection task.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4763380"},{"name":"Exploring Quality Metrics to Support Defect Management Process in a Multi-site Organization - A Case Study","snippet":"In large software development projects, the number of defects can be considerably high and defect management can become even more challenging when the development is distributed over several sites. Defect reduction solutions and commonly agreed defect management methods are needed to handle the defects and to meet the target quality level of the software, measured by the number of open defects. In this study, a combination of three quality metrics was used to support the defect management process in four consecutive multi-site software development programs involving several hundred people, and the result was compared to a program not using the described quality criteria set. According to the results, defect closing speed was improved, the number of open defects was reduced, and defects were reported earlier in programs that were using the quality metrics.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4700326"},{"name":"The research on the relation between magnetic leakage signal character and defect character","snippet":"In magnetic flux leakage (MFL) testing of pipelines, quantitatively classifying defects remains to be a difficult problem. The mathematical model of MFL field is presented based on the finite element (FE) method, and two-dimensional axisymmetric FE model of MFL testing system is also established by adopting ANSYS software to simulate magnetostatics. The results show that different defects cause different signal. There are certain functions between the defect character and MFL signal character. Defect is quantitatively classified by MFL signal character.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4593331"},{"name":"Design aspects and pattern prediction for phased arrays with subarray position errors","snippet":"In modern array design, the antenna elements are often grouped into mechanical units such as printed antenna boards and mechanical subarrays\/multipacks. This contributes to a more cost efficient manufacturing process and facilitates integration, handling, reuse and exchange of units, but it also makes the antenna element position errors correlated. Classical papers predict the statistical sidelobe level based on the assumption of uncorrelated errors, but using this for the general case, the statistical sidelobe level is under estimated. In this paper, the statistical sidelobe level for arrays with correlated position errors is predicted. Furthermore, rules of thumb relating antenna element position tolerances and mechanical array design to antenna array performance (sidelobe level) are given. Finally, array design aspects are discussed.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5613332"},{"name":"Genetic algorithms applied to optimal tolerance levels of multiattribute inspection errors","snippet":"In modern manufacturing environment, inspection equipment often can deal with more than one quality characteristic simultaneously. At the design stage of such inspection equipment, it is necessary to identify optimal combination of inspection error tolerance levels of multiattributes. We suggest a genetic algorithm by which one can determine the optimal tolerance levels of errors for multiinspection attributes at a minimum cost of ownership (COO). The COO model is formulated as a function of not only the initial purchase cost but also the inspection cost over lifetime. Our approach is expected to effectively contribute to marketing as well as manufacturing of inspection equipment.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1262388"},{"name":"Performance of restricted earth fault protection scheme in the presence of current transformer remanence","snippet":"In modern power system protection, an accurate transformation of primary short circuit current is vital to ensure correct operation of the high impedance Restricted Earth Fault (REF) protection scheme used for transformer protection. The dc component in the fault current as well as the remanent flux cans causes severe saturation conditions if the current transformer is not selected correctly. Behavior of the current transformer during the transient condition is important as this will determine the stability of the REF protection scheme. This paper discusses how the remanence phenomenon beside the dc component in the fault current can causes severe saturation to the current transformer and affects the stability of the REF protection scheme.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4762536"},{"name":"Stiffness and load free transmission error for the Multi-Flexible-Body-Dynamics (MFBD) Simulation of a wind turbine gearbox using a FE-based tooth contact analysis","snippet":"In most MFBD-systems it is possible to use a gear element to create a dynamic model of a wind turbine gearbox. This article shows a possibility to calculate the stiffness and the load free transmission error for this element using a FE-based tooth contact analysis.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4497320"},{"name":"Organized, automatic registration and analysis of performance and fault processes [in nuclear power plants]","snippet":"In nuclear power plants in Slovakia and in the Czech Republic, organized tests are being carried out on electric equipment, in order to verify their reliability. This paper introduces methods for the calculation of derived electric quantities","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=929257"},{"name":"Correction for continuous motion in small animal PET","snippet":"In small animal PET imaging experiments, animals are generally required to be anaesthetized to avoid motion artifacts. However, anaesthesia can alter biochemical pathways within the brain, thus affecting the physiological parameters under investigation. The ability to image conscious animals would overcome this problem and open up the possibility of entirely new investigational paradigms.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4774487"},{"name":"Implicit Social Network Model for Predicting and Tracking the Location of Faults","snippet":"In software testing and maintenance activities, the observed faults and bugs are reported in bug report managing systems (BRMS) for further analysis and repair. According to the information provided by bug reports, developers need to find out the location of these faults and fix them. However, bug locating usually involves intensively browsing back and forth through bug reports and software code and thus incurs unpredictable cost of labor and time. Hence, establishing a robust model to efficiently and effectively locate and track faults is crucial to facilitate software testing and maintenance. In our observation, some related bug locations are tightly associated with the implicit links among source files. In this paper, we present an implicit social network model using PageRank to establish a social network graph with the extracted links. When a new bug report arrives, the prediction model provides users with likely bug locations according to the implicit social network graph constructed from the co-cited source files. The proposed approach has been implemented in real-world software archives and can effectively predict correct bug locations.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4591547"},{"name":"Error Analysis of the Complex Kronecker Canonical Form","snippet":"In some interesting applications in control and system theory, i.e. in engineering, in ecology (Leslie population model), in financial\/actuarial (Leontief multi input - multi output) science, linear descriptor (singular) differential\/difference equations with time-invariant coefficients and (non-) consistent initial conditions have been extensively used. The solution properties of those systems are based on the Kronecker canonical form, which is an important component of the Matrix Pencil Theory. In this paper, we present some preliminary results for the error analysis of the complex Kronecker canonical form based on the Euclidean norm. Finally, under some weak assumptions an interesting new necessary condition is also derived.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5480451"},{"name":"Correction [to \"Taking Advantage of Mutual Coupling in Radio-Communication Systems Using a Multi-Port Antenna Array\" [Aug 07 208-220]","snippet":"In the above titled paper (ibid., vol. 49, no. 4, pp. 208-220, Aug 07), several items required correction, including equation (28). The corrections to the text and equation are presented here.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4455896"},{"name":"Mean-square-error reduction for quantized FIR filters","snippet":"In the article the author discuss fundamental properties of canonic signed digit (CSD) fixed point representation of numbers. Although properties of CSD format are well known from literature, published proofs are tedious and occupy lot of columns of text. Here the problem has been reduced to the problem of combinatorial number. The tool for this reduction is a \"drawer lemma\" - lemma about the distribution of identical objects in drawers or holes. Next there is proposed an algorithm for the computation and quantization of canonic signed digit (CSD) coefficients in a constant-coefficient multiplierless FIR filter. The algorithm is proven to be optimal in the mean square error sense. The algorithm is recurrent and unexpectedly simple, so it can be easily implemented inside any mathematical program as MATLAB or MATHCAD","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1653040"},{"name":"Curve fitting algorithm using iterative error minimization for sketch beautification","snippet":"In previous sketch recognition systems, curve has been fitted by a bit heuristic method. In this paper, we solved the problem by finding the optimal parameter of quadratic Bezier curve and utilize the error minimization between an input curve and a fitting curve by using iterative error minimization. First, we interpolated the input curve to compute the distance because the input curve consists of a set of sparse points. Then, we define the objective function. To find the optimal parameter, we assume that the initial parameter is known. Then, we derive the gradient vector with respect to the current parameter, and the parameter is updated by the gradient vector. This two steps are repeated until the error is not reduced. From the experiment, the average approximation error of the proposed algorithm was 0.946433 about 1400 synthesized curves, and this result demonstrates that the given curve can be fitted very closely by using the proposed fitting algorithm.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4761535"},{"name":"An integration of H.264 based Error Concealment technique and the SPLIT layer protocol","snippet":"In recent years Error Correction, Error Concealment, Error Control and Fairness techniques have been developed to counter the inevitable errors encountered on a non-ideal network. The Split-Layer Video Multicast Protocol (SPLIT) has been uniquely designed to take advantages of these techniques. Its primary advantage is its ability to conserve bandwidth while maintaining a level of satisfactory quality. The basic principle is to conceal errors that are created by dropping a layer, due to network congestion and recovering the quality using already developed error concealment techniques. It is the intention of this paper to research and develop this integration of SPLIT with an established error concealment algorithm.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1628352"},{"name":"A multi-path routing protocol with fault tolerance in mobile ad hoc networks","snippet":"In recent years many researches have focused on ad-hoc networks, mainly because of their independence to any specific structure. These networks suffers from frequent and rapid topology changes that cause many challenges in their routing. Most of the routing protocols try to find a path between source and destination nodes because any path will expire, offer a short period, the path reconstruction may cause the network inefficiency. The proposed protocol build two paths between source and destination and create backup paths during the route reply process, route maintenance process and local recovery process in order to improve the data transfer and the fault tolerance. The protocol performance is demonstrated by using the simulation results obtain from the global mobile simulation software(Glomosim). The experimental results show that this protocol can decrease the packet loss ratio rather than DSR and SMR and it is useful for the applications that need a high level of reliability.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5349359"},{"name":"Common Software-Aging-Related Faults in Fault-Tolerant Systems","snippet":"In recent years, remarkable attention has been focused on software aging phenomena, in which the performance of software systems degrades with time. Fault-tolerant software systems which provide high assurance may suffer from such phenomena. Based on the common software-aging-related faults in fault-tolerant systems, a behavior model of a double-version fault-tolerant software system is established using Markov reward model. The performance of the system such as expected service rate in steady state is evaluated and the sensitivity analysis of some parameters is performed.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5172646"},{"name":"Building a Transformer Defects Database for UHF Partial Discharge Diagnostics","snippet":"In the case of a defective transformer, when a partial discharge is detected and recorded, critical information can be deduced from its pattern, such as the type of defect, its criticality or even information on the level of degradation of the insulation. This information can help to determine the remaining life of the transformer and thus provide criteria for its maintenance and operation. In this paper different artificial PD patterns will be recorded in the laboratory, representative of specific transformer defects, in order to build a database for comparison purposes when measuring on-line. This can greatly improve the recognition and identification of the defect and thus help take some important life assessment conclusions on the transformer.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4538637"},{"name":"Extraction error modeling and automated model debugging in high-performance custom designs","snippet":"In the design cycle of high-performance integrated circuits, it is common that certain components are designed directly at the transistor level. This level of design representation may not be appropriate for test generation tools that usually require a model expressed at the gate level. Logic extraction is a key step in test model generation to produce a gate-level netlist from the transistor-level representation. This is a semi-automated process which is error-prone. Once a test model is found to be erroneous, manual debugging is required, which is a resource-intensive and time-consuming process. This paper presents an in-depth analysis of typical sets of extraction errors found in the test model representations of the pipelines in high-performance designs today. It also develops an automated debugging solution for single extraction errors for pipelines with no state equivalence information. A suite of experiments on circuits with similar architecture to that found in the industry confirms the fitness and practicality of the solution","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1661625"},{"name":"Learning fault-tolerance from nature","snippet":"In the last decade, there has been a considerable increase of interest in fault-tolerant computing due to dependability problems related to process scaling, embedded software, and ubiquitous computing. In this paper, we discuss an approach to fault-tolerance which is inspired by biological systems. Biological systems are capable of maintaining their functionality under a variety of genetic changes and external perturbations. They have natural self-healing, self-maintaining, self-replicating and self-assembling mechanisms. We present experimental and numerical evidence that the intrinsic fault-tolerance of biological systems is due to the dynamical phase in which the gene regulatory network operates. The dynamical phase is, in turn, determined by the subtle way in which redundancy is allocated in the network. By understanding the principles of redundancy allocation at the genetic level, we may find ways to build chips that possess the inherent fault-tolerance of biological systems.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4657478"},{"name":"3D-3 Classification of Defects for Guided Waves Inspected Pipes by a Neural Network Approach","snippet":"In this paper the effectiveness of a procedure that allows the flaws characterization of pipes inspected by a long range guided waves is investigated. The method performs the extraction of correlation coefficients between the x, y, z components of the displacement of simulated guided waves reflected by defects on pipes. These features feed a neural network classifier which evaluates the dimensions of well defined geometry defects on the pipe under test. The results show lower error rates in the evaluation of both angular and axial extent of a defect.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4409622"},{"name":"Fault detection techniques for effective line side asset monitoring","snippet":"In this paper the results of current research into the state-of-the-art in predictive fault detection and diagnosis methods for railway line-side assets is presented. Research to date has mainly focussed on point machines, track circuits and level crossing systems. It will be argued that, through the use of examples, that the most appropriate method for robust fault detection is based around generic models that are tuned for a particular instance of an asset. Furthermore, once a fault has been detected, it is necessary to have an a priori knowledge of the symptoms that are observable under fault conditions to reliably diagnose faults.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1514694"},{"name":"Faults digital library in burning process","snippet":"In this paper was developed a faults library for the steam boiler, the accent being on the study of the situations when faults occur in the burning process (furnace). This library is used in the monitoring and control system, the main object consisting in detection and localization of different faults. The system realise the fault detection in real time, comparison the process response with the response of the other models from the digital library and when are applied the same commands or signals in the same mode that in real process. In the last two sections of the paper was developed the application of fault detection algorithm based on plant model of burning process (included in this paper) and graphical results of fault detection experiments.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4588719"},{"name":"Correction of the Off-Axis Reflector Beam Squint in Passive Images of the Fourth Stokes Parameter at 91 GHz","snippet":"In this paper we analyze effects of the antenna beam squint on the fourth Stokes parameter V images taken by the fully polarimetric imager SPIRA and suggest methods to correct them. The first images of complex scenery, containing man- made and natural objects, showed unexpected features in the y-parameter with amplitudes of up to 30 K, depending on the contrast in the total intensity images. They have pronounced variation in elevation direction, whereas in azimuth (horizontal) only small changes are observed. A simple relation can be established between the measured fourth Stokes parameter and the scene brightness distributions in V and the total intensity I. It allows correction of the beam-squint effects in the V Stokes parameter image using image processing methods. Another, less general method, could be more easily applied to SPIRA images, achieving a comparable enhancement. In this way, the beam-squint effects were reduced down to the uncertainty of the instrumental polarimetric calibration.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4458880"},{"name":"Sampling error analysis applied to high-accuracy measurements","snippet":"In this paper we apply a mathematical analysis of the main error sources in sampling theory to estimate alias and integration errors in asynchronous digital sampling measurements.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4574795"},{"name":"On the structure and performance of a novel blind source separation based carrier phase synchronization error compensator","snippet":"In this paper we carry out a detailed performance analysis of a novel blind-source-separation (BSS) based DSP algorithm that tackles the carrier phase synchronization error problem. The results indicate that the mismatch can be effectively compensated during the normal operation as well as in the rapidly changing environments. Since the compensation is carried out before any modulation specific processing, the proposed method works with all standard modulation formats and lends itself to efficient real-time custom integrated hardware or software implementations.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1187251"},{"name":"Automatic correction of exposure problems in photo printer","snippet":"In this paper we consider a problem of automatic enhancement of amateur photos in photo printer. The purpose of correction consists of making photos more pleasant for an observer. The photos with various exposure problems and with poorly distinguishable details in shadow areas are analyzed. Our approach is based on contrast stretching and alpha-blending of both brightness of the initial image and estimations of reflectance. For obtaining reflectance estimation a simplified illumination model is used. The luminance is estimated using bilateral filter. Reflectance is estimated using heuristic functions of ratio between brightness of the initial image and estimation of luminance. The correction parameters are chosen adaptively based on histogram analysis. Noise suppression and some sharpening occur during correction. The time and memory optimization issues are considered. Look-up tables and recursive separable bilateral filter are applied to speed up the algorithm. The quality of the algorithm is evaluated by surveying of observer's opinions and by comparisons with already existing software and hardware solutions for local shadow correction. The proposed algorithm was implemented into firmware of Samsung dye-sublimation compact photo printer","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1689452"},{"name":"In-flight fault detection and isolation in aircraft flight control systems","snippet":"In this paper we consider the problem of test design for real-time fault detection and isolation (FDI) in the flight control system of fixed-wing aircraft. We focus on the faults that are manifested in the control surface elements (e.g., aileron, elevator, rudder and stabilizer) of an aircraft. For demonstration purposes, we restrict our focus on the faults belonging to nine basic fault classes. The diagnostic tests are performed on the features extracted from fifty monitored system parameters. The proposed tests are able to uniquely isolate each of the faults at almost all severity levels. A neural network-based flight control simulator, FLTZreg, is used for the simulation of various faults in fixed-wing aircraft flight control systems for the purpose of FDI","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1559659"},{"name":"Experimental evaluation of differential thermal errors in magnetoelastic stress sensors for Re<180","snippet":"In this paper we discuss laboratory results from solenoidal magnetoelastic measurements performed on a sample of bridge suspension cable. The experiments were designed to answer basic questions regarding the potential effects of inhomogeneous temperature fields and differential thermal errors between sensor and sample on the accuracy of coercive force measurements. The experiments were conducted with a sample of suspension cable installed in a temperature-controlled thermal chamber. Internal cable temperatures were measured with fine thermocouples which did not breach the sheath, permitting knowledge of the temperature field to be obtained without altering heat transfer. Two sets of experiments were conducted. First, magnetic measurements were taken with the sensor\/cable in thermal equilibrium at two different temperatures, and at intermediate points when the temperatures of different ferromagnetic components of the system varied. The magnitude of different heat transfer effects were quantified, and correlated with estimates of the coercive force. Secondly, in order to judge the absolute accuracy of the measurements and to obtain data for further optimizing computer simulations, reference magnetic data was measured using two techniques on a single wire sample of the cable steel. These results are an important step in quantifying the overall accuracy of these sensors.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1037211"},{"name":"Failure semantics of mobile agent systems involved in network fault management","snippet":"In this paper we examine what failure semantics are desirable for services provided by a mobile agent system (MAS) when assuming the MAS to be part of a network fault management system. We also present results from an evaluation project where the failure semantics of state of the art MAS were examined","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=830446"},{"name":"Worst case reliability prediction based on a prior estimate of residual defects","snippet":"In this paper we extend an earlier worst case bound reliability theory to derive a worst case reliability function R(t), which gives the worst case probability of surviving a further time t given an estimate of residual defects in the software N and a prior test time T. The earlier theory and its extension are presented and the paper also considers the case where there is a low probability of any defect existing in the program. For the \"fractional defect\" case, there can be a high probability of surviving any subsequent time t. The implications of the theory are discussed and compared with alternative reliability models.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1173274"},{"name":"Analyzing and Improving the Simulation Algorithm of IEEE 802.11DCF Error Frame Model in QualNet Simulator","snippet":"In this paper we focus on the concept of IEEE 802.11 DCF error frame model. Based on the analysis of the model in detail, we point out the error of the simulation algorithm of the DCF error frame model in QualNet simulation environment. Furthermore, we modify the simulation algorithm in QualNet strictly according to the specifications of the DCF error frame model. The simulation results show that compared with the original simulation algorithm, the modified algorithm can simulate the related specifications of the DCF error frame model more correctly, thus can provide more reliable simulation results.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5670858"},{"name":"A Rule Based Approach for Skew Correction and Removal of Insignificant Data from Scanned Text Documents of Devanagari Script","snippet":"In this paper we have presented a rule based approach for removing insignificant data and skew from scanned documents of Devanagari script. To develop an OCR system for Devanagari script is not an easy job hence proper preprocessing of these scanned documents requires noise removal and correcting skew from the image. The proposed system is based on rule based methods, morphological operations and connected component labeling. Images used for the experiment are binarised grayscale images. Experiments and results show that presented method is robust for preprocessing scanned images of Devanagari text documents.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4618869"},{"name":"Topological Properties of a New Fault Tolerant Interconnection Network for Parallel Computer","snippet":"In this paper we introduce a new interconnection network, the extended varietal hypercube with cross connection denoted by EVHC(n,k). This network has hierarchical structure and it overcomes the poor fault tolerant properties of extended varietal hypercube. This network has low diameter, constant degree connectivity and low message traffic density.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4731294"},{"name":"Error resilience tools in the MPEG-4 and H.264 video coding standards","snippet":"In this paper we introduce error resilience tools which are used by MPEG-4 and H. 264 video coding standards. MPEG-4 as well as H.264 offers error resilience tools which were implemented by older video coding standards, but they also introduce new tools and improved efficiency of previously used tools. This paper offers only brief review of them, but it focuses on their most important characteristics.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4542722"},{"name":"Closed-Form Error Analysis of Dual-Hop Relaying Systems over Nakagami-m Fading Channels","snippet":"In this paper we investigate the end-to-end performance of dual-hop relaying systems over non identical-Nakagami-m fading channels. Our analysis considers channel state information (CSI-) assisted relays that just amplify and retransmit the information signal also known as \"non-regenerative\" relays. New closed-form expressions for the average bit error probability (ABEP) are derived. The proposed expressions apply to general operating scenarios with distinct Nakagami-m fading parameters and average signal to noise ratios (SNRs) between the hops. When the fading parameter is an odd multiple of one half, the ABEP is expressed in terms of hypergeometric functions. When m takes any real non integer value, the obtained results involve the fourth Appell's hypergeometric function. For an arbitrary fading parameter, an analysis of such a scheme is performed using the well known moment-based approach.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5506419"},{"name":"Unified Architectural Support for Soft-Error Protection or Software Bug Detection","snippet":"In this paper we propose a unified architectural support that can be used flexibly for either soft-error protection or software bug detection. Our approach is based on dynamically detecting and enforcing instruction- level invariants. A hardware table is designed to keep track of run-time invariant information. During program execution, instructions access this table and compare their produced results against the stored invariants. Any violation of the predicted invariant suggests a potential abnormal behavior, which could be a result of a soft error or a latent software bug. In case of a soft error, monitoring invariant violations provides opportunistic soft-error protection to multiple structures in processor pipelines. Our experimental results show that invariant violations detect soft errors promptly and as a result, simple pipeline squashing is able to fix most of the detected soft errors. Meanwhile, the same approach can be easily adapted for software bug detection. The proposed architectural support eliminates the substantial performance overhead associated with software-based bug-detection approaches and enables continuous monitoring of production code.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4336201"},{"name":"Fault tolerant multipath routing with overlap-aware path selection and dynamic packet distribution on overlay network for real-time streaming applications","snippet":"In this paper we propose overlap-aware path selection and dynamic packet distribution due to failure detection in multipath routing overlay network. Real-time communications that utilize UDP do not ensure reliability for realizing fast transmission. Therefore congestion or failure in a network deteriorates the quality of service significantly. The proposed method seeks an alternate path that hardly overlaps an IP path so as to improve its reliability. The proposed method also detects congestion or failure by differential of packet loss rate and apportion packets to the IP path and the alternate path dynamically. Evaluation on PlanetLab shows the proposed method avoids congestion Consequently the influence of congestion and failure lessens and the proposed multipath routing improves the reliability which can be used for real-time communications.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4638724"},{"name":"Early resynchronization, error detection and error concealment for reliable video decoding","snippet":"In this paper we proposed a robust video decoding scheme using early resynchronization, error detection (ED) and error concealment (EC). Many video coding standards such as H.263+ and MPEG-2 make the compressed video signals more vulnerable to channel errors because of error propagation due to using variables length coding (VLC). Channel errors cannot only result in false decoding of current bits but also make the decoder lose synchronization. The data between the errors and next resynchronization marker (RM) will be discarded even if they are not corrupted by channel errors. Early resynchronization can retrieve unaffected data between the errors and next RM, which can greatly reduce error propagation due to using VLC. We combined early resynchronization with deliberately designed ED and EC technologies and tested three early resynchronization schemes in our simulation. It has been found that the third scheme can give the best tradeoff between the final reconstructed image quality and searching times.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1209730"},{"name":"A note on fault diagnosis algorithms","snippet":"In this paper we review algorithms for checking diagnosability of discrete-event systems and timed automata. We point out that the diagnosability problems in both cases reduce to the emptiness problem for (timed) BuAchi automata. Moreover, it is known that, checking whether a discrete-event system is diagnosable, can also be reduced to checking bounded diagnosability. We establish a similar result for timed automata. We also provide a synthesis of the complexity results for the different fault diagnosis problems.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5399968"},{"name":"A new scheduling algorithm for dynamic task and fault tolerant in heterogeneous grid systems using Genetic Algorithm","snippet":"In this paper with studying of all parameters in grid environment a new scheduling algorithm for independent task is introduced according to Genetic Algorithm. This algorithm can be more efficient and more dependable than similar previous algorithms. The simulated results and reasons for reaching to better makespan and more efficiency in the grid environment. In the grids with high fault with high fault rate for fault tolerant is used from check point method that has more efficiency that other methods such as retry, migration and replication. This method maintains effective efficiency in these situations. So the servicing quality increases in various grid environments and also the average time of task recoveries decreases considerably The main purpose of this paper is reducing the repeating of the generations in Genetic Algorithm for reaching higher speed and also considering the communications costs (available in fitness function) with maintaining the fitness efficiency. The simulations are done with Gridsim for showing the created improvement at proposed algorithm rather than previous algorithms.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5564753"},{"name":"JPEG 2000 backward compatible error protection with Reed-Solomon codes","snippet":"In this paper, a backward compatible header error protection mechanism is described. It consists of the addition of a dedicated marker segment to a JPEG 2000 codestream, that will contain the error correction data generated by a block error correction code (e.g. a Reed Solomon code). This mechanism allows leaving the original data intact, hence providing backward compatibility with the already standardised JPEG 2000. Neither side information from higher level, nor extra signalling encapsulation is needed, as the required information is directly embedded in the codestream and also protected. Finally, it is shown how this mechanism can be used for perform unequal error protection of the whole JPEG 2000 stream.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1261166"},{"name":"Evaluation of two algorithms of multiple corrections on complementarity conditions","snippet":"In this paper, a careful analysis and evaluation for two approaches based on multiple corrections on complementarity conditions in the nonlinear predictor-corrector primal-dual interior point algorithm (PCPDIPA) is carried out. Unlike the other methodologies that improve the performance of PCPDIPA from the perspective of problem modeling\/formulation, these two approaches directly tackle the centrality issue of the algorithm by re-using the available matrix factorization. Several parameters controlling the performance of these two approaches are identified by a series of numerical testing on several large-scale cases. Numerical comparison results are provided as indices for choosing appropriate centrality correction methodology for other power system applications.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1356299"},{"name":"Fault Diagnosis Implementation of Induction Machines based on Advanced Digital Signal Processing Techniques","snippet":"In this paper, a comprehensive cross correlation-based fault diagnostic method is proposed for real time DSP implementation. It covers both fault monitoring and decision making stages. In practice, a motor driven by an adjustable speed drive is run at various operating points where the frequency, amplitude and phase of the fault signatures varies with time. These dynamic changes are considered as one of the common factor that yields erroneous fault tracking and unstable fault detection. In this paper, the proposed algorithms deals with the operating point dependent ambiguities and threshold issues. It is theoretically and experimentally verified that the motor fault can continuously be tracked when the operating point changes within a limited range.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4802778"},{"name":"Fuzzy Time-Frequency defect classifier for NDT applications","snippet":"In this paper, a customized classifier is presented for the industry-practiced nondestructive evaluation (NDE) protocols using a hybrid-fuzzy inference system (FIS) to classify the and characterize the defects commonly present in the steel pipes used in the gas\/petroleum industry. The presented system is hybrid in the sense that it utilizes both soft computing through fuzzy set theory, as well as conventional parametric analysis through time-frequency (TF) methods. Various TF transforms have been tested and the most suitable one for this application, multiform tiltable exponential distribution (MTED), is presented here. Four defining states are considered in the paper; slag, porosity, crack, and lack-of-fusion, representing the four most critical types of defects present in welds on the pipes. The necessary features are calculated using the TF coefficients and are then supplied to the fuzzy inference system as input to be used in the classification. The resulting system has shown excellent defect classification with very low misclassification and false alarm rates.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5407574"},{"name":"On the Optimization of Local and End-to-end Forward Error Correction","snippet":"In this paper we investigate where best to use forward error correction if one or more mobile radio links is included on a path. On one hand, if the errors are handled locally where they appear, the knowledge of the channel conditions are better and no extra redundancy will have to traverse the other links. On the other hand, the requirements of the application are better known at the end nodes, hence the error correction can be better tuned to the needs of the application end-to-end. The first aspect investigated is the effect of a correlated error process, which is exemplified by a fading radio channel. The length of the error correcting code and the burst tolerance is essential for performance when correlation is taken into account, therefore packet level coding applied endto- end is efficient at high correlation whereas local bit error correction is more efficient at low correlation. TCP is used to exemplify the difficulties in estimating the parameters needed to implement effective coding both locally and end-to-end. Furthermore, simulation results show how the locally optimal parameters differ depending on the end-to-end path. Performance comparisons in several cases demonstrate that end-to-end forward error correction (FEC) can often be efficient to mitigate problems that are in principle local to a specific link.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5755358"},{"name":"Memory fault diagnosis by syndrome compression","snippet":"In this paper we present a data compression technique that can be used to speed up the transmission of diagnosis data from the embedded RAM with built-in self-diagnosis (BISD) support. The proposed approach compresses the faulty-cell address and March syndrome to about 28% of the original size under the March-17N diagnostic test algorithm. The key component of the compressor is a novel syndrome-accumulation circuit, which can be realized by a content-addressable memory. Experimental results show that the area overhead is about 0.9% for a 1Mb SRAM with 164 faults. The proposed compression technique reduces the time for diagnostic test, as well as the tester storage capacity requirement","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=915007"},{"name":"An Efficient Fault-Tolerant Routing Methodology for Meshes and Tori","snippet":"In this paper we present a methodology to design fault-tolerant routing algorithms for regular direct interconnection networks. It supports fully adaptive routing, does not degrade performance in the absence of faults, and supports a reasonably large number of faults without significantly degrading performance. The methodology is mainly based on the selection of an intermediate node (if needed) for each source-destination pair. Packets are adaptively routed to the intermediate node and, at this node, without being ejected, they are adaptively forwarded to their destinations. In order to allow deadlock-free minimal adaptive routing, the methodology requires only one additional virtual channel (for a total of three), even for tori. Evaluation results for a 4 x 4 x 4 torus network show that the methodology is 5-fault tolerant. Indeed, for up to 14 link failures, the percentage of fault combinations supported is higher than 99.96%. Additionally, network throughput degrades by less than 10% when injecting three random link faults without disabling any node. In contrast, a mechanism similar to the one proposed in the BlueGene\/L, that disables some network planes, would strongly degrade network throughput by 79%.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1650124"},{"name":"Nonparametric model error bounds for control design in the presence of nonlinear distortions","snippet":"In this paper we present a procedure to generate nonparametric bounds on the model errors of a measured frequency response function that are due to nonlinear distortions. In a first step the nonlinear system is represented by a linear system (the best linear approximation) plus a noise source that accounts for the unmodelled nonlinear effects. In a second step, the model error bounds are calculated starting from the measured noise source characteristics. The whole process is embedded in a simple and time efficient experimental procedure","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=980733"},{"name":"Analysis of stator short-circuit faults for induction machine using finite element modeling","snippet":"In this paper we present an analysis on the different types of short-circuit (sc) that may affect the stator windings by means of a finite element model. Three cases of short-circuit were simulated on the electrical circuit of stator. The machine is modelled in magneto-evolving.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5585589"},{"name":"Design optimization of time- and cost-constrained fault-tolerant distributed embedded systems","snippet":"In this paper we present an approach to the design optimization of fault tolerant embedded systems for safety-critical applications. Processes are statically scheduled and communications are performed using the time-triggered protocol. We use process re-execution and replication for tolerating transient faults. Our design optimization approach decides the mapping of processes to processors and the assignment of fault-tolerant policies to processes such that transient faults are tolerated and the timing constraints of the application are satisfied. We present several heuristics which are able to find fault-tolerant implementations given a limited amount of resources. The developed algorithms are evaluated using extensive experiments, including a real-life example.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1395691"},{"name":"Fault Recognition on Power Networks via SNR Analysis","snippet":"In this paper we present and to an extent analyze, findings from a real installation of broadband over power lines done by the Greek Public Power Corporation in the region of Larissa. The findings indicate the correlation between faulty equipment on the power networks and the signal-to-noise ratio of the power-line signal induced thus showing the possibility of developing a new method of fault recognition in power networks.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5235861"},{"name":"Design and implementation of a pluggable fault tolerant CORBA infrastructure","snippet":"In this paper we present the design and implementation of a Pluggable Fault Tolerant CORBA Infrastructure that provides fault tolerance for CORBA applications by utilizing the pluggable protocols framework that is available for most CORBA ORBS. Our approach does not require modification to the CORBA ORB, and requires only minimal modifications to the application. Moreover; it avoids the difficulty of retrieving and assigning the ORB state, by incorporating the fault tolerance mechanisms into the ORB. The Pluggable Fault Tolerant CORBA Infrastructure achieves performance that is similar to, or better than, that of other Fault Tolerant CORBA systems, while providing strong replica consistency.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1015513"},{"name":"Fault detection, diagnosis and control in a tactical aerospace vehicle","snippet":"In this paper we propose a fault-tolerant control ( FTC ) scheme using multiple controller switching. The performance of this scheme is studied on a tactical aerospace vehicle. A parity space (PS) based residual generation approach is used to detect the fault. Once a fault is detected the diagnosis scheme identifies the faulty actuator. Using this information on-line reconfiguration of the controller is done based on the configuration of the existing healthy actuator. To implement this scheme no modification were done in hardware (H\/W) configuration and only existing redundancies were utilised. Simulation with nonlinear 6-degree of freedom (6-DoF) model shows that the above fault tolerant control approach is able to reduce the probability of failure due to actuators.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1273138"},{"name":"Intelligent fault diagnosis system based on UML","snippet":"In this paper, it is united that the ordinary software project method and the object-oriented method. The object-oriented analysis, the object-oriented design and the object-oriented modeling in the intelligent fault diagnosis system with UML is introduced, which decreases the complexity of the intelligent fault diagnosis system that is more manageable. The diagnostic reasoning adopts the technique of expert system and reasoning under uncertainty. The intelligent fault diagnosis system is a very important part of the power station simulation system. With the data gathered from the devices or the converted parameters, it can find the devices which are out of order by reasoning. The diagnostic reasoning adopts the technique of expert system and reasoning under uncertainty.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5413067"},{"name":"Joint Fault-Tolerant Design of the Chinese Space Robotic Arm","snippet":"In this paper, joint reliability design for the Chinese space robotic arm has been discussed. Redundant controller unit, redundant can bus communication unit and latch-up power protection unit have been outlined. The fault tree of the joint has been built. Moreover, the new algorithm of auto-adjust thresholding value has been presented for fault detection, and the fault-tolerant strategies of joint have been proposed. Experimental results demonstrate the effectiveness of the joint fault-tolerant design","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4097993"},{"name":"Error density metrics for business process model","snippet":"In this paper, metrics for business process model (BPM), are proposed, which are capable to measure the usability and effectiveness of BPMs. The proposed model is adapting error density metrics to BPMs by considering the similarities between the conceptual characteristics of BPMs and software products. We applied seven software metrics for evaluating quality of business processes\/ process models. Results show that our metrics help the organization to improve their process, as weighted measurements are indicators for unexpected situations\/behaviour for business processes.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5291871"},{"name":"Power system fault diagnosis modeling techniques based on encoded Petri nets","snippet":"In this paper, power system fault diagnosis based on Petri nets and coding theory is further studied. Previous research work is briefly reviewed. Characteristics of Petri nets model in power system fault diagnosis and identification are demonstrated in detail, also a fast model revision algorithm of power components is proposed, which makes the scheme more applicable to large-scale power network. The method is tested in the IEEE 118-bus power system and simulation results show that the suggested approach is accurate by using of error correction theory, model revision is easy, fast when power network is expanded or topology is changed, which makes the encoded Petri nets method more applicable in real power system","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1709100"},{"name":"Threshold calculation using LMI-technique and its integration in the design of fault detection systems","snippet":"In this paper, problems related to the threshold calculation and its integration in the design of observer-based fault detection systems are studied. The focus of the study is the application of practical fault detection methods in the framework of observer-based fault detection schemes. The basic idea consists in the formulation of threshold calculation as some standard optimization problems which are then solved using the well-established LMI-optimization technique.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1272607"},{"name":"Channel frame error rate for Bluetooth in the presence of microwave ovens","snippet":"In this paper, radiation from microwave ovens is measured using PRISM, a custom-built device designed to measure transmissions in the ISM band. The measured signals, treated as a rising noise floor, are then applied to a semi-analytic simulation to determine the probability of frame error rate (FER) per channel for six Bluetooth packet types.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1040736"},{"name":"State Estimation and Fast Fault Detection For Ship Electrical Systems","snippet":"In this paper, the advantages of state estimation on ship power systems are demonstrated and extended to perform fast fault detection. For more-electric ships it becomes critical to monitor the system and respond to faults within milliseconds to limit damage from the fault and transfer to an intact supply. A compact ship allows syncronization of sampled real time voltage and current data from electrical sensors to compute the phase angle and voltage magnitude at every bus on the system. On a ship power system the data samples can be collected every 0.5 milliseconds by the central computer. Bad data analysis, smoothed values for the operating condition, and differential current sensing are performed within milliseconds. An example of state estimation is done for an LPD17 assault ship. Other examples of using phase data and fault detection are given in the paper. Algorithms for remedial action may be activated by the results of the state estimation and fault detection.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4233823"},{"name":"Fundamental Limitations on Designing Optimally Fault-Tolerant Redundant Manipulators","snippet":"In this paper, the authors examine the problem of designing nominal manipulator Jacobians that are optimally fault tolerant to one or more joint failures. Optimality is defined here in terms of the worst-case relative manipulability index. While this approach is applicable to both serial and parallel mechanisms, it is especially applicable to parallel mechanisms with a limited workspace. It is shown that a previously derived inequality for the worst-case relative manipulability index is generally not achieved for fully spatial manipulators and that the concept of optimal fault tolerance to multiple failures is more subtle than previously indicated. Lastly, the authors identify the class of 8-DOF Gough--Stewart platforms that are optimally fault tolerant for up to two joint failures. Examples of optimally fault-tolerant 7- and 8-DOF mechanisms are presented.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4623141"},{"name":"On the Use of Behavioral Models for the Integrated Performance and Reliability Evaluation of Fault-Tolerant Avionics Systems","snippet":"In this paper, the authors propose an integrated methodology for the reliability and performance analysis of fault-tolerant systems. This methodology uses a behavioral model of the system dynamics, similar to the ones used by control engineers when designing the control system, but incorporates additional artifacts to model the failure behavior of the system components. These artifacts include component failure modes (and associated failure rates) and how those failure modes affect the dynamic behavior of the component. The methodology bases the system evaluation on the analysis of the dynamics of the different configurations the system can reach after component failures occur. For each of the possible system configurations, a performance evaluation of its dynamic behavior is carried out to check whether its properties, e.g., accuracy, overshoot, or settling time, which are called performance metrics, meet system requirements. After all system configurations have been evaluated, the values of the performance metrics for each configuration and the probabilities of going from the nominal configuration (no component failures) to any other configuration are merged into a set of probabilistic measures of performance. To illustrate the methodology, and to introduce a tool that the authors developed in MATLAB\/SIMULINKreg that supports this methodology, the authors present a case-study of a lateral-directional flight control system for a fighter aircraft","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4106294"},{"name":"Intraoperative ultrasonography for the correction of brainshift based on the matching of hyperechogenic structures","snippet":"In this paper, a global approach based on 3D freehand ultrasound imaging is proposed to (a) correct the error of the neuronavigation system in image-patient registration and (b) compensate for the deformations of the cerebral structures occurring during a neurosurgical procedure. The rigid and non rigid multimodal registrations are achieved by matching the hyperechogenic structures of brain. The quantitative evaluation of the non rigid registration was performed within a framework based on synthetic deformation. Finally, experiments were carried out on real data sets of 4 patients with lesions such as cavernoma and low-grade glioma. Qualitative and quantitative results on the estimated error performed by neuronavigation system and the estimated brain deformations are given.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5490261"},{"name":"A High Speed and Low Cost Error Correction Technique for the Carry Select Adder","snippet":"In this paper, a high speed and low cost error correction technique is proposed for the Carry Select Adder (CSA) which can correct both transient and permanent errors and is applicable on all partitioning types of the basic CSA circuit. The proposed error correction technique is compatible with all existing error detection techniques which are proposed for the CSA adder. The synthesized results show that applying this novel error correction technique to a CSA with error detection technique results in up to 18.4%, 3.1% and 14.9%, increase in power consumption, delay and area respectively.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5066539"},{"name":"Detection and treatment of faults in automated machines based on Petri nets and Bayesian networks","snippet":"In this paper, a methodology for considering detection and treatment of faults in automated machines is introduced. This methodology is based on the integration of Petri nets for diagnosis (BPN) and Bayesian networks. After that, the integration among detection\/treatment of faults and the \"normal\" processes (represented by Petri nets, PN) is possible. This integration allows us to develop a fault tolerant supervisor, which considers all the processes in the same structure. A case study of fault tolerant AGV is considered. Finally, a simulation tool for edition and analysis of models with these characteristics is introduced.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1267910"},{"name":"Embedded model-based fault diagnosis for on-board diagnosis of engine control systems","snippet":"In this paper, a model-based fault diagnosis scheme for on-board diagnosis in spark ignition (SI) engine control systems is presented. The developed fault diagnosis system fully makes use of the available control structure and is embedded into the control loops. As a result, the implementation of the diagnosis system is realized with low demands on engineering costs, computational power and memory. The developed diagnosis scheme has been successfully applied to the air intake system of an SI-engine","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1507295"},{"name":"Exact Fault-Tolerant Feasibility Analysis of Fixed-Priority Real-Time Tasks","snippet":"In this paper, a necessary and sufficient (exact) feasibility test is proposed for fixed-priority scheduling of a periodic task set to tolerate multiple faults on uniprocessor. We consider a fault model such that multiple faults can occur in any task and at any time, even during recovery operations. The proposed test considers tolerating a maximum of f faults that can occur within any time interval equal to the largest relative deadline of the task set. The feasibility of the task set is checked based on the maximum workload requested by the higher-priority jobs within the released time and deadline of the job of each task that is released at the critical instant. The maximum workload is calculated using a novel technique to compose the execution time of the higher-priority jobs. To the best of our knowledge, no other work (assuming the same fault model as ours) has derived an exact feasibility test for periodic task sets having a lower time complexity than that of the test proposed in this paper.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5591653"},{"name":"Induction Motor Electrical Fault Diagnosis Using Voltage Spectrum of an Auxiliary Winding - Part II","snippet":"In this paper, a new method for induction motor fault diagnosis is presented. It is based on the so-called voltage spectrum of an auxiliary small winding inserted between two of the stator phases. An expression of the inserted inductance voltage is presented. After that, discrete Fourier transform analyzer is required for converting the voltage signal from the time domain to the frequency domain. Simulations results curried out for defected and non defected motor show the effectiveness of the proposed method.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4570408"},{"name":"A low complexity and efficient slice grouping method for H.264\/AVC in error prone environments","snippet":"In this paper, a new method is proposed for Macroblock (MB) importance classification of inter frames. Instead of selecting the most important MBs, the least important MBs are decided first. It makes use of the properties of skip mode in the H.264\/AVC standard as the first step. Because the number of MBs chosen as skip mode in a frame varies, further classification is usually required. Four other different features therefore are considered to determine the Important Factor of the remaining MBs. It has been proved that the proposed method can provide good objective and subjective video quality performance, whilst also being simple and fast.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5031443"},{"name":"Using run-time reconfiguration for fault injection in hardware prototypes","snippet":"In this paper, a new methodology for the injection of single event upsets (SEU) in memory elements is introduced. SEUs in memory elements can occur due to many reasons (e.g. particle hits, radiation) and at any time. It becomes therefore important to examine the behaviour of circuits when an SEU occurs in them. Reconfigurable hardware (especially FPGAs) was shown to be suitable to emulate the behaviour of a logic design and to realise fault injection. The proposed methodology for SEU injection exploits FPGAs and, contrarily to the most common fault injection techniques, realises the injection directly in the reconfigurable hardware, taking advantage of run-time reconfiguration capabilities of the device. In this case, no modification of the initial design description is needed to inject a fault, that results in avoiding hardware overheads and specific synthesis, place and route phases.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1173521"},{"name":"Harmonic-suppressed Elliptic-Function Low-pass Filter Using Defected Ground Structure","snippet":"In this paper, a new microstrip elliptic-function low-pass filter(LPF) using a defected ground structure(DGS) is proposed. By integrating DGS units, the proposed filter provides attenuation- pole for the widc-stopband characteristic due to the resonance characteristic of DGS. The equivalent-circuit model and the 3-D model simulation for the DGS are used to determine the dimension parameters. Etching the designed DGS on the backside of an interdigital elliptic-function LPF, a new microstrip LPF with broad stopband is obtained after- impedance matching. The experimental results show the new filter can suppress the harmonic signal very well.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4393639"},{"name":"Remote sensing digital image processing techniques in active faults survey","snippet":"In this paper, an effective method is presented to identify active faults from different sources of remote sensing images. First, we compared the capability of some satellite sensors in active faults survey. Then, we discussed a few digital image processing approaches used for information enhancement and feature extraction related to faults. Those methods include band ratio, PCA (Principal Components Analysis), Tasseled Cap Transformation, filtering and texture statistics, etc. Extensive experiments were implemented to validate the efficiency of those methods. We collected Landsat MSS, TM and ETM Plus images of Shandong Province, northern China. DEM (Digital Elevation Model) data of 25 m resolution and Chinese resource satellite-Resource-2 images with pixel size of about 5 m are also acquired in very important active faults regions. The experimental results show that remote sensing multi-spectral images have great potentials in large scale active faults investigation. We also get satisfied results when deal with invisible faults those lying beneath the earth surface.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1294452"},{"name":"Error resilience video coding in H.264 encoder with potential distortion tracking","snippet":"In this paper, an efficient rate-distortion (RD) model for an H.264 video encoder in a packet loss environment is presented. The encoder keeps tracking the potential error propagation on a block basis by taking into account the source characteristics, network conditions as well as the error concealment method. The end-to-end distortion invoked in this RD model is estimated according to the potential error-propagated distortion stored in a distortion map. The distortion map, in terms of each frame, is derived after the frame is encoded, which can be used for the RD-based encoding of the subsequent frames. Since the channel distortion has been considered in the proposed RD model, the new Lagarangian parameter is derived accordingly. The proposed method outperforms the error robust rate-distortion optimization method in the H.264 test model better in terms of both transmission efficiency and computational complexity.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1418715"},{"name":"Extended Fault-Location Formulation for Power Distribution Systems","snippet":"In this paper, an extended impedance-based fault-location formulation for generalized distribution systems is presented. The majority of distribution feeders are characterized by having several laterals, nonsymmetrical lines, highly unbalanced operation, and time-varying loads. These characteristics compromise traditional fault-location methods performance. The proposed method uses only local voltages and currents as input data. The current load profile is obtained through these measurements. The formulation considers load variation effects and different fault types. Results are obtained from numerical simulations by using a real distribution system from the Electrical Energy Distribution State Company of Rio Grande do Sul (CEEE-D), Southern Brazil. Comparative results show the technique robustness with respect to fault type and traditional fault-location problems, such as fault distance, resistance, inception angle, and load variation. The formulation was implemented as embedded software and is currently used at CEEE-D's distribution operation center.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4797797"},{"name":"Research on monitoring and fault diagnostic system of turbine-generator","snippet":"In this paper, based on the systematic analysis on the structure, operation, and fault of a large generator, the software of MDST (monitoring and diagnosis system) is designed and compiled, and it is applied on the spot. The software MDST consults the national standard and the departmental standard and the correlative prescription of regulation of generator on operation, experiment, examination and reparation, and quality verification etc., and under full consideration of the factors, such as functional realization of the software, customer requirements, operator-computer interactive communication, friendly interface, authority partition, and secure and stable operation of the software and so on","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=973778"},{"name":"Software reliability modeling of fault detection and correction processes","snippet":"In this paper, both fault detection and correction processes are considered in software reliability growth modeling. The dependency of the two processes is first studied from the viewpoint of the fault number in two ways. One is the ratio of corrected fault number to detected fault number, which appears S-shaped. And the other is the difference between the detected fault number and corrected fault number, which appears Bell-shaped. Then based on the ratio and difference functions, two software reliability models are proposed for both fault detection and correction processes. The proposed models are evaluated by a data set of software testing. The experimental results show that the new models fit the data set of fault detection and correction processes very well.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4914730"},{"name":"On Unequal Error Protection of Convolutional Codes From an Algebraic Perspective","snippet":"In this paper, convolutional codes are studied for unequal error protection (UEP) from an algebraic theoretical viewpoint. We first show that for every convolutional code there exists at least one optimal generator matrix with respect to UEP. The UEP optimality of convolutional encoders is then combined with several algebraic properties, e.g., systematic, basic, canonical, and minimal, to establish the fundamentals of convolutional codes for UEP. In addition, a generic lower bound on the length of a UEP convolutional code is proposed. Good UEP codes with their lengths equal to the derived lower bound are obtained by computer search.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5361497"},{"name":"The Error Reduced ADI-CPML Method for EMC Simulation","snippet":"In this paper, convolutional perfectly matched layer (CPML) is developed for the recently proposed error reduced (ER) ADI-FDTD method to solve electromagnetic compatibility problems efficiently. Its numerical results are examined and compared with the conventional ADI-CPML method. It is found that for a CFL number equal to 5, the reflection error of the ER- ADI-CPML is approximately 12 dB better than the conventional ADI-CPML method.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4305689"},{"name":"Fault Detection Using Differential Flatness in Flight Guidance Systems","snippet":"In this paper, flight guidance dynamics are shown to be implicit differentially flat with respect to the inertial position of an aircraft. This proves the existence of a set of relations between these flat outputs and the state variables representative of flight guidance dynamics and between these flat outputs and the basic inputs to flight guidance dynamics. A neural network is introduced to obtain, from the actual trajectory, nominal flight parameters which can be compared with actual values to detect abnormal behaviour","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4159728"},{"name":"Spatio-temporal boundary matching algorithm for temporal error concealment","snippet":"In this paper, a novel temporal error concealment algorithm, called spatio-temporal boundary matching algorithm (STBMA), is proposed to recover the information lost in the video transmission. Different from the classical boundary matching algorithm (BMA), which just considers the spatial smoothness property, the proposed algorithm introduces a new distortion function to exploit both the spatial and temporal smoothness properties to recover the lost motion vector (MV) from candidates. The new distortion function involves two terms: spatial distortion term and temporal distortion term. Since both the spatial and temporal smoothness properties are involved, the proposed method can better minimize the distortion of the recovered block and recover more accurate MV. The proposed algorithm has been tested on H.264 reference software JM 9.0. The experimental results demonstrate the proposed algorithm can obtain better PSNR performance and visual quality, compared with BMA which is adopted in H.264","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1692678"},{"name":"Three-Stage Error Concealment for Distributed Speech Recognition (DSR) with Histogram-Based Quantization (HQ) Under Noisy Environment","snippet":"In this paper, a three-stage error concealment (EC) framework based on the recently proposed histogram-based quantization (HQ) for distributed speech recognition (DSR) is proposed, in which noisy input speech is assumed and both the transmission errors and environmental noise are considered jointly. The first stage detects the erroneous feature parameters at both the frame and subvector levels. The second stage then reconstructs the detected erroneous subvectors by MAP estimation, considering the prior speech source statistics, the channel transition probability, and the reliability of the received subvectors. The third stage then considers the uncertainty of the estimated vectors during Viterbi decoding. At each stage, the error concealment (EC) techniques properly exploit the inherent robust nature of histogram-based quantization (HQ). Extensive experiments with AURORA 2.0 testing environment and GPRS simulation indicated the proposed framework is able to offer significantly improved performance against a wide variety of environmental noise and transmission error conditions.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4218241"},{"name":"Concurrent error detection for involutional functions with applications in fault-tolerant cryptographic hardware design","snippet":"In this paper, a time redundancy based Concurrent Error Detection (CED) technique targeting involutional functions is presented. A function F is an involution if F(F(x))=x. The proposed CED technique exploits the involution property and checks if x=F(F(x)). Unlike traditional time redundancy based CED methods, this technique can detect both permanent and transient faults","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1629149"},{"name":"A new adaptive hybrid neural network and fuzzy logic based fault classification approach for transmission lines protection","snippet":"In this paper, an adaptive hybrid neural networks and fuzzy logic based algorithm is proposed to classify fault types in transmission lines. The proposed method is able to identify all ten shunt faults in transmission lines with high level of robustness against variable conditions such as measured amplitudes and fault resistance. In this approach, a two-end unsynchronized measurement of the signals is used. For real-time estimation of unknown synchronization angle and three phase phasors a two-layer adaptive linear neural (ADALINE) network is used. The estimated parameters are fed to a fuzzy logic system to classify fault types. This method is feasible to be used in digital distance relays which are able to be programmed, to share and discourse data with all protective and monitoring devices. The proposed method is evaluated by a number of simulations conducted in PSCAD\/EMTDC and MATLAB software.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4762602"},{"name":"Power Factor Correction of Direct Torque Controlled Brushless DC Motor Drive","snippet":"In this paper, an algorithm for power factor correction (PFC) of direct torque control (DTC) brushless dc motor drive in the constant torque region is presented. The proposed DTC approach introduces a two-phase conduction mode as opposed to the conventional three-phase DTC drives. Unlike conventional six-step PWM current control, by properly selecting the inverter voltage space vectors of the two-phase conduction mode from a simple look-up table at a predefined sampling time, the desired quasi-square wave current is obtained. Therefore, a much faster torque response is achieved compared to conventional current control. Furthermore, to eliminate the low-frequency torque oscillations caused by the non-ideal trapezoidal shape of the actual back-EMF waveform of the BLDC motor, a pre-stored back-EMF versus position look-up table is designed. The duty cycle of the boost converter is determined by a control algorithm. This control algorithm is based on the input voltage, output voltage which is the dc-link of the BLDC motor drive, and the inductor current using the average current control method with input voltage feed-forward compensation during each sampling period of the drive system. A theoretical concept is developed and the validity and effectiveness of the proposed DTC of BLDC motor drive scheme with PFC are verified through the experimental results. The test results verify that the proposed PFC for DTC of BLDC motor drive improves the power factor from 0.77 to about 0.9997 irrespective of the load.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4347801"},{"name":"An approach for intelligent detection and fault diagnosis of vacuum circuit breakers","snippet":"In this paper, an approach for intelligent detection and fault diagnosis of vacuum circuit breakers is introduced, by which, the condition of a vacuum circuit breaker can be monitored on-line, and the detectable faults can be identified, located, displayed and saved for the use of analyzing their change tendencies. The main detecting principles and diagnostics are described. Both the hardware structure and software design are also presented.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=993739"},{"name":"A new three-phase inverter power-factor correction (PFC) scheme using field programmable gate array","snippet":"In this paper, a new three-phase Inverter power-factor correction PFC scheme is proposed using field programmable gate array (FPGA) technology all the functions can be implemented in a single chip. The implementation tool fit the entered design into the target device (XC4008E). Design verification includes functional simulator, in circuit testing and timing simulation the main function is to verify the proper operation of the designed circuit. It will compile a design file into a configuration file that is optimized in terms of use of logic gates and interconnections for the target device. Power factor measures how effective electrical power is being used. A high power factor means that electrical power is being utilized effectively, while a low power factor indicates poor utilization of electrical power. The simplest way to improve power factor is to add power factor correction capacitors to your plant distribution system. In this paper a new technique is proposed to improve the power factor, experimental results are presented to show the effectiveness of the proposed technique.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1189308"},{"name":"Broken bar fault detection in induction motors based on modified winding Function","snippet":"In this paper, a new turn function for skewed rotor bars based on winding Function approach is presented. This approach has been used for simulating the machine behavior under healthy and broken rotor conditions. Proposed method is applied for rotor bars fault detection based on an advanced Park's vectors approach.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5611213"},{"name":"Power System Fault Estimation under Non-white Noise","snippet":"In this paper, a novel fault detection and isolation filter design method for power distribution in DC electronic system is proposed. The proposed double filter method is a combination of quadratic programming (QP) and Kalman filter which has three main advantages: first, it could estimate the fault position accurately under non-white noise. Second, this is a sequential method which could be used for real-time fault estimation with limited computation. Third, the model is simple and could be used on other circuits without much effort. The performance of the proposed method is verified by numerical simulations.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5743383"},{"name":"An novel loss protection scheme for H.264 video stream based on Frame Error Propagation Index","snippet":"In this paper, a novel GOP level unequal loss protection (G-ULP) scheme is proposed for robust H.264-coded video streaming over packet loss networks. This scheme use frame error propagation index (FEPI) to characterize video quality degradation caused by error propagation in different frames in a GOP when suffer from packet loss. A fast FEPI calculation method in compression domain is also proposed in this paper. By exploiting the unequal significance in different frames in a GOP, different amount of forward error correction (FEC) packets are allocated to different frames in a GOP. The optimal FEC allocation algorithm is based on the FEPI of each frame in the GOP. The simulation results show that the proposed G-ULP scheme can improve the receiver side reconstructed video quality remarkably under different channel loss patterns.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4685150"},{"name":"A Decision Tree-Based Method for Fault Classification in Double-Circuit Transmission Lines","snippet":"In this paper, a novel method for fault classification of double-circuit transmission lines is presented. The proposed method needs voltages and currents of only one side of the protected line. After detecting the exact time of fault inception and calculating the odd harmonics of the measured signals, up to the nineteenth, a decision tree algorithm has been employed for recognition of the intercircuit fault type. Also, the proposed method is extended for classification of crossover faults in these transmission lines. Simulation results have shown that the proposed method can classify the faults in less than a quarter of a cycle with the highest possible accuracy.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5556052"},{"name":"Quasi-cyclic generalized ldpc codes with low error floors","snippet":"In this paper, a novel methodology for designing structured generalized LDPC (G-LDPC) codes is presented. The proposed design results in quasi-cyclic G-LDPC codes for which efficient encoding is feasible through shift-register-based circuits. The structure imposed on the bipartite graphs, together with the choice of simple component codes, leads to a class of codes suitable for fast iterative decoding. A pragmatic approach to the construction of G-LDPC codes is proposed. The approach is based on the substitution of check nodes in the protograph of a low-density parity-check code with stronger nodes based, for instance, on Hamming codes. Such a design approach, which we call LDPC code doping, leads to low-rate quasi-cyclic G-LDPC codes with excellent performance in both the error floor and waterfall regions on the additive white Gaussian noise channel.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4436088"},{"name":"A New Topology of Fault-current Limiter and its control strategy","snippet":"In this paper a new type of fault current limiter based on DC reactor with using superconductor are presented. In normal operation condition the limiter has no obvious effect on loads. When fault happens, the bypass AC reactor and series resistor will insert the fault line automatically to limit the short circuit current, when the control circuit detects a short circuit fault, the solid state bridge in fault line works as an inverter and is closed as soon as possible. Subsequently the fault current is fully limited by the bypass AC reactor and series resistor. The magnitude of L<sub>ac<\/sub> and r<sub>ac<\/sub> must be equal with protected load. By using the electro-magnetic transients in DC systems which are the simulator of electric networks (EMTDC) software we carried out analysis of the voltage and current waveforms for fault conditions. Waveforms are considered in calculating the voltage drop at substation during the fault. The analysis used in selecting an appropriate inductance value for designing","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4108772"},{"name":"Optimal worst case estimation for LPV-FIR models with bounded errors","snippet":"In this paper discrete time linear parameter varying (LPV) models with finite impulse response (FIR) dynamic structure are considered. Measurement errors are assumed to be bounded and in such condition the worst case parameter estimate errors are derived together with the input sequences that allow their determination. The main result of the paper shows that the optimal input design of LPV-FIR models is achieved by combining the available results on optimal input design for invariant FIR models with the results on optimal input design for static blocks","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=914636"},{"name":"Intelligent Fault Isolation of Control Valves in a Power Plant","snippet":"In this paper fault happening for control valves in a power plant is considered. The method is implemented for boiler feed water valve and is extendable to boiler fuel valve and governor valve. Fault kind and location is determined using measurement of boiler state variables. In spite of robust controller design, it may be unable to handle these faults and restricts their effects. A fuzzy compensator does fault accommodation tasks prosperously with modification set-points to the controller. Fuzzy supervisor is implemented in a way which prompts simulation time related to the current implements. Simulation results show the effectiveness of the proposed methodology","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1691734"},{"name":"Modified residue codes based on residue number system as a fault tolerance scheme","snippet":"In this paper residue number system (RNS) arithmetic and redundant residue number system (RRNS) based codes as well as their properties are reviewed. We propose the modification of the RRNS based error correction codes with less number of residues. Our method reduces the hardware overhead drastically and also improves the performance of the system. We review the properties of the modified RRNS codes. We propose applications of the modified RRNS codes as fault tolerance scheme.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5174110"},{"name":"Computer simulation of the native point defects structure in CdTe","snippet":"In this paper the computer simulation of the structure of point defects in CdTe is developed by the numeral solution of electron neutrality equation. The software presented contains lexical analyzer, which allows to construct, analyse and solve the electron neutrality equations of various type. The results of the performer calculations coincide well with experimental data, obtained from the high temperature (500-1200 K) measurements of kinetic coefficients carried out on the CdTe single crystals right their growth.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4839745"},{"name":"Robust hierarchical mobile IPv6 (RH-MIPv6): an enhancement for survivability and fault-tolerance in mobile IP systems","snippet":"In wireless networks, system survivability is one of the most important issues in providing quality of service (QoS). However, since failure of home agent (HA) or mobile anchor point (MAP) causes service interruption, the hierarchical mobile IPv6 (HMIPv6) has only weak survivability. In this paper, we propose robust hierarchical mobile IPv6 (RH-MIPv6), which provides fault tolerance and robustness in mobile networks. In RH-MIPv6, a mobile node (MN) registers primary (P-RCoA) and secondary (S-RCoA) regional care of addresses to two different MAPs (primary and secondary) simultaneously. We develop a mechanism to enable the mobile node or correspondent node (CN) to detect the failure of primary MAP and change their attachment from the primary to secondary MAP. By this recovery procedure, it is possible to reduce the failure recovery time. Analytical evaluation indicates that RH-MIPv6 has faster recovery time than HMIPv6 and we also show through simulation as like analytical result. Consequently, RH-MIPv6 shows about 60% faster recovery time compared with HMIPv6.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1285378"},{"name":"Performance Analysis of Error Control Codes for Wireless Sensor Networks","snippet":"In wireless sensor networks, the data transmitted from the sensor nodes are vulnerable to corruption by errors induced by noisy channels and other factors. Hence it is necessary to provide a proper error control scheme to reduce the bit error rate (BER). Due to the stringent energy constraint in sensor networks, it is vital to use energy efficient error control scheme. In this paper, we focus our study on the performance analysis of various error control codes in terms of their BER performance and power consumption on different platforms. In detail, error control codes with different constraints are implemented and simulated using VHDL. Implementation on FPGA and ASIC design is carried out and the energy consumption is measured. The error control performance of these codes is evaluated in terms of bit error rate (BER) by transmitting randomly generated data through a Gaussian channel. Based on the study and comparison of the three different error control codes, we identify that binary-BCH codes with ASIC implementation are best suitable for wireless sensor networks","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4151795"},{"name":"Application layer error correction scheme for video header protection on wireless network","snippet":"In wireless video streaming application, video information may be corrupted by a noisy channel. By introducing error resilience and error concealment techniques, many researchers have tried to eliminate quality degradation of reconstructed picture in decoding a corrupted data. On the contrary, there are relatively fewer works discussing the ways to diminish the corruption. Hence, system designers need to use different methods to restrict the error cause by channel within a tolerable extent. In other words, the system will be difficult to be implemented in practical design. In this paper, we propose a way to protect the video header information in application layer without modifying standardized syntax. Beside, we also consider channel condition of wireless transmission and propose a way to reduce redundant bits used in channel coding. By doing this, the bitstream can be simply transmitted over practical wireless network and the reconstructed picture quality outperforms the original one.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1565873"},{"name":"Optimal packet scheduling for wireless video streaming with error-prone feedback","snippet":"In wireless video transmission, burst packet errors generally produce more catastrophic results than equal number of isolated errors. To miniimize the playback distortion it is crucial for the sender to know the packet errors at the receiver and then optimally schedule next transmissions. Unfortunately, in practice, feedback errors result in inaccurate observations of the receiving status. In this paper, we develop an optimal scheduling framework to minimize the expected distortion by first estimating the receiving status. Then, we jointly consider the source and channel characteristics and optimally choose the packets to transmit. The optimal transmission strategy is computed through a partially observable Markov decision process. The experimental results show that the proposed framework improves the average peak signal-to-noise ratio (PSNR) by 0.6-1.3 dB upon using a traditional system without packet scheduling. Moreover, we show that the proposed method smoothes out the bursty distortion periods and results in less fluctuating PSNR values.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1311374"},{"name":"Practical Evaluation of Opportunistic Error Correction","snippet":"In, we have proposed a novel cross-layer scheme based on resolution adaptive ADCs and fountain codes for the OFDM systems to lower the power consumption in ADCs. The simulation results show that it saves more than 70% power consumption in ADCs comparing to the current IEEE 802.11a system. In this paper, we investigate its performance in the realworld. Measurement results show that the FEC layer used in the IEEE 802.11a system consumes around 59 times of the amount of power in ADCs comparing to the LDPC codes from the IEEE 802.11n standard, whose power consumption in ADCs is around 26 times of the proposed cross-layer method. In addition, this new cross-layer approach only needs to process the well-received packets to save the processing power. The latter can not be applied in the current FEC schemes.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5425810"},{"name":"Effect of Implantation Defects and Carbon Incorporation on Si\/SiGe Bipolar Characteristics","snippet":"Incorporation of carbon in SiGe has attracted great interest, which makes SiGeC based heterojunction transistors as an attractive device for high frequency applications. Carbon addition in SiGe dramatically reduces out-diffusion of boron caused by excess of interstitials generated by extrinsic base implantation. However carbon incorporation negatively influences the electrical device characteristics. In this paper we investigate active implantation defects, the aim was to the specify space localization and parasitic effects of this ones. Second, we investigate the impact of carbon content on the electrical characteristics of device, the results show that indeed C contents A 1% severely degrade transistor performances.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5380214"},{"name":"Analysis of Real-Time Systems Sensitivity to Transient Faults Using MicroC Kernel","snippet":"Increasing complexity of safety-critical systems that support real-time multitasking applications requests the concurrency management offered by real-time operating systems (RTOS). Real-time systems can suffer severe consequences if the functional as well as the time specifications are not met. In addition, real-time systems are subject to transient errors originating from several sources, including the impact of high energy particles on sensitive areas of integrated circuits. Therefore, the evaluation of the sensitivity of RTOS to transient faults is a major issue. This paper explores sensitivity of RTOS kernels in safety-critical systems. We characterize and analyze the consequences of transient faults on key components of the kernel of MicroC, a popular RTOS. We specifically focus on its task scheduling and context switching modules. Classes of fault syndromes specific to safety-critical real-time systems are identified. Results reported in this paper demonstrate that 34% of faults that affect the scheduling and context switching functions led to scheduling dysfunctions. This represents an important fraction of faults that cannot be ignored during the design phase of safety-critical applications running under an RTOS","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1684036"},{"name":"Software detection mechanisms providing full coverage against single bit-flip faults","snippet":"Increasing design complexity for current and future generations of microelectronic technologies leads to an increased sensitivity to transient bit-flip errors. These errors can cause unpredictable behaviors and corrupt data integrity and system availability. This work proposes new solutions to detect all classes of faults, including those that escape conventional software detection mechanisms, allowing full protection against transient bit-flip errors. The proposed solutions, particularly well suited for low-cost safety-critical microprocessor-based applications, have been validated through exhaustive fault injection experiments performed on a set of real and synthetic benchmark programs. The fault model taken into consideration was single bit-flip errors corrupting memory cells accessible to the user by means of the processor instruction set. The obtained results demonstrate the effectiveness of the proposed solutions.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1369518"},{"name":"Modeling the coverage and effectiveness of fault-management architectures in layered distributed systems","snippet":"Increasingly, fault-tolerant distributed software applications use a separate architecture for failure detection instead of coding the mechanisms inside the application itself. Such a structure removes the intricacies of the failure detection mechanisms from the application, and avoids repeating them in every program. However, successful system reconfiguration now depends on the management architecture (which does both fault detection and reconfiguration), and on management subsystem failures, as well as on the application. This paper presents an approach which computes the architecture-based system reconfiguration coverage simultaneously with its performability.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1029020"},{"name":"Influence of Magnetic Saturation on Diagnostic Signal of Induction Motor Cage Faults","snippet":"Induction motor rotor cage diagnostics bases on additional components in stator phase current with (1-2s)f<sub>o<\/sub> and (1+2s)f<sub>o<\/sub> frequencies. The (1-2s)f<sub>o<\/sub> component arises as the main effect of the cage asymmetry whereas the (1+2s)f<sub>o<\/sub> component is a secondary effect. It is commonly known, that it arises due to speed ripples, which are generated by alternating component of electromagnetic torque in a motor with faulty cage. This paper shows that magnetic saturation of a main magnetic circuit also generates the (1+2s)f<sub>o<\/sub> component in stator currents. A special mathematical model accounting for saturation is used to prove this thesis. Results of quantitative analysis of an influence of inertia and saturation on the (1+2s)f<sub>o<\/sub> component is shown.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4393101"},{"name":"Optimizing the fault tolerance capabilities of distributed real-time systems","snippet":"Industrial real-time systems typically have to satisfy complex requirements, mapped to the task attributes, eventually guaranteed by a fixed priority scheduler in a distributed environment. These systems consist of a mix of hard and soft tasks with varying criticality, as well as associated fault tolerance requirements. Time redundancy techniques are often preferred in industrial applications and, hence, it is extremely important to devise resource efficient methodologies for scheduling real-time tasks under failure assumptions. In this paper, we propose a methodology to provide a priori guarantees in distributed real-time systems with redundancy requirements. We do so by identifying temporal feasibility windows for all task executions and re-executions, as well as allocating them on different processing nodes. We then use optimization theory to derive the optimal feasibility windows that maximize the utilization on each node, while avoiding overloads. Finally on each node, we use integer linear programming (ILP) to derive fixed priority task attributes that guarantee the task executions within the derived feasibility windows, while keeping the associated costs minimized.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5347216"},{"name":"Evaluation of fault tolerance latency from real-time application's perspectives","snippet":"Information on Fault Tolerance Latency (FTL), which is defined as the total time required by all sequential steps taken to recover from an error, is important to the design and evaluation of fault-tolerant computers used in safety-critical real-time control systems with deadline information. In this paper, we evaluate FTL in terms of several random and deterministic variables accounting for fault behaviors and\/or the capability and performance of error-handling mechanisms, while considering various fault tolerance mechanisms based on the trade-off between temporal and spatial redundancy, and use the evaluated FTL to check if an error-handling policy can meet the Control System Deadline (CSD) for a given real-time application","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=822564"},{"name":"Data-fitting reconstruction for defect inspection of airplane aluminum structure in infrared thermographic NDT","snippet":"Infrared (IR) thermography has already demonstrated to be an effective tool for nondestructive testing and evaluation (NDT & E) applications. Pulsed thermography (PT) is such kind of technique often used in rapid and wide-area sub-surface inspection. In aviation industry, metal structures of aluminum, which has high thermal conductivity and diffusivity, usually need to be verified in material integrality both for manufacture and maintenance. And it is generally difficult to get sufficient noise-free sampling data for an accurate analysis due to swift heat conduction in material of high thermal conductivity, if the IR device is not good enough for a high sampling rate. So a data-fitting process is offered to reconstruct the sequence from a PT test for cost-effective detection by a commercial mediocre infrared imaging system. This method brings two evident advantages: increased image quality by reducing temporal stochastic noise, and discretionary number of reconstructed frame at any precision for quantitative analysis. A particular specimen of duralumin is machined to emulate airplane components. And experimental difference curves and images are given, behaving well to indicate the validity of this economical inspection.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5138294"},{"name":"An accurate analysis of the effects of soft errors in the instruction and data caches of a pipelined microprocessor","snippet":"Instruction and data caches are well known architectural solutions that allow significant improvement in the performance of high-end processors. Due to their sensitivity to soft errors, they are often disabled in safety critical applications, thus sacrificing performance for improved dependability. In this paper, we report an accurate analysis of the effects of soft errors in the instruction and data caches of a soft core implementing the SPARC architecture. Thanks to an efficient simulation-based fault injection environment we developed, we are able to present in this paper an extensive analysis of the effects of soft errors on a processor running several applications under different memory configurations. The procedure we followed allows the precise computation of the processor failure rate when the cache is enabled even without resorting to expensive radiation experiments.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1253674"},{"name":"Design of control programs for efficient handling of errors in flexible manufacturing cells","snippet":"Insufficient indication of errors is a problem in many manufacturing systems. Lack of support for resynchronization of the cell and its control system is another, less obvious problem. A third problem connected to errors in manufacturing cells is lack of support for manual control. In order to resolve an error situation manual control of the cell is often required. The problem is that some of the manual operations may be blocked due to machine protection. When an operator is to execute a blocked operation the only response is that nothing happens. This paper proposes a method where control programs with integrated functions for error detection, resynchronization, and support for manual control are generated out of information that already exists in the development process of a manufacturing system.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1307400"},{"name":"Review of fault diagnosis in control systems","snippet":"In this paper, we review the major achievements on the research of fault diagnosis in control systems (FDCS) from three aspects which including fault detection, fault isolation and hybrid intelligent fault diagnosis. Fault detection and isolation (FDI) are two important stages in the diagnosis process while hybrid intelligent fault diagnosis is the hot issue in current research field. The particular feature of FDCS is using the closed-loop monitoring information in control system to establish the quantitative and qualitative process model, detecting and then isolating the main failures in sensors, actuators, and the controlled process; the main challenge of FDCS is reducing the false alarm rate and missing alarm rate, improving the sensitivity and rapidity. The robust fault detection in the transition process, the knowledge acquisition for quantitative and qualitative diagnosis based on process history data, and hybrid intelligent fault diagnosis system architecture are worthy of a deeper research.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5195065"},{"name":"Iterative correction of frequency response mismatches in time-interleaved ADCs: A novel framework and case study in OFDM systems","snippet":"In this paper, we study a versatile iterative framework for the correction of frequency response mismatch in time-interleaved ADCs. Based on a general time varying linear system model, we establish a flexible iterative framework, which enables the development of various efficient iterative correction algorithms. In particular, we study the Gauss-Seidel iteration in detail to illustrate how the correction problem can be solved iteratively, and show that the iterative structure can be efficiently implemented using Farrow-based variable digital filters with few general-purpose multipliers. Simulation results show that the proposed iterative structure performs better than conventional compensation structures. Moreover, a preliminary study on the BER performance of OFDM systems due to TI ADC mismatch is conducted.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5543057"},{"name":"Impact of transmission error in DCH channel on the performance of UMTS networks","snippet":"In this paper, we study the impact of dedicated channel (DCH) transmission error on the performance of the application layer in universal mobile terrestrial system (UMTS) networks. We simulate a UMTS network with fully implemented protocol stack and study the impact of transmission error in the physical layer on the throughput and the delay variance (jitter) as performance metrics in the application layer. Our simulation results indicate that the net error rate for the delivered data in the application layer in acknowledged mode (AM) is smaller than that of unacknowledged mode (UM) however, in both AM and UM modes the channel throughput perceived in the application layer is decreased by increasing channel error rate in an approximately linear fashion. Simulation results also indicate that increasing\/decreasing channel error rate in the physical layer has no significant impact on the delay variance in both AM and UM modes.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4564873"},{"name":"Feature Selection with Imbalanced Data for Software Defect Prediction","snippet":"In this paper, we study the learning impact of data sampling followed by attribute selection on the classification models built with binary class imbalanced data within the scenario of software quality engineering. We use a wrapper-based attribute ranking technique to select a subset of attributes, and the random undersampling technique (RUS) on the majority class to alleviate the negative effects of imbalanced data on the prediction models. The datasets used in the empirical study were collected from numerous software projects. Five data preprocessing scenarios were explored in these experiments, including: (1) training on the original, unaltered fit dataset, (2) training on a sampled version of the fit dataset, (3) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on the unsampled fit dataset, (4) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on a sampled version of the fit dataset, and (5) training on a sampled version of the fit dataset using only the attributes chosen by feature selection based on the sampled version of the fit dataset. We compared the performances of the classification models constructed over these five different scenarios. The results demonstrate that the classification models constructed on the sampled fit data with or without feature selection (case 2 and case 5) significantly outperformed the classification models built with the other cases (unsampled fit data). Moreover, the two scenarios using sampled data (case 2 and case 5) showed very similar performances, but the subset of attributes (case 5) is only around 15% or 30% of the complete set of attributes (case 2).","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5381844"},{"name":"The Two-Level-Turn-Model Fault-tolerant Routing Scheme in Tori with Convex Faults","snippet":"In this paper, we would present a fault-tolerant wormhole routing scheme, called two-level-turn-model scheme, in the tori with convex faults. The reason why we choose the convex faults is that the shape of the convex faults is instrumental to design an effective fault-tolerant routing algorithm. Compared with many other solutions, which mainly focus on providing extra virtual channels to tolerate the faults, one of the advantages of our solution is that it is based on the turn model, which itself could tolerate some faults for some messages. At the same time, our solution could work effectively no matter where the fault region locates and no matter whether the fault regions are connected. In our solution, two patterns of the turn model are complementary to tolerate the faults. With a few limits to the shape of the convex faults, at most five virtual channels per physical channel are required to avoid the deadlock.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4624895"},{"name":"An Alternative Approach to Fault Location on Main Line Feeders and Laterals in Low Voltage Overhead Distribution Networks","snippet":"In this study, a digital fault location and monitoring technique for overhead power distribution lines with laterals is presented. The technique is based on utilising the fault voltage and current samples obtained at a single location of a typical distribution system with laterals; these are then filtered after the analogue to digital conversion process by using digital filtering techniques to obtain power frequency components of voltage and current samples. In the implementation of the algorithm, superimposed voltage and current components rather than total values are used to minimise the effects of pre-fault loading on the accuracy. The effectiveness of this method is verified through electromagnetic transients program (EMTP) simulations.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4497011"},{"name":"Identification and fault diagnosis of a simulated model of an industrial gas turbine","snippet":"In this study, a model-based procedure exploiting analytical redundancy for the detection and isolation of faults of a gas turbine system is presented. The diagnosis scheme is based on the generation of so-called \"residuals\" that are errors between estimated and measured variables of the process. The work is completed under both noise-free and noisy conditions. Residual analysis and statistical tests are used for fault detection and isolation, respectively. The final section shows how the actual size of each fault can be estimated using a multilayer perceptron neural network used as a nonlinear function approximator. The proposed fault detection and isolation tool has been tested on a single-shaft industrial gas turbine model.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1495411"},{"name":"The design and implementation of a microcontroller-based single phase on- line uninterrupted power supply with power factor correction","snippet":"In this study, the design and implementation of a microcontroller-based single phase on-line UPS (Uninterrupted Power Supply) with PFC (Power Factor Correction) were made practically. SP-320-24 SMPS (Switch Mode Power Supply) module was used to correct the input power factor. Input power factor value was held at the desired value in uninterrupted power supply topologies. In the realized system, two PIC16F876 were used as microcontroller. One of them was used to generate sinusoidal PWM (Pulse Width Modulation) signals that are used to drive n-channel MOSFETs in push pull inverter and to assure feedback control. Other one was used to control and display units. Harmonics were eliminated and output filter was simplified by using sinusoidal PWM technology.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5355274"},{"name":"Detection of small defects by THz-waves for non-destructive testing in dielectric layered structures","snippet":"In this study, the small defects detection in dielectric layered structures by THz waves for nondestructive testing. Finite element method were used for modelling of the structures.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5546032"},{"name":"Errors Estimation Models for Embedded Software Development Projects","snippet":"In this study, we analyze and evaluate development data obtained in the course of \"OMRON software Co.\", that develops equipments used in financial markets, as well as data obtained during the development of software. Because of this, it is becoming very important for the software- development corporations to find how to develop software efficiently while guaranteeing delivery time and quality, and holding down developing costs. Especially, estimating manpower of new projects and guaranteeing quality of software are important, because the estimation relates to costs and the quality relates to the reliability of corporations. In the field of embedded software, development techniques, management techniques, tools, testing techniques, reusing techniques, real time operating systems and so on have already been studied. However, there is few studies about the relationship between the development scales and the total errors using the data accumulated by past projects. Hence, we establish a model for the first trial to estimate the errors of a new project by using regression analysis.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4297020"},{"name":"Fault Diagnosis of Power Transformer Using SVM and FCM","snippet":"In this study, we are concerned with fault diagnosis of power transformer. The objective is to explore the use of some advanced techniques such as SVM and FCM and quantify their effectiveness when dealing with dissolved gases extracted from power transformers. The proposed fault diagnosis system consists of data acquisition, fault\/normal diagnosis, identification of fault and analysis of aging degree parts. In data acquisition part, concentrated gases are extracted from transformer for data gas analysis. In fault\/normal diagnosis part, SVM is performed to separate normal state from fault types. The determination of fault type is executed by multi-class SVM in identification part. Although the inputted data is normal state, the analysis of aging degree is performed by considering the distance measure calculated by comparing with reference model constructed by FCM and input data. Our approach makes it possible to measure the possibility and degree of aging in normal transformer as well as the identification of faults in abnormal transformer. As the simulation results to verify the effectiveness, the proposed method showed more improved classification results than conventional methods.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4570291"},{"name":"Error performance of maximal-ratio combining with transmit antenna selection in flat Nakagami-m fading channels","snippet":"In this paper, the performance of an uncoded multiple-input-multiple-output (MIMO) scheme combining single transmit antenna selection and receiver maximal-ratio combining (the TAS\/MRC scheme) is investigated for independent flat Nakagami-m fading channels with arbitrary real-valued m. The outage probability is first derived. Then the error rate expressions are attained from two different approaches. First, based on the observation of the instantaneous channel gain, the binary phase-shift keying (BPSK) asymptotic bit error rate (BER) expression is derived, and the exact BER expression is obtained as an infinite series, which converges for reasonably large signal-to-noise ratios (SNRs). Then the exact symbol error rate (SER) expressions are attained as a multiple infinite sum based on the moment generating function (MGF) method for M-ary phase-shift keying (M-PSK) and quadrature amplitude modulation (M-QAM). The asymptotic SER expressions reveal a diversity order equal to the product of the m parameter, the number of transmit antennas and the number of receive antennas. Theoretical analysis is verified by simulation.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4786523"},{"name":"Error in projection of planewaves using various basis functions","snippet":"In this paper, the projection error of RMS in 1D and 2D case is analyzed. The analytical projection error on the infinite meshes are given in closed form for the pulse basis, triangular basis, the second-order basis in 1D case, the divergence-conforming basis on rectangular element and the one-directional triangular element in 2D case. In addition, the projection error is numerically calculated for various basis functions with a finite computational domain. There are good agreements between the analytical and numerical results. It is found the projection error of p-th order 1D basis function is asymptotically proportional to (p+1) power of the size of the element. More results and discussions about the projection errors will be presented at the conference.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4619717"},{"name":"Design and analysis of AC\/DC converters with interleaved Power Factor Correction","snippet":"In this paper, two current sharing control schemes base on average current mode control is proposed for interleaved Power Factor Correction (PFC) converter. The boost inductor current of each phases of interleaved PFC must be sensed to achieve current sharing and power limiting. To meet this requirement, we proposed a current transformers current sense skill which achieve current sharing and also can improve current harmonic distortion during high line operation. Finally, the SPICE like tool of simulation for the whole system is built up for verification.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4528886"},{"name":"Improvement of CAN BUS Performance by Using Error-Correction Codes","snippet":"In this paper, two variants of the Hybrid Automatic Repeat Request (HARQ) scheme for CAN bus are presented. The basic HARQ uses error-correction code based on the Reed-Solomon (RS) technique and the Cyclic Redundancy Check (CRC) method to detect errors. The second scheme uses the cyclic error-correction method instead of the CRC error-detection method to further improve the throughput. Moreover, the second scheme uses no additional bit overhead when compared with the basic HARQ scheme. This paper presents the performance of the proposed schemes using MATLAB and NS2 simulations. Experimental data of error patterns were used for realistic evaluation. The basic HARQ method corrects 100% of error bursts shorter than 7 bits. When the burst length falls between 7 to 10 the scheme corrects between 86% and 56% of the corrupted frames. Network Simulator (NS2) simulations showed that the throughput increased by 92% when the user message size was increased from the standard 64 bits to 512 bits as a result of reduced overhead per user bit.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4380382"},{"name":"A resource manager for optimal resource selection and fault tolerance service in Grids","snippet":"In this paper, we address the issues of resource management and fault tolerance in Grids. In Grids, the state of the selected resources for job execution is a primary factor that determines the computing performance. Specifically, we propose a resource manager for optimal resource selection. The resource manager automatically selects the optimal resources among candidate resources using a genetic algorithm. Typically, the probability of failure is higher in Grid computing than in a traditional parallel computing and the failure of resources affects job execution fatally. Therefore, a fault tolerance service is essential in computational Grids and Grid services are often expected to meet some minimum levels of quality of service (QoS) for desirable operation. To address this issue, we also propose fault tolerance service to satisfy QoS requirements. We extend the definition of failures, such as process failure, processor failure, and network failure, and design the fault detector and fault manager. The simulation results indicate that our approaches are promising in that (1) our resource manager finds the optimal set of resources that guarantees the optimal performance; (2) the fault detector detects the occurrence of resource failures; and (3) the fault manager guarantees that the submitted jobs complete and improves the performance of job execution due to job migration even if some failures happen.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1336659"},{"name":"Particle filtering for adaptive sensor fault detection and identification","snippet":"In this paper, we address the problem of adaptive sensor fault identification and validation by particle filtering. The model-based approaches are developed, where the sensor system is modeled by a Markov switch dynamic state-space model. To handle the nonlinearity of the problem, two different particle filters: mixture Kalman filter (MKF) and stochastic M-algorithm (SMA) are proposed. Simulation results are presented to compare the effectiveness and complexity of MKF and SMA methods","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1642284"},{"name":"Fault tolerance in IEEE 802.11 WLANs","snippet":"in this paper, we address the problem of enhancing the fault tolerance of IEEE 802.11 wireless local area networks focusing on tolerating access point - AP failures. We develop a fault detection approach, which promises to be more effective to identify AP failures. In particular, we focus on the problem of overcoming APs failures working with reconfiguration of the remaining APs by changing parameters such as power level and frequency channels. Our approach consists of two main phases: Design and Fault Response. In Design phase, we deal with quantifying, placement and setting up of APs according to both area coverage and performance criteria. In Fault Response phase we consider the reconfiguration of active APs in order to deal with AP fault in the service area. Finally, we describe one of the major characteristics of the proposed architecture, which is a simple implementation in concordance with established IEEE 802.11 standards and related management systems.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4433349"},{"name":"Probability of error of space-time coded OFDM systems with frequency offset in frequency-selective Rayleigh fading channels","snippet":"In this paper, we analyze the performance of space-time coded (ST-coded) orthogonal frequency division multiplexing (OFDM) systems with carrier frequency offset (CFO) in a frequency-selective Rayleigh fading channel. Closed-form analytical expressions are derived for the symbol error probability (SEP) for M-PSK and M-QAM modulation schemes. The SEP expressions derived are valid and exact for OFDM systems with highly frequency-selective wireless channels where the subcarrier channel responses are i.i.d. with Rayleigh fading. Also, for the case where imperfect channel knowledge is used for space-time decoding, we derive expressions for constellation phase-rotation and post-equalized SINR (signal-to-interference-and-noise ratio) degradation due to CFO. The numerical results demonstrate the sensitivity of the receiver error performance to CFO in ST-coded OFDM systems.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1494818"},{"name":"Transmission characteristic of defect for the 90 bend photonic crystal","snippet":"In this paper, we analyzed the transmission characteristics of the point-defected around the outer layers of 90-degree bend photonic crystal. The simulation results show the vertical and horizontal point-defects have much effect than slanted ones.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5588430"},{"name":"Dynamic Multiple-Fault Diagnosis With Imperfect Tests","snippet":"In this paper, we consider a model for the dynamic multiple-fault diagnosis (DMFD) problem arising in online monitoring of complex systems and present a solution. This problem involves real-time inference of the most likely set of faults and their time-evolution based on blocks of unreliable test outcomes over time. In the DMFD problem, there is a finite set of mutually independent fault states, and a finite set of sensors (tests) is used to monitor their status. We model the dependence of test outcomes on the fault states via the traditional D-matrix (fault dictionary). The tests are imperfect in the sense that they can have missed detections, false alarms, or may be available asynchronously. Based on the imperfect observations over time, the problem is to identify the most likely evolution of fault states over time. The DMFD problem is an intractable NP-hard combinatorial optimization problem. Consequently, we decompose the DMFD problem into a series of decoupled subproblems, one for each sample epoch. For a single-epoch MFD, we develop a fast and high-quality deterministic simulated annealing method. Based on the sequential inferences, a local search-and-update scheme is applied to further improve the solution. Finally, we discuss how the method can be extended to dependent faults.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5281205"},{"name":"Fault management for next-generation IP-over-WDM networks","snippet":"In this paper, we present a broad outline for fault management for next generation IP-over-WDM network. This system contains three different components namely fault detection, fault recovery and fairness service provisions for different needs. We also review various fault detection and fault recovery schemes for WDM networks.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1505835"},{"name":"A heuristic method to reduce fault candidates for a speedy fault diagnosis","snippet":"In this paper, we present a heuristic method to reduce fault candidates for an efficient fault diagnosis. This paper uses a matching algorithm for the exact fault diagnosis. But the time consumption of a fault diagnosis using the matching algorithm is huge. So, we present a new method to reduce the fault diagnosis time. The method to reduce the time consumption is separated into two different phases which are a pattern comparison and a back-tracing comparison in failing pattern. The proposed method reduces fault candidates by comparing failing patterns with good patterns during critical path tracing process and comparing back-tracing from non-erroneous POs with back-tracing erroneous POs. The proposed method increases the simulation speed than the conventional algorithms. And this method is also applicable to any other fault diagnosis algorithms. Experimental results on ISCAS'85 and ISCAS'89 benchmark circuits show that fault candidate lists are reduced than those of previous diagnosis methods.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4815607"},{"name":"Fault detection, isolation, and accommodation in a UAV longitudinal control system","snippet":"In this paper, we present a multiple-model based method of analyzing for the longitudinal controller performance loss caused by actuator faults in the aircraft elevator system. More specifically, we consider the effects of the failure-induced elevator actuator bandwidth reduction integrated with the longitudinal flight dynamics. Results of the proposed multiple-model based fault detection, isolation, and accommodation (FDIA) algorithm are applied to an uninhabited air vehicle (UAV) subject to multiple actuator failures. Simulation results illustrate that the proposed fault-tolerant controller is an effective and efficient FDIA methodology, and could practically be vital to attack a wide range of similar applications.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5676046"},{"name":"A practical system for online diagnosis of control valve faults","snippet":"In this paper, we present a new control valve monitoring system using the nonparametric statistical hypothesis tests for the diagnosis of backlash, deadband, leakage, and blocking, four common faults found in many control systems. To make our system practical and inexpensive, we utilize the sensor measurements available in most control systems. For the classification of individual faults, we extract the geometric features from the measurement signals and detect the faults from their temporal trends. Based on the features, the statistical hypothesis tests are applied to discriminate the faults.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4434224"},{"name":"Online Failure Forecast for Fault-Tolerant Data Stream Processing","snippet":"In this paper, we present a new online failure forecast system to achieve predictive failure management for fault-tolerant data stream processing. Different from previous reactive or proactive approaches, predictive failure management employs failure forecast to perform informed and just-in-time preventive actions on abnormal components only. We employ stream-based online learning methods to continuously classify runtime operator state into normal, alert, or failure, based on collected feature streams. We have implemented the online failure forecast system as part of the IBM system S stream processing system. Our experiments show that the on-line failure forecast system can achieve good prediction accuracy for a range of stream processing software failures, while imposing low overhead to the stream system.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4497565"},{"name":"Single-image vignetting correction using radial gradient symmetry","snippet":"In this paper, we present a novel single-image vignetting method based on the symmetric distribution of the radial gradient (RG). The radial gradient is the image gradient along the radial direction with respect to the image center. We show that the RG distribution for natural images without vignetting is generally symmetric. However, this distribution is skewed by vignetting. We develop two variants of this technique, both of which remove vignetting by minimizing asymmetry of the RG distribution. Compared with prior approaches to single-image vignetting correction, our method does not require segmentation and the results are generally better. Experiments show our technique works for a wide range of images and it achieves a speed-up of 4-5 times compared with a state-of-the-art method.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4587413"},{"name":"A technique for fault diagnosis of defects in scan chains","snippet":"In this paper, we present a scan chain fault diagnosis procedure. The diagnosis for a single scan chain fault is performed in three steps. The first step uses special chain test patterns to determine both the faulty chain and the fault type in the faulty chain. The second step uses a novel procedure to generate special test patterns to identify the suspect scan cell within a range of scan cells. Unlike previously proposed methods that restrict the location of the faulty scan cell only from the scan chain output side, our method restricts the location of the faulty scan cell from both the scan chain output side and the scan chain input side. Hence the number of suspect scan cells is reduced significantly in this step. The final step further improves the diagnostic resolution by ranking the suspect scan cells inside this range. The proposed technique handles both stuck-at and timing failures (transition faults and hold time faults). The extension of the procedure to diagnose multiple faults is discussed. The experimental results show the effectiveness of the proposed method","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=966642"},{"name":"Energy-efficient soft error-tolerant digital signal processing","snippet":"In this paper, we present energy-efficient soft error-tolerant techniques for digital signal processing (DSP) systems. The proposed technique, referred to as algorithmic soft error-tolerance (ASET), employs low-complexity estimators of a main DSP block to achieve reliable operation in the presence of soft errors. Three distinct ASET techniques - spatial, temporal and spatio-temporal- are presented. For frequency selective finite-impulse response (FIR) filtering, it is shown that the proposed techniques provide robustness in the presence of soft error rates of up to P\/sub er\/=10\/sup -2\/ and P\/sub er\/=10\/sup -3\/ in a single-event upset scenario. The power dissipation of the proposed techniques ranges from 1.1 X to 1.7 X (spatial ASET) and 1.05 X to 1.17 X (spatio-temporal and temporal ASET) when the desired signal-to-noise ratio SNR\/sub des\/=25 dB. In comparison, the power dissipation of the commonly employed triple modular redundancy technique is 2.9 X.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1637464"},{"name":"The effectiveness of using non redundant test cases with program spectra for bug localization","snippet":"In this paper, we present our approach of using non redundant test cases with program spectra (one of the automated bug localization techniques) to locate software bugs in a program. We evaluate several spectra metrics (functions mapped from program spectra) using the non redundant test cases. Extensive evaluation on Siemens Test Suite and subset of Unix datasets shows the effectiveness of locating bug using non redundant test cases with program spectra. In this paper, we also show that by adding duplicates of non redundant test cases, the stability and performance of spectra metrics are affected.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5234587"},{"name":"Partial simulation-driven ATPG for detection and diagnosis of faults in analog circuits","snippet":"In this paper, we propose a novel fault-oriented test generation methodology for detection and isolation of faults in analog circuits. Given the description of the circuit-under-test, the proposed test generator computes the optimal transient test stimuli in order to detect and isolate a given set of faults. It also computes the optimal set of test nodes to probe at, and the time instants to make measurements. The test generation program accommodates the effects introduced by component tolerances and measurement inaccuracy, and can be tailored to fit the signal generation capabilities of a hardware tester. Experimental results show that the proposed technique can be applied to generate transient tests for both linear and non-linear analog circuits of moderate complexity in reasonably less CPU time. This will significantly impact the test development costs for an analog circuit and will decrease the time-to-market of a product. Finally, the short duration and the easy-to-apply feature of the test stimuli will lead to significant reduction in production test costs.","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=896532"},{"name":"A band-notched ultra-wideband 1 to 4 wilkinson power divider using symmetric defected ground structure","snippet":"In this paper, we propose a novel ultra-wideband Wilkinson power divider with symmetric defected ground structure (DGS) having frequency band-notch characteristic. The Wilkinson power divider was invented in 1960 and has wide applications in microwave circuits and antenna feeds. But it has a narrow bandwidth. Several schemes have been devised to increase its bandwidth. The main proposal used series connection of several sections having considerably increased bandwidth and high isolation between outputs for equal power divider. We use connection of three sections to obtain the UWB Wilkinson power divider. It is well known that the DGS of the microstrip line is implemented by making artificial defect on the ground and the ground defect provides a resonance property in transfer characteristic. In the microstrip line, DGS on the ground plane provides band rejection characteristic at some resonance frequency corresponding to the size of defect on the ground. In this paper, band-notch characteristic is achieved by inserting a symmetric spiral DGS under the microstrip line of the power divider. Experimental results of the constructed prototype are presented.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4395805"},{"name":"Proxy-Based SNR Scalable Error Tracking for Real-Time Video Transmission Over Wireless Broadband","snippet":"In this paper, we propose a proxy-based SNR scalable error tracking framework for real-time video transmission where the server is wired connected to Internet and the client is connected to Internet through wireless broadband networks. We assume that all errors (packet losses) result from wireless links, and wired links are assumed to be error-free. The client sends back NACKs with the information about base layer lost packets to the proxy via a feedback channel. Once the NACK is received, the proxy uses the motion data and the side information received from the streaming server to perform error tracking. We compare our approach to the original proxy-based error tracking scheme without scalability support at the same bitrate and bit error rate. Experimental results show that the proposed method can effectively improve performance","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4036814"},{"name":"A fast and accurate multi-cycle soft error rate estimation approach to resilient embedded systems design","snippet":"In this paper, we propose a very fast and accurate analytical approach to estimate the overall SER and to identify the most vulnerable gates, flip-flops, and paths of a circuit. Using such information, designers can selectively protect the vulnerable parts resulting in lower power and area overheads that are the most important factors in embedded systems. Unlike previous approaches, the proposed approach firstly does not rely on fault injection or fault simulation; secondly it measures the SER for multi cycles of circuit operation; thirdly, the proposed approach accurately computes all three masking factors, namely, logical, electrical, and timing masking; fourthly, the effects of error propagation in re-convergent fanouts are considered in the proposed approach. SERs estimated by the proposed approach for some ISCAS89 circuit benchmarks are compared with that estimated by the Monte Carlo (MC) simulation based fault injection approach. The results show that the proposed approach is about four orders of magnitude faster than the MC fault injection approach while having an accuracy of about 97%. This level of fastness and accuracy makes the proposed approach a viable solution to measure the SER of very large size circuits used in industry.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5544952"},{"name":"An Effective Error Concealment Framework For H.264 Decoder Based on Video Scene Change Detection","snippet":"In this paper, we propose an effective error concealment framework for H.264 decoder based on the scene change detection. The proposed framework quickly and accurately detects whether scene change occurs in the decoding frame, based on the detection result, both corrupted intra frames and damaged inter frames can be reconstructed by spatial or improved temporal EC (Error Concealment) algorithm. The experiment shows that, compared with the traditional error concealment method in the H.264\/A VC non- normative decoder, the proposed framework has better robustness and can efficiently improve the visual quality and PSNR of the decoded video.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4297099"},{"name":"Generalization of Rateless Codes for Unequal Error Protection and Recovery Time: Asymptotic Analysis","snippet":"In this paper, we propose rateless codes that provide unequal error protection (UEP) property. We analyze the asymptotic properties of these codes under the iterative decoding algorithm. We further verify our work with simulations. The simulation results indicate that the proposed codes have strong UEP property. Moreover, the UEP property does not have a considerable drawback on the overall performance of the codes. We also discuss that the proposed codes can provide unequal recovery time (URT). This means that given a target bit error rate, different parts of information bits can be decoded after receiving different amounts of encoded bits. This implies that the information bits can be recovered in a progressive manner. This URT property may be used for sequential data recovery in video\/audio streaming","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4036017"},{"name":"Fault Tolerance & Testable Software Security: A Method of Quantifiable Non-Malleability with Respect to Time","snippet":"In this paper, we demonstrate there exists practical limits to the recoverability and integrity verification (non-malleability) of software with respect to time a property to the best of our knowledge not demonstrated previously; this in turn, implies practical limits to software security using current existing processing hardware. Non-malleability applied to software implies that it should be infeasible for an attacker to modify a piece of software, thus creating a software fault. Given the recoverability limitation, we demonstrate a quantifiable definition for secure software with respect to integrity\/tamper resistance.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4233045"},{"name":"A CPW bandpass filter using defected ground structure with shorting stubs for 60 GHz applications","snippet":"In this paper, we design and fabricate a coplanar waveguide (CPW) bandpass filter operating at 60 GHz. A new DGS pattern is proposed and serves as the unit cell. We cascade two unit cells to construct a second-order bandpass filter. The equivalent lumped element circuit model was extracted form the full-wave electromagnetic simulation. Moreover, we obtain good agreement between circuit model analysis and simulation. The measurement results for the proposed bandpass filter reveals a 3 dB pass band with a bandwidth of 6.8 GHz. Furthermore, the insertion loss at 60 GHz is less than 2 dB.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5578757"},{"name":"Bivariate Software Fault-Detection Models","snippet":"In this paper, we develop bivariate software fault-detection models with two time measures: calendar time (day) and test-execution time (CPU time) and incorporate both of them to assess the quantitative software reliability with higher accuracy. The resulting stochastic models are characterized by a simple binomial process and the bivariate order statistics of software fault-detection times with different time scales.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4291048"},{"name":"A high resolution and early triggering on multi layer processing defects","snippet":"In this paper, we discussed the methodology of multi layers defect monitoring using probability of occurrence (p) instead of average value. We also show how the probability of occurrence (p) is able to early trigger a defectivity event thus preventing major yield impact compare to average value.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4493098"},{"name":"Fault management using passive testing for mobile IPv6 networks","snippet":"In this paper, we employ the communicating finite state machine (CFSM) model for networks to investigate fault management using passive testing. First, we introduce the concept of passive testing. Then, we introduce the CFSM model, the observer model and the fault model with necessary assumptions. We introduce the fault detection algorithm using passive testing. Then, we briefly present our new passive testing approach for fault location, fault identification, and fault coverage based on the CFSM model. We illustrate the effectiveness of our new technique through simulation of a practical protocol example, a 4-node mobile IPv6 network. Finally, conclusions and potential extensions are discussed","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=965909"},{"name":"The complexity of adding failsafe fault-tolerance","snippet":"In this paper, we focus our attention on the problem of automating the addition of failsafe fault-tolerance where fault-tolerance is added to an existing (fault-intolerant) program. A failsafe fault-tolerant program satisfies its specification (including safety and liveness) in the absence of faults. And, in the presence of faults, it satisfies its safety specification. We present a somewhat unexpected result that, in general, the problem of adding failsafe fault-tolerance in distributed programs is NP-hard. Towards this end, we reduce the 3-SAT problem to the problem of adding failsafe fault-tolerance. We also identify a class of specifications, monotonic specifications and a class of programs, monotonic programs. Given a (positive) monotonic specification and a (negative) monotonic program, we show that failsafe fault-tolerance can be added in polynomial time. We note that the monotonicity restrictions are met for commonly encountered problems such as Byzantine agreement, distributed consensus, and atomic commitment. Finally, we argue that the restrictions on the specifications and programs are necessary to add failsafe fault-tolerance in polynomial time; we prove that if only one of these conditions is satisfied, the addition of failsafe fault-tolerance is still NP-hard.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1022271"},{"name":"The effect of the specification model on the complexity of adding masking fault tolerance","snippet":"In this paper, we investigate the effect of the representation of safety specification on the complexity of adding masking fault tolerance to programs - where, in the presence of faults, the program 1) recovers to states from where it satisfies its (safety and liveness) specification and 2) preserves its safety specification during recovery. Specifically, we concentrate on two approaches for modeling the safety specifications: 1) the bad transition (BT) model, where safety is modeled as a set of bad transitions that should not be executed by the program, and 2) the bad pair (BP) model, where safety is modeled as a set of finite sequences consisting of at most two successive transitions. If the safety specification is specified in the BT model, then it is known that the complexity of automatic addition of masking fault tolerance to high atomicity programs - where processes can read\/write all program variables in an atomic step) - is polynomial in the state space of the program. However, for the case where one uses the BP model to specify safety specification, we show that the problem of adding masking fault tolerance to high atomicity programs is NP-complete. Therefore, we argue that automated synthesis of fault-tolerant programs is likely to be more successful if one focuses on problems where safety can be represented in the BT model.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1542056"},{"name":"Research and Implementation of Fault Diagnosis System on Oil Field Based on Intelligence Integrating","snippet":"In this paper, the authors put forward the integrating intelligence diagnosis model and the key technologies on this model in order to solve the work status diagnosis problem of oil well .The merits of this system included: knowledge acquisition automatically, high speed of identifying, high degree of intelligent zing, etc., and this system can identify 21 kinds of faults, this diagnosis technology was more exact comparing to other diagnosis methods.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5138032"},{"name":"A strategy to replace the damaged element for fault-tolerant induction motor drive","snippet":"In this paper, the best moment to replace to the damaged element in a fault-tolerant induction motor drive working with a open-loop and closed-loop control is presented, a previous stage of fault-diagnostic to detect a short-circuit or open-circuit failure in the power device is considered. The technique is based on the connection of bidirectional switches to electrically isolate the damaged element by mean of fuse blown corresponding and to replace only the damaged device by another healthful one at the most suitable moment, the main issue is to diminish the tracking error of the motor current during the fault transient. Experimental and simulation results are obtained in order to validate the technique proposed.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4723458"},{"name":"Control strategy of transformer coupling solid state fault current limiter and its experimental study with capacitance load","snippet":"In this paper, the control strategy of transformer coupling three phase bridge type solid state fault current limiter (TC-SSFCL) is presented. It is validated in an experimental system. It's proved that the control strategy ensures TC-SSFCL has excellent control performance and current-limiting effectiveness in its normal startup, operation and current-limiting conditions. Issues faced by TC-SSFCL in a double-side power system are analyzed and solutions are suggested. The series resonance which appears in the test of TC-SSFCL with capacitance load is analyzed in detail. To suppress the resonance, both methods, pre-triggering the SCR bridge and adding a damping resistance in parallel with the coupling transformer, are discussed. The simulation results verify that the proposed strategies for suppressing the resonance are effective.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5348308"},{"name":"Fault Tolerant Control for Nonlinear Systems: Sum-of-Squares Optimization Approach","snippet":"In this paper, the fault tolerant control problem of nonlinear systems against actuator failures is considered. By representing the open-loop nonlinear systems in a state dependent linear-like polynomial form and implementing a special class of Lyapunov functions, the above problem can be formulated in terms of state dependent linear polynomial inequalities. Semidefinite programming relaxations based on the sum of squares decomposition are then used to efficiently solve such inequalities.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4389303"},{"name":"Fault Tolerant Strategies for BLDC Motor Drives under Switch Faults","snippet":"In this paper, the fault tolerant system for BLDC motors has been proposed to maintain the control performance under switching device faults of inverter. The proposed fault tolerant system provides compensation for open-circuit faults and short-circuit faults in power converter. The fault identification is quickly achieved by simple algorithm using the characteristic of BLDC motor drives. The drive system after fault identification is reconfigured by the four-switch topology connecting a faulty leg to the middle point of DC-link using bidirectional switches. The feasibility of the proposed fault tolerant system is proved by simulation","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4025443"},{"name":"The SRGM Framework of Integrated Fault Detection Process and Correction Process","snippet":"In this paper, the hypothesis that the detected faults will be immediately removed is revised. An integration of fault detection process and correction process is considered. According to the statistic of the faults, the two new frameworks, which are the SRGM framework including repeated faults (CRDW) and the SRGM framework excluding repeated faults (CNRDW), are presented. The above two frameworks, not only can predict the number of cumulative detected faults, but also can predict the number of corrected faults. In this paper, as an example, two reliability models are gained from CNRDW with different detection process and different correction process. The fitting capability and prediction capability of the two models are evaluated by an open software failure data set. The experimental results show that the presented models have a fairly accurate fitting capability and prediction capability compared with other software reliability growth models.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4722142"},{"name":"Implementation of a bug algorithm in the e-puck from a hybrid control viewpoint","snippet":"In this paper, the implementation in the e-puck robot of an algorithm to keep going in a trajectory while evading fixed obstacles is presented. A review of some existing algorithms for trajectory tracking with obstacles avoidance is done. Also the basic characteristics of the mobile robot e-puck and the programming environment used to implement the control algorithm and prove the performance of the robot are summarized. By modeling the kinematics of the robot and simulating the implementation of the algorithm, the good performance of the control in both the simulated environment and the real robot in different surroundings are shown. The effects of different environmental factors in the performance of the robot are analyzed. This leads to suggest some algorithm improvements as a matter of future works.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5587242"},{"name":"A Monte Carlo study of deconvolution algorithms for partial volume correction in quantitative PET","snippet":"In this study, we evaluated several deconvolution methods for partial volume (PV) correction within dynamic positron emission tomography (PET) brain imaging and compared their performance with a PV correction method based on structural imaging. The motivation for this study stemmed from the errors in structural imaging based PV correction that are caused by magnetic resonance (MR) image segmentation and MR-PET registration inaccuracies. The studied deconvolution methods included variants of the iterative Richardson-Lucy deconvolution, variants of the reblurred Van Cittert deconvolution and the linear Wiener deconvolution. Our material consisted of a database of 16 Monte Carlo simulated dynamic <sup>11<\/sup>C-Raclopride images with the same underlying physiology but differing underlying anatomy. We compared the binding potential (BP) values in putamen and caudate resulting from differing PV correction methods to the values computed based on the ground truth time activity curves (TACs). In addition, root mean square errors between TACs extracted from deconvolved images and the ground truth TACs were computed. The iterative deconvolution approaches featured better performance than the linear one. As expected, MR based PV correction under ideal conditions (perfect MR-PET registration and MR image segmentation) yielded more accurate quantification than the deconvolution based methods. However, the iterative deconvolution methods clearly improved the quantitative accuracy of computed physiological parameters (BP) as compared to the case of no PV correction. As variants of the reblurred Van Cittert deconvolution resulted in a lower anatomy-induced variance to the BP values, we consider them to be more interesting than Richardson-Lucy type deconvolution methods.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4179761"},{"name":"Compiler Optimizations for Fault Tolerance Software Checking","snippet":"In this work we propose a set of compiler optimizations to identify and remove redundant checks from the replicated code. Two checks are considered redundant if they check the same variable. In this work we evaluate two levels of hardware or system support: memory without support for checkpointing and rollback, where memory is guaranteed to not be corrupted with wrong values and memory with low-cost support for checkpointing and rollback. We also consider the situation where register file is protected with parity or ECC, such as Intel Itanium, Sun UltraSPARC and IBM Power4-6 because software implementations can take advantage of this hardware feature and reduce some of the replicated instructions. We have evaluated our approach using LLVM as our compiler infrastructure and PIN for fault injection. Our experimental results with Spec benchmarks on a Pentium 4 show that in the case where memory is guaranteed not to be corrupted, performance improves by an average 6.2%. With more support for checkpoint performance improves by an average 14.7%. A software fault tolerant system that takes advantage of the register safe platforms improves by an average 16.0%. Fault injection experiments show that our techniques do not decrease fault coverage, although they slightly increase the number of segmentation faults.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4336261"},{"name":"Calculating Functions of Interval Type-2 Fuzzy Numbers for Fault Current Analysis","snippet":"In this work, functions of type-2 fuzzy numbers are analyzed. For the special case of interval type-2 fuzzy numbers, the type-2 membership function of the output variable is calculated using the lower and upper membership functions of the input variables and the vertex method. This procedure is used in an application where the type-2 fuzzy fault currents of an electric distribution system are calculated. The results are shown and the advantages of the approach are discussed","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4088977"},{"name":"Fault tolerance system for UAV using hardware in the Loop Simulation","snippet":"In Unmanned Aerial Vehicle system, the application software is large and becoming hard to meet the real-time application. UAV system may suffer faults in the controlled plant as well as in the execution platform. The execution platforms support a modern real-time embedded system, but distributed architecture is made of heterogeneous components that may incur transient or permanent faults. In the proposed system, we have developed UAV system based on Hardware in-the Loop Simulation (HILS), which is used for testing the UAV with real time data and real environment. This technique of simulation makes the system to be tested extrinsically with various conditions. The major part of this system deals with fault tolerance system for the UAV with fault injection mechanism and fault detection mechanism using the fault tree analysis. The fault recovery mechanism is also used for making the UAV to land in the safe mode. This paper also deals with the hardware in the simulation setup for the UAV system using the QNX operation system. This reliable architecture can enhance analysis capabilities for critical safety properties and reduce costs for such systems using Hardware in-the Loop Simulation.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5488603"},{"name":"Extended transfer bound error analysis in the presence of channel random nuisance parameter","snippet":"In this paper, we present the extended transfer bound analysis for the error performance of a general trellis code in the channel with the overall correlated continuous valued nuisance parameters. We introduce proper parameter model and include it into a new extended form of the transfer function. In this way, both, the new additional parameter space and the original error space are incorporated into system error analysis. An example application with simple trellis code and Rayleigh fading channel is investigated in order to demonstrate the functionality of the principle. Computer simulation results are presented for two different codes and various fading scenarios, and comparisons are made among analytical and measured system error performances. It was shown that for fading amplitude our approach was able to predict correct error asymptotes","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1651699"},{"name":"Diagnosis of Defects on Scan Enable and Clock Trees","snippet":"In this paper, we propose a different software based method that does not require any extra effort manipulating test parameters. First, we assume the defects are on scan chains and use previously published chain diagnosis algorithms presented in Y. Huang et al. (2005) to identify the suspect scan cells. Secondly, if there is at least one faulty chain that is modeled with stuck-at-X fault, we attempt to diagnose with stuck-at-0 fault model at scan enable. As we mentioned earlier that the shift operation is incorrect when the scan cell value is obtained from the system logic for each shift cycle, it is very likely we see both stuck-at-1 and stuck-at-0 at scan cells. So stuck-at-X fault model at scan cells is a sign of the stuck-at-0 fault model for scan enable defects","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1656920"},{"name":"Design of LDPC-based error correcting cipher","snippet":"In this paper, we propose a LDPC error correcting cipher which joints the Advanced Encryption Standard (AES) and LDPC code together. The LDPC error correcting cipher which is based on the wide trail strategy is a six round block cipher that encrypts 256 bit plaintexts using secret key to produce 512 bit ciphertexts, and the key is composed of 128 bit AES secret key and LDPC generator matrix. By using the LDPC generator matrix with high performance in diffusion property, we made the LDPC error correcting cipher as secure as the Advanced Encryption Standard (AES) against linear, differential attacks in fewer rounds. Even the square attack has no effect on attacking the cipher. Lastly, the process of encrypting\/decrypting is implemented, and the security and error correction capacity is analyzed also.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6414833"},{"name":"Helping users avoid bugs in GUI applications","snippet":"In this paper, we propose a method to help users avoid bugs in GUI applications. In particular, users would use the application normally and report bugs that they encounter to prevent anyone - including themselves - from encountering those bugs again. When a user attempts an action that has led to problems in the past, he\/she will receive a warning and will be given the opportunity to abort the action - thus avoiding the bug altogether and keeping the application stable. Of course, bugs should be fixed eventually by the application developers, but our approach allows application users to collaboratively help each other avoid bugs - thus making the application more usable in the meantime. We demonstrate this approach using our \"Stabilizer\" prototype. We also include a preliminary evaluation of the Stabilizer's bug prediction.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1553553"},{"name":"Theoretical expression of error event probability for a trellis chaos coded modulation concatenated with space-time blok code","snippet":"In this paper, we propose a new insights for the chaos coded modulation (CCM) schemes originally proposed by Kozic & al. using the approximation of the distance spectrum distribution with some usual laws, a complete study of the performances of these CCM schemes when they are concatenated with a Space Time Block Code (STBC) is proposed. Accurate bounds are obtained even in the case of time selective channels.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7096508"},{"name":"Zero Defect Strategy for Electronic Components in Automotive Applications","snippet":"Key challenges for components like ASICs and power semiconductors in automotive applications are introduced. The background for zero defects is highlighted. A vision of a strategy to achieve zero defects will be drawn","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4079434"},{"name":"Laboratory system for a traceable measurement of error vector magnitude","snippet":"Laboratory system for a traceable measurement of error vector magnitude (EVM) is presented. It comprises a sampling oscilloscope which is directly traceable to basic physical quantities and a spectrum analyzer which is used for routine measurements and calibrations. The system is able to measure EVM for basic digitally modulated signals with low uncertainty.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5296163"},{"name":"An NN-based atmospheric correction algorithm for Landsat\/TM thermal infrared data","snippet":"Land surface temperature (LST) is a key variable for studies of global or regional land surface processes, energy and water cycle, and thus, has important applications in various areas. Atmospheric correction is a major issue in LST retrieval using remote sensing data because the presence of the atmosphere always influences the radiation from the ground to the space sensor. Atmospheric correction of thermal infrared (TIR) data for land surface temperature retrieval is to estimate the three atmospheric parameters: transmittance, path radiance and the downward radiance. Typically the atmospheric parameters are obtained using atmospheric profiles combined with a radiative transfer model (RTM). But this approach is time-consuming and expensive, which is impractical for high-speed (near-realtime) operational atmospheric correction. An artificial neural network (NN) based atmospheric correction model for Landsat\/TM thermal infrared data is proposed. The multi-layer feed-forward neural network (MFNN) is selected, in which the atmospheric profiles (temperature, humidity and pressure), elevation and scan angle are the input variables, and the atmospheric parameters are the output variables. The MFNN is combined with the radiative transfer simulation, using MODTRAN 4.0 and the latest global assimilated data. Finally, the transmittance and path radiance derived by the MFNN-based algorithm is compared with MODTRAN4.0 results. The RMSE for both parameters are 0.0031 and 0.035 Wm<sup>-2<\/sup>sr<sup>-1<\/sup>m<sup>-1<\/sup>, respectively. The results indicate that the proposed approach can be a practical method for Landsat\/TM thermal data in both accuracy and efficiency.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5476046"},{"name":"On a nonlinear multiple-centrality-corrections interior-point method for optimal power flow","snippet":"Large scale nonlinear optimal power flow (OPF) problems have been efficiently solved by extensions from linear programming to nonlinear programming of the primal-dual logarithmic barrier interior-point method and its predictor-corrector variant. Motivated by the impressive performance of the nonlinear predictor-corrector extension, in this paper we extend from linear programming to nonlinear OPF the efficient multiple centrality corrections (MCC) technique that was developed by Gondzio. The numerical performance of the proposed MCC algorithm is evaluated on a set of power networks ranging in size from 118 buses to 2098 buses. Extensive computational results demonstrate that the MCC technique is fast and robust, and outperforms the successful predictor-corrector technique","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=918290"},{"name":"Finding Defects in Natural Language Confidentiality Requirements","snippet":"Large-scale software systems must adhere to complex, multi-lateral security and privacy requirements from regulations. It is industrial practice to define such requirements in form of natural language (NL) documents. Currently existing approaches to analyzing NL confidentiality requirements rely on a manual linguistic transformation and normalization of the original text, prior to the analysis. This paper presents an alternative approach to analyzing requirements by using semantic annotations placed directly into the original NL documents. The benefits of this alternative approach are twofold: (1) it can effectively be supported by an interactive annotation tool and (2) there is a direct traceability between annotation structures and the original NL documents. We have evaluated our method and tool support using the same real-world case study that was used to evaluate the earlier linguistic approach. Our results show that our method generates the same results, i.e., it uncovers the same problems.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5328562"},{"name":"Correction of the interpolation error of quadro-phase detection in interferometry","snippet":"Laser interferometers using quadro-phase detectors for the fringe counting are frequently used for ultra-precise displacement measurements. The accuracy of interferometers in nanometrology is limited mainly by the interpolation error of the detector system. The new method based on applications of curve fitting using nonlinear last square method was developed for correcting the interpolation error of quadro-phase detectors. The results from the experimental data obtained in interferometrical comparator CMI IK-1 are presented.","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=850979"},{"name":"Active and latent failure conditions leading to human error in physical security","snippet":"Latent failure conditions describe the set of background circumstances which eventually lead to an unsafe act. These latent and active failures are generally uncovered during an accident investigation following an incident such as an airplane crash. This paper proposes that many evident preconditions for system failure exist in the physical security industry. This proposition is based upon observing control room operator's capabilities and practices while conducting operations in a 3D virtual control room simulator. The nature of physical security industry is such that there is very little real-time feedback on operator and system performance. Operators are expected to maintain high levels of detection performance day after day, month after month. In most cases how the approximately 500,000 individuals that are protecting the nation's critical infrastructure will perform will only be known when an actual crisis occurs. It is proposed that the latent preconditions for failures in the physical security industry can be uncovered using the same methodologies used in accident investigation. The causal factors for human performance breakdown can be uncovered through simulation exercises rather than through actual incident investigation. Remedial measures can then be developed and validated in the simulator prior to being implemented in actual operations.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5678736"},{"name":"An error resilience scheme for layered video coding","snippet":"Layered video coding combined with prioritized transmission is widely recognized as one of the schemes for providing error resilience in video transport system. We examine the error performance of data partitioning coded MPEG-2 video bitstreams transmitted over channel subject to bit errors. While base layer errors cannot be tolerated, only a limited amount of errors in the enhancement layer is acceptable. Further improvements on bit error resilience can be achieved using the EREC coder in the enhancement layer. Our results show the PSNR gain of up to 3 dB with EREC coded enhancement layer and no errors in the base layer.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1529110"},{"name":"ACE: an aggressive classifier ensemble with error detection, correction and cleansing","snippet":"Learning from noisy data is a challenging and reality issue for real-world data mining applications. Common practices include data cleansing, error detection and classifier ensembling. The essential goal is to reduce noise impacts and enhance the learners built from the noise corrupted data, so as to benefit further data mining procedures. In this paper, we present a novel framework that unifies error detection, correction and data cleansing to build an aggressive classifier ensemble for effective learning from noisy data. Being aggressive, the classifier ensemble is built from the data that has been preprocessed by the data cleansing and correcting techniques. Experimental comparisons will demonstrate that such an aggressive classifier ensemble is superior to the model built from the original noisy data, and is more reliable in enhancing the learning theory extracted from noisy data sources, in comparison with simple data correction or cleansing efforts","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1562954"},{"name":"Stream Prediction Model Based on Tendency Correction","snippet":"Linear regression model is widely used in data stream prediction processing. In order to eliminate the prediction deviation caused by small data set, curve tendency correction technique is used to increase the prediction accuracy. Firstly the weighted moving method is used to modify the prediction function parameters. This algorithm improves the predicting accuracy, but causes low efficiency of time and space. Based on this algorithm, the exponential smoothing method is proposed. It is proved that this algorithm can reduce the space and time complexity, and also improves the prediction accuracy.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5368078"},{"name":"Error propagations for local bundle adjustment","snippet":"Local bundle adjustment (LBA) has recently been introduced to estimate the geometry of image sequences taken by a calibrated camera. Its advantage over standard (global) bundle adjustment is a great reduction of computational complexity, which allows real-time performances with a similar accuracy. However, no confidence measure on the LBA result such as uncertainty or covariance has yet been introduced. This paper introduces statistical models and estimation methods for uncertainty with two desirable properties: (1) uncertainty propagation along the sequence and (2) real-time calculation. We also explain why this problem is more complicated than it may appear at first glance, and we provide results on video sequences.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5206824"},{"name":"Consistent outdoor vehicle localization by bounded-error state estimation","snippet":"Localization is a part of many automotive applications where safety is of crucial importance. We think that the best way to guarantee the safety in these applications is to guarantee the results of their embedded localization algorithms. Unfortunately localization of vehicles is mostly solved by Bayesian methods which (due to their structure) can only guarantee their results in a probabilistic way. Interval analysis allows an alternative approach with bounded-error state estimation. Such an approach provides a bounded set of configurations that is guaranteed to surround the actual vehicle configuration. We have validated the bounded-error state estimation with an outdoor vehicle equipped with odometers, a GPS receiver and a gyro. With the experimental results we compare the bounded-error state estimation with the particle filter localization in terms of consistency and computation time.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5354673"},{"name":"Request Path Driven Model for Performance Fault Diagnoses","snippet":"Locating and diagnosing performance faults in distributed systems is crucial but challenging. Distributed systems are increasingly complex, full of various correlation and dependency, and exhibit dramatic dynamics. All these made traditional approaches prone to high false alarms. In this paper, we propose a novel system modeling technique, which encodes component's dynamic dependencies and behavior characteristics into system's meta-model and takes it as a unifying framework to deploy component's sub-models. We propose an automatic analyze approach to distill, from request travel paths, request path signatures, the essential information of component's dynamic behaviors, and use it to induce metamodel with Bayesian network, and then use the model to make fault location and diagnoses. We take up fault-injection experiments with RUBiS, a TPCW alike benchmark, simulating eBay.com. The results indicate that our model approach provides effective problem diagnosis, i.e., Bayesian network technique is effective for fault detecting and pinpointing, in terms of request tracing context. Moreover, meta-model induced with request paths, provides an effective guidance for learning statistical correlations among metrics across the system, which effectively avoid 'false alarms' in fault pinpointing. As a case study, we construct a proactive recovery framework, which integrate our system modeling technique with software rejuvenation technique to guarantee system's quality of services.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5634347"},{"name":"Defects Inspecting System for Tapered Roller Bearings Based on Machine Vision","snippet":"Many bearing manufacturers take traditional manual methods to inspect defects of bearings, which are in low efficiency, costly and unreliable. In this paper, a defects inspecting system for tapered roller bearing based on machine vision (DISTRB) is introduced. DISTRB is used to realize online automatic inspection, which includes mechanical parts, electrical parts, lighting equipments, a CCD, a computer and its software. DISTRB actualizes automatic online inspection by CCDs and defects recognizing algorithms and takes the place of workers' eyes. Some implementation devices are used to get high-quality images. Defects recognition algorithms of DISTRB are mainly elaborated. The defects of bearings include roller-missing in tapered roller bearings (RM) and some rollers being installed opposite to the direction of standard need (ROD). The key of DISTRB is to use practical image processing skills to improve the accuracy of the defects recognizing algorithms. The accuracy and stability of DISTRB mainly depends on the algorithms with a precondition of a fixed lighting part and imaging part. The DISTRB involves multi-domains such as mechatronic engineering, computer science, image-processing and pattern recognition. In this paper, taking image of type 32011X bearing as a research object, defects recognizing algorithms for RM and ROD defects are illustrated. After theoretical calculation and experiments in laboratory and practical applications in factories, the defects inspecting system is reliable, stable and effective.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5629504"},{"name":"A novel nanocomposite and its application in repairing bone defects","snippet":"Many commercial bone graft substitutes (BGS) and experimental bone tissue engineering scaffolds have been developed for bone repair and regeneration. Among them, sol-gel processed bioactive glasses showed promise due to their unique nanostructure. This study reports the in vivo bone regeneration using a newly developed porous bioactive and resorbable nanoscomposite that is composed of nano-bioactive glass (BG), collagen (COL), hyaluronic acid (HYA) and phosphatidylserine (PS), BG-COL-HYA-PS. The nanocomposite was prepared by a combination of sol-gel and freeze-drying methods. A rabbit radius defect model was used to evaluate bone regeneration at time points of 2, 4 and 8 weeks. Techniques including radiography, histology, fluorescent marker, and micro-CT were applied to characterize the new bone formation. In addition, ectopic bone formation was also investigated for the osteoinductivity of the materials. 8 weeks' results showed that (1) nearly complete bone regeneration was achieved for the BG-COL-HYA-PS nanocomposite that was combined with a bovine bone morphogenetic protein (BMP); (2) partial bone regeneration was achieved for the BG-COL-HYA-PS composites alone; and (3) control remained empty. This study demonstrated that the novel BG-COL-HYA-PS nanocomposite with or without the grafting of BMP, is a promising BGS or a tissue engineering scaffold for non-load bearing orthopaedic applications.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4484477"},{"name":"Application fault tolerance with Armor middleware","snippet":"Many current approaches to software-implemented fault tolerance (SIFT) rely on process replication, which is often prohibitively expensive for practical use due to its high performance overhead and cost. The adaptive reconfigurable mobile objects of reliability (Armor) middleware architecture offers a scalable low-overhead way to provide high-dependability services to applications. It uses coordinated multithreaded processes to manage redundant resources across interconnected nodes, detect errors in user applications and infrastructural components, and provide failure recovery. The authors describe the experiences and lessons learned in deploying Armor in several diverse fields.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1405971"},{"name":"How does resource utilization affect fault tolerance?","snippet":"Many fault-tolerant architectures are based on the single-fault assumption, hence accumulation of dormant faults represents a potential reliability hazard. Based on the example of the fail-silent Time-Triggered Architecture we study sources and effects of dormant faults. We identify software as being more prone to dormant faults than hardware. By means of modeling we reveal a high sensitivity of the MTTF to the existence of even a small amount of irregularly used resources. We propose on-line testing as a means of coping with dormant faults and sketch an appropriate test strategy","Year":2000,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=887163"},{"name":"Applying fault-tolerant solutions of circulant graphs to meshes and hypercubes","snippet":"Many important architectures such as rings, meshes and hypercubes can be modeled as circulant graphs. As a result, circulant graphs have received a lot of attention, and a new method was developed for designing fault-tolerant solutions for them. We review this method in this paper, and examine its applications to the design of fault-tolerant solutions for meshes and hypercubes. Our results indicate that these solutions are efficient.","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1430056"},{"name":"Improved Bolstering Error Estimation for Gene Ranking","snippet":"Many methods have been proposed to identify differentially expressed genes in diseased tissues. The performance of the method is closely related to the evaluation metric. We examine several error estimation algorithms (i.e., cross validation, bootstrap, resubstitution, and resubstitution with bolstering) for three classifiers (i.e., support vector machine, Fisher's discriminant, and signed distance function). To control the classifier's data-overfitting problem, usually caused by small sample size for many real datasets, we generate synthetic datasets based on real data. This way, we can monitor sample size impact when evaluating the metrics. We find that resubstitution with bolstering has the best result, especially with respect to computational efficiency. However, classical bolstering tends to bias in high dimensions. Thus, we further investigate ways to reduce bolstering estimation bias without increasing computational intensity. Results of our investigation indicate that the estimator tends to become unbiased as the sample size increases. We also find that modified bolstering is the best among all metrics in terms of estimation accuracy and computational efficiency.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4353372"},{"name":"A quantitative error analysis for mobile network planning","snippet":"Many planning tools and algorithms are being developed to make network planning more powerful and efficient. Commonly, we need to do error analysis for the planning tools in order to evaluate them and thus improve the algorithms. This paper presents methods of quantitative error analysis for network planning. To do error analysis, drive test data are collected and then compared with planning results. In particular, the author discusses measurement requirement because it is considered that the accuracy of error analysis depends on the sampling data.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1209049"},{"name":"Fault tolerance for embedded control system","snippet":"Many products with embedded electronic systems demand high reliability and high security, such that they can be trusted to operate in safety-critical applications. Safe and secure system engineering requires designers to consider all the consequences of errors and intrusions in their systems in addition to normal modes of operation. The proposed system for embedded control system meets the requirements of fault tolerance as a major issue. Fault tolerance proposed in this paper is based on redundancy, transient faults, and fault detection which are processed by software techniques. This fault tolerance system can enhance analysis capabilities for critical safety properties and reduce certification costs.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5341096"},{"name":"Towards energy-aware software-based fault tolerance in real-time systems","snippet":"Many real-time systems employed in defense, space, and consumer applications have power constraints and high reliability requirements. In this paper, we focus on the relationship between fault tolerance techniques and energy consumption. In particular, we establish the energy efficiency of Application Level Fault Tolerance (ALFT) over other software-based fault tolerance methods. We then develop sensible energy-aware heuristics for ALFT schemes. The heuristics yield up to 40% energy savings.","Year":2002,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1029573"},{"name":"Using Logic Criterion Feasibility to Reduce Test Set Size While Guaranteeing Double Fault Detection","snippet":"Logic criteria are used in software testing to find inputs that guarantee detecting certain faults. Thus, satisfying a logic criterion guarantees killing certain mutants. Some logic criteria are composed of other criteria. Determining component criterion feasibility can be used as a means to reduce test set size without sacrificing fault detection. This paper introduces a new logic criterion based on component criterion feasibility. Given a predicate in minimal DNF, a determination is made of which component criteria are feasible for individual literals and terms. This in turn provides determination of which criteria are necessary to detect double faults and kill second-order mutants. A test set satisfying this new criterion guarantees detecting the same double faults as a larger test set satisfying another criterion. An empirical study using predicates in avionics software showed that tests sets satisfying the new criterion detected all but one double fault type. For this one double fault type, 99.91% of the double faults were detected and combining equivalent single faults nearly always yielded an equivalent double fault.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4976383"},{"name":"Automatic Correction of Loop Transformations","snippet":"Loop nest optimization is a combinatorial problem. Due to the growing complexity of modern architectures, it involves two increasingly difficult tasks: (1) analyzing the profitability of sequences of transformations to enhance parallelism, locality, and resource usage, which amounts to a hard problem on a non-linear objective function; (2) the construction and exploration of search space of legal transformation sequences. Practical optimizing and parallelizing compilers decouple these tasks, resorting to a predefined set of enabling transformations to eliminate all sorts of optimization-limiting semantical constraints. State-of-the-art optimization heuristics face a hard decision problem on the selection of enabling transformations only remotely related to performance. We propose a new design where optimization heuristics first address the main performance anomalies, then correct potentially illegal loop transformations a posteriori, attempting to minimize the performance impact of the necessary adjustments. We propose a general method to correct any sequence of loop transformations through a combination of loop shifting, code motion and index-set splitting. Sequences of transformations are modeled by compositions of geometric transformations on multidimensional affine schedules. We provide experimental evidence of the scalability of the algorithms on real loop optimizations.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4336220"},{"name":"Highly Parallel FPGA Emulation for LDPC Error Floor Characterization in Perpendicular Magnetic Recording Channel","snippet":"Low-density parity-check (LDPC) codes offer a promising error correction approach for high-density magnetic recording systems due to their near-Shannon limit error-correcting performance. However, evaluation of LDPC codes at the extremely low bit error rates (BER) required by hard disk drive systems, typically around 10<sup>-12<\/sup> to 10<sup>- 15<\/sup>, cannot be carried out on high-performance workstations using conventional Monte Carlo techniques in a tractable amount of time. Even field-programmable gate array (FPGA) emulation platforms take a few weeks to reach BER between 10<sup>-11<\/sup> and 10<sup>-12<\/sup>. Thus, we implemented a highly parallel FPGA processing cluster to emulate a perpendicular magnetic recording channel, which enabled us to accelerate the emulation by > 100times over the fastest reported emulation. This increased throughput enabled us to characterize the performance of LDPC code BER down to near 10<sup>-14<\/sup> and investigate its error floor.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5257218"},{"name":"An adaptive error concealment mechanism for H.264\/AVC encoded low-resolution video streaming","snippet":"Low-rate video is widely used especially in mobile communications. H.264\/AVC (advanced video coding) is well suited for the real-time error resilient transport over packet oriented networks. In real-time communications, lost packets at the receiver cannot be avoided. Therefore, it is essential to design efficient error concealment methods which allow to visually reduce the degradation caused by the missing information. Each method has its own quality of reconstruction. We implemented various efficient error concealment techniques and investigated their performance in different scenarios. As a result, we propose and evaluate an adaptive error concealment mechanism that accomplishes both - good performance and low complexity enabling the deployment for mobile video streaming applications. This mechanism selects suitable error concealment method according to the amount of instantaneous spatial and temporal information of the video sequence.","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7071253"},{"name":"Machine Learning and Bias Correction of MODIS Aerosol Optical Depth","snippet":"Machine-learning approaches (neural networks and support vector machines) are used to explore the reasons for a persistent bias between aerosol optical depth (AOD) retrieved from the MODerate resolution Imaging Spectroradiometer (MODIS) and the accurate ground-based Aerosol Robotic Network. While this bias falls within the expected uncertainty of the MODIS algorithms, there is room for algorithm improvement. The results of the machine-learning approaches suggest a link between the MODIS AOD biases and surface type. MODIS-derived AOD may be showing dependence on the surface type either because of the link between surface type and surface reflectance or because of the covariance between aerosol properties and surface type.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5159498"},{"name":"Employing Rapid Prototyping biomedical model to assist the surgical planning of defect mandibular reconstruction","snippet":"Mandibular defects need customized titanium plates for reconstruction. The conventional design process of titanium plate is prior to surgical interventions. The efficiency and accuracy of the conventional method is mainly dependent on experiences and skills of the designer. In order to decrease the dependence on design experience and enhance the participation of surgeons in the design process, designing and fabricating a customized titanium plate implant according to Rapid Prototyping (RP) biomedical model could be employed. RP biomedical model is greatly convenient to diagnosis and treatment planning. It could decrease the operation time and the risk of misinterpretation of the medical problem. In these cases, the operation time was reduced by 1.5-2.5 hours. A physical biomedical model also facilitates surgery planning and makes the rehearsal and simulation of the operation possibly. The customized titanium plate shape conforms to the patient's mandible anatomy and is thick enough to provide the strength and stiffness needed to fix the mandible until the bone grafts and soft tissues heal. At the same time it is thin enough to avoid extensive intrusion into soft tissues and to avoid weighing too much for the patient's jaw.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5639569"},{"name":"A Method for Predicting Marker Tracking Error","snippet":"Many augmented reality (AR) applications use marker-based vision tracking systems to recover camera pose by detecting one or more planar landmarks. However, most of these systems do not interactively quantify the accuracy of the pose they calculate. Instead, the accuracy of these systems is either ignored, assumed to be a fixed value, or determined using error tables (constructed in an off-line ground-truthed process) along with a run-time interpolation scheme. The validity of these approaches are questionable as errors are strongly dependent on the intrinsic and extrinsic camera parameters and scene geometry. In this paper we present an algorithm for predicting the statistics of marker tracker error in real-time. Based on the scaled spherical simplex unscented transform (SSSUT), the algorithm is applied to the augmented reality toolkit plus (ARToolKitPlus). The results are validated using precision off-line photogrammetric techniques.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4538841"},{"name":"Efficient Intra Refresh Using Motion Affected Region Tracking for Surveillance Video over Error Prone Networks","snippet":"Intra refresh is well-known as a simple technique for eliminating temporal error propagation in a predictive video coding system. However, for surveillance video systems, most of the existing intra refresh methods do not make good use of the properties of surveillance video. This paper proposes an efficient error recovery scheme using a novel intra refresh based on motion affected region tracking (MTIR). For every frame between two successive intra refresh frames, the motion affected regions are statistic analyzed and the error sensitive ones are intra refreshed in an refreshed frame. To suppress the potential spatial and temporal error propagation, constrained intra prediction is used for the intra refreshed macroblocks, and the reference number of an inter predicted frame which behinds an intra refreshed frame is limited. Experimental results show that compared with existing intra refresh methods and flexible macroblock ordering, the proposed method can achieve a considerable improvement on both objective peak signal noise ratio (PSNR) and subjective visual quality at the same bit rate and packet loss rate.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4696338"},{"name":"Upset-like fault injection in VHDL descriptions: A method and preliminary results","snippet":"Investigates an approach allowing one to evaluate the consequences of single event upset phenomena for the reliable operation of processors. The method is based on the simulation of bit flips using a modified version of a high-level circuit description. Preliminary results illustrate the potential of this new strategy","Year":2001,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=966778"},{"name":"Experiences, strategies, and challenges in building fault-tolerant CORBA systems","snippet":"It has been almost a decade since the earliest reliable CORBA implementation and, despite the adoption of the fault-tolerant CORBA (FT-CORBA) standard by the Object Management Group, CORBA is still not considered the preferred platform for building dependable distributed applications. Among the obstacles to FT-CORBA's widespread deployment are the complexity of the new standard, the lack of understanding in implementing and deploying reliable CORBA applications, and the fact that current FT-CORBA do not lend themselves readily to complex, real-world applications. We candidly share our independent experiences as developers of two distinct reliable CORBA infrastructures (OGS and Eternal) and as contributors to the FT-CORBA standardization process. Our objective is to reveal the intricacies, challenges, and strategies in developing fault-tolerant CORBA systems, including our own. Starting with an overview of the new FT-CORBA standard, we discuss its limitations, along with techniques for best exploiting it. We reflect on the difficulties that we have encountered in building dependable CORBA systems, the solutions that we developed to address these challenges, and the lessons that we learned. Finally, we highlight some of the open issues, such as nondeterminism and partitioning, that remain to be resolved.","Year":2004,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1275293"},{"name":"Definition of Managed Objects for Fault Management of Satellite Network","snippet":"It is a new subject for network management to investigate the fault management of satellite network. Fault management of satellite network can acquire data of managed objects by means of network management protocol. Multiplex network management protocol (MNMP) exhibits great advantages on satellite network management. However, the current MIB of MNMP defines only a part of management objects, most of which focus on the transformation of management objects of SNMP MIB and CMIP. According to objects for fault management of the four granularities, i.e. network-, function-, device-, and component-class of satellite network, this paper defines the corresponding information of management objects. The aforementioned definition conforms to definition of MNMP managed object (DMMO). All these work can solve the lack of objects in MIB fault management for MNMP.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5231531"},{"name":"Engineering Equipment Integrated Fault Diagnosis System Based on Component Technology","snippet":"It is difficult to develop the corresponding fault diagnosis system and the software reusability is bad because the engineering equipment type are so many and their performance is diverse. This paper discussed the solution to engineering equipment integrated fault diagnosis system based on component technology, put forward the system model and gave the system frame designment process and working principle. The software was designed based on the three-layer hierarchy. It is easy to reuse and maintain, and the operation of the software is simple. A kind of new theory and method to develop the engineering equipment fault diagnosis system for the future was provided.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5203051"},{"name":"Fault Diagnosis of Generator Based on D-S Evidence Theory","snippet":"It is difficult to identify the fault type with the signal gathered from the sensors. In this paper, a new fusion algorithm based on the Dempster-Shafer theory of evidence and neural networks is brought forward. This method combines the advantages of D-S evidence theory and the BP neural network. Neural networks are used to pretreated the data gathered from the embedded sensors in the monitoring system of hydropower plant. Compared with the approaches that only adopt D-S evidence theory or neural networks, the accuracy of diagnostic results is obviously improved, and the signals analysis proved this conclusion. This method has been applied in the monitoring system of JiLin FengMan hydropower plant successfully.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4696285"},{"name":"Injecting intermittent faults for the dependability validation of commercial microcontrollers","snippet":"It is expected that intermittent faults will be a great challenge in modern VLSI circuits. In this work, we present a case study of the effects of intermittent faults on the behavior of a commercial microcontroller. The methodology used lies in VHDL-based fault injection technique, which allows a systematic and exhaustive analysis of the influence of different fault and system parameters. From the simulation traces, the occurrences of failures and latent errors have been logged. To extend the study, the results obtained have been compared to those got when injecting transient and permanent faults. The applied methodology can be generalized to more complex systems.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4695899"},{"name":"Partial Discharge Pattern Characteristic of HV Cable Joints with Typical Artificial Defect","snippet":"Insulation failure of XLPE cable system, particularly in joints and terminations, mostly attributes to local defects resulted from inadequate manufacturing or poor installation workmanship. Since it is well-acknowledged that partial discharge (PD) has a relationship with the discharge source (fault or defect), PD detection and pattern recognition have been widely accepted as a means to provide information on both the type and severity of defects or potential failures, which is further expected to give advice on repair or replacement of cable accessories. This paper presents a laboratory experiment on HOkV XLPE cable joints with artificial typical defects and their partial discharge pattern characteristic. PD patterns and statistical operators of these defects shows dissimilarity, which can be utilized as samples for further on-site PD pattern recognition.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5449349"},{"name":"The rising threat of vulnerabilities due to integer errors","snippet":"Integer errors are mistakes a programmer makes in sensitive operations involving integer data-type variables. Bugs caused by incorrect integer use are a fact of life for developers. In early 2001, it became clear that integer errors frequently cause security vulnerabilities. The article explains the vulnerabilities, and offers guidelines to prevent the introduction of these flaws. The most important thing is to maintain awareness of the risks during software development.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1219077"},{"name":"Snooze: A Scalable, Fault-Tolerant and Distributed Consolidation Manager for Large-Scale Clusters","snippet":"Intelligent workload consolidation and dynamic cluster adaptation offer a great opportunity for energy savings in current large-scale clusters. Because of the heterogeneous nature of these environments, scalable, fault-tolerant and distributed consolidation managers are necessary in order to efficiently manage their workload and thus conserve energy and reduce the operating costs. However, most of the consolidation managers available nowadays do not fulfill these requirements. Hence, they are mostly centralized and solely designed to be operated in virtualized environments. In this work, we present the architecture of a novel scalable, fault-tolerant and distributed consolidation manager called Snooze that is able to dynamically consolidate the workload of a software and hardware heterogeneous large-scale cluster composed out of resources using the virtualization and Single System Image (SSI)technologies. Therefore, a common cluster monitoring and management API is introduced, which provides a uniform and transparent access to the features of the underlying platforms. Our architecture is open to support any future technologies and can be easily extended with monitoring metrics and algorithms. Finally, a comprehensive use case study demonstrates the feasibility of our approach to manage the energy consumption of a large-scale cluster.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5724821"},{"name":"Enhanced server fault-tolerance for improved user experience","snippet":"Interactive applications such as email, calendar, and maps are migrating from local desktop machines to data centers due to the many advantages offered by such a computing environment. Furthermore, this trend is creating a marked increase in the deployment of servers at data centers. To ride the price\/performance curves for CPU, memory and other hardware, inexpensive commodity machines are the most cost effective choices for a data center. However, due to low availability numbers of these machines, the probability of server failures is relatively high. Server failures can in turn cause service outages, degrade user experience and eventually result in lost revenue for businesses. We propose a TCP splice-based Web server architecture that seamlessly tolerates both Web proxy and backend server failures. The client TCP connection and sessions are preserved, and failover to alternate servers in case of server failures is fast and client transparent. The architecture provides support for both deterministic and non-deterministic server applications. A prototype of this architecture has been implemented in Linux, and the paper presents detailed performance results for a PHP-based Webmail application deployed over this architecture.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4630085"},{"name":"Improvement of Interpixel Uniformity in Carbon Nanotube Field Emission Display by Luminance Correction Circuit","snippet":"Interpixel uniformity is critical to image quality, and is hard to be improved by structural reformation of emissive elements because of nonuniformity in luminance characteristics of subpixels in self-emissive device like the carbon nanotube field emission display (CNT-FED). In this paper, we discuss the improvement of the interpixel uniformity by individually controlling the luminance of all subpixels in a display panel. We propose a prototype CNT-FED with improved interpixel uniformity using luminance correction circuitry. The analysis of interpixel uniformity with the proposed luminance correction circuit shows that the index of interpixel uniformity increases about 8% with only about 10% luminance reduction ratio that is a relatively small penalty compared to the enhancement of the interpixel uniformity. The proposed correction method can also be used to ensure improvement of the interpixel uniformity in large-size CNT-FED panels.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4444642"},{"name":"Studying effect of location and resistance of inter-turn faults on fault current in power transformers","snippet":"Inter-turn (turn-to-turn) fault is one of the most important failures which could occur in power transformers. This phenomenon could seriously reduce the useful life length of transformers. Meanwhile, transformer protection schemes such as differential relays are not able to detect this kind of fault. This type of fault should be studied carefully to determine its features and characteristics. In this paper the effect of fault location and fault resistance on the amplitude of fault current is studied. It is found that change of fault location along the winding has considerable effect on fault current amplitude. It would also be shown that, even small fault resistance could have major effect on fault current amplitude. In this paper, a real 240\/11 kV, 27 MVA transformer is used for simulation studies.","Year":2007,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4468934"},{"name":"A generic method for fault injection in circuits","snippet":"Microcircuits dedicated to security in smartcards are targeted by more and more sophisticated attacks like fault attacks that combine physical disturbance and cryptanalysis. The use of simulation for circuit validation considering these attacks is limited by the time needed to compute the result of the chosen fault injections. Usually, this choice is made by the user according to his knowledge of the circuit functionality. The aim of this paper is to propose a generic and semi-automatic method to reduce the number of fault injections using types of data stored in registers (latch by latch)","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4155291"},{"name":"Built-in sequential fault self-testing of array multipliers","snippet":"Microprocessor datapath architectures operate on signed numbers usually represented in two's-complement or sign-magnitude formats. The multiplication operation is performed by optimized array multipliers of various architectures which are often produced by automatic module generators. Array multipliers have either a standard, nonrecoded signed (or unsigned) architecture or a recoded (modified Booth's algorithm) architecture. High-quality testing of array multipliers based on a comprehensive sequential fault model and not affecting their well-optimized structure has not been proposed in the past. In this paper, we present a built-in self-testing (BIST) architecture for signed and unsigned array multipliers with respect to a comprehensive sequential fault model. The BIST architecture does not alter the well-optimized multiplier structure. The proposed test sets can be applied externally but their regular nature makes them very suitable for embedded, self-test application by simple specialized hardware which imposes small overheads. Two different implementations of the BIST architecture are proposed. The first implementation focuses on the test invalidation problem and targets robust sequential fault testing, while the second one focuses on test cost reduction (test time and hardware overhead).","Year":2005,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1397804"},{"name":"Optical Fault Attacks on AES: A Threat in Violet","snippet":"Microprocessors are the heart of the devices we rely on every day. However, their non-volatile memory, which often contains sensitive information, can be manipulated by ultraviolet (UV) irradiation. This paper gives practical results demonstrating that the non-volatile memory can be erased with UV light by investigating the effects of UV-Clight with a wavelength of 254 nm on four different depackaged microcontrollers. We demonstrate that an adversary can use this effect to attack an AES software implementation by manipulating the 256- bit S-box table. We show that if only a single byte of the table is changed, 2 500 pairs of correct and faulty encrypted inputs are sufficient to recover the key with a probability of 90%, in case the key schedule is not modified by the attack. Furthermore, we emphasize this by presenting a practical attack on an AES implementation running on an 8-bit microcontroller. Our attack involves only a standard decapsulation procedure and the use of alow-cost UV lamp.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5412863"},{"name":"Mining Top-K Fault Tolerant Frequent Patterns with Sliding Windows in Data Streams","snippet":"Mining frequent patterns over streaming data has become an important research focus field with broad applications. However, the real-world data may be usually polluted by uncontrolled factors. Fault-tolerant frequent pattern can express more generalized information than frequent pattern which is absolutely matched. Therefore, a novel single-pass algorithm is proposed for efficiently mining top-k fault-tolerant frequent pattern from data streams without minimum support threshold specified by user. A novel data structure is developed for maintaining the essential information of itemsets generated so far. Experimental results show that the developed algorithm is an efficient method for mining top-k fault-tolerant frequent pattern from data streams.","Year":2010,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5565961"},{"name":"Integrated monitoring and Control of Cycloconverter Drive System for Fault Diagnosis and Predictive Maintenance","snippet":"Mining power systems have a unique industrial environment: low short-circuit levels, high-power non-linear equipments, transient over-voltages, high altitude locations, dust, vibrations, etc., where faults easily occur affecting the integrity of the electronics equipment and drives reducing reliability with undesirable effects on safety and production. In order to reduce the occurrence of failures, most power installations have protection systems intended for reliable and selective clearance of faults, but their protection signals are usually not integrated to the distributed control system (DCS). With advances of information technologies, it becomes more practical to create more powerful signal processing systems for a predictive monitoring function implementation. Predictive maintenance capability can reduce the costs associated with downtimes, as well as to avoid safety hazards associated with unexpected and catastrophic failures. This paper is mainly focused on the study and implementation of a laboratory scale prototype and monitoring system for a cycloconverter with a real-time model for observation of internal variables that works integrated with the DCS control. The main contribution of this system, over the conventional monitoring and diagnosis systems is the reduction of required measurements and the use of control variables for improving performance and robust system surveillance","Year":2006,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4025386"},{"name":"A novel co-evolutionary approach to automatic software bug fixing","snippet":"Many tasks in software engineering are very expensive, and that has led the investigation to how to automate them. In particular, software testing can take up to half of the resources of the development of new software. Although there has been a lot of work on automating the testing phase, fixing a bug after its presence has been discovered is still a duty of the programmers. In this paper we propose an evolutionary approach to automate the task of fixing bugs. This novel evolutionary approach is based on co-evolution, in which programs and test cases co-evolve, influencing each other with the aim of fixing the bugs of the programs. This competitive co-evolution is similar to what happens in nature for predators and prey. The user needs only to provide a buggy program and a formal specification of it. No other information is required. Hence, the approach may work for any implementable software. We show some preliminary experiments in which bugs in an implementation of a sorting algorithm are automatically fixed.","Year":2008,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=4630793"},{"name":"Semi-automatic fault localization and behavior verification for physical system simulation models","snippet":"Mathematical modeling and simulation of complex physical systems are emerging as key technologies in engineering. Modern approaches to physical system simulation allow users to specify simulation models with the help of equation-based languages. Due to the high-level declarative abstraction of these languages program errors are extremely hard to find. This paper presents an algorithmic semi-automated debugging framework for equation-based modeling languages. We show how program slicing and dicing performed at the intermediate code level combined with assertion checking techniques can automate, to a large extent, the error finding process and behavior verification for physical system simulation models.","Year":2003,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=1240315"},{"name":"Fault detection and location of open-circuited switch faults in matrix converter drive systems","snippet":"Matrix converter based electric vehicles can be effectively applied to military vehicles due to weight and volume reduction as well as high temperature operation with no dc-bus capacitors fragile in a harsh environment. For successful applications for military vehicle areas, satisfactory reliability issues have to be incorporated into the matrix converter drives. This paper proposes a fault diagnostic technique for detecting and locating open-circuited faults in switching components of matrix converter drive systems. In this paper, the fault-mode behaviors of the matrix converter are, in detail, explored under the open-circuited switch fault conditions. Based on the investigated knowledge of the converter behaviors, the proposed scheme enables the matrix converter drive to detect and exactly identify power switches in which open-circuited faults have occurred. The proposed fault diagnostic algorithm is based on monitoring nine voltage errors assigned to nine bi-directional switches of the matrix converter. The voltage error signals are constructed with simple comparison of measured input and output voltages. In case that any of bi-directional switches are associated with open-circuited switch faults, the dedicated voltage error signals rise over a certain threshold value, which can be possible to detect a fault occurrence and locate the faulty switch. Since the developed diagnostic method requires no construction of reference output voltages from the pulsewidth modulation (PWM) reference signals, it can be implemented with simple and robust features. Verification results are presented to demonstrate the feasibility of the proposed technique.","Year":2009,"url":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=5289539"}]

    totalResults = searchResultsData.length;
    totalPages = Math.ceil(totalResults / 10);
    displaySearchResults();
    // Update pagination
    updatePagination();
    
    preprocessing_worker.postMessage([searchResultsData,freqMap]);
}

//   })
//   .catch(error => {
//     console.error(error);
//   });
// }

// Event listener for previous page button
prevPageButton.addEventListener('click', () => {
  currentOffset -= 10;
  currentPage--;
  displaySearchResults();
  updatePagination();
});

// Event listener for next page button
nextPageButton.addEventListener('click', () => {
  currentOffset += 10;
  currentPage++; 

  searchResultsData = searchResultsData.map(obj=>{
    if(!obj.clicks==0){
      trainData.push(obj);
    }
    return obj;
  })

  // If nothing is clicked
  if(Object.keys(trainData).length == 0){
    console.log("No Clicks recorded")
    displaySearchResults();
    updatePagination();
  }else{
    console.log("Updating ranks")

    // Retrain your model and rerank the results
    trainWorker.postMessage([trainData]);
    // console.log(trainData);

    // Sort test data based on similarity scores
    testData.sort((a, b) => b.score - a.score);
  }
});

// Function to display search results for current page
function displaySearchResults() {
  const startIndex = (currentPage - 1) * 10;
  const endIndex = startIndex + 10;
  // If no results are clicked
  if(Object.keys(trainData).length == 0){
    console.log("No clicks recorded for display")
    resultsToDisplay = searchResultsData.slice(startIndex, endIndex);
  }else{
    console.log("Fetching new ranked results")
    resultsToDisplay = testData.slice(0,10);
  }
  testData = searchResultsData.slice(endIndex,);

  let html = '';
  resultsToDisplay.forEach(result => {
    html += `
      <article>
        <h2><a href="${result.url}" target ="_blank">${result.name}</a></h2>
        <p>${result.snippet}</p>
      </article>
    `;
  });
  searchResults.innerHTML = html;
  console.log("Search Results slice displayed for page",currentPage)

  // Count clicks on hyperlinks
  const hyperlinks = document.querySelectorAll('#search-results a');
  hyperlinks.forEach(hyperlink => {
    hyperlink.addEventListener('click', () => {
      const clickedUrl = hyperlink.href;
      // Adding number of clicks to the data
      searchResultsData = searchResultsData.map(obj =>{
        if(obj.url == clickedUrl){
          obj.clicks++;
          console.log('Click acknowledged',clickedUrl);
        }
        return obj;
      })
    });
  });
}

// Function to update pagination and navigation
function updatePagination() {
  pageNumber.textContent = currentPage;
  totalPageCount.textContent = totalPages;

  if (currentPage === 1) {
    prevPageButton.disabled = true;
  } else {
    prevPageButton.disabled = false;
  }

  if (currentPage === totalPages) {
    nextPageButton.disabled = true;
  } else {
    nextPageButton.disabled = false;
  }
  console.log("Pagination Updated for page",currentPage)
}
